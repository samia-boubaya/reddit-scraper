{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d29526d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# README\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5a37d",
   "metadata": {},
   "source": [
    "## ‚≠ê **Reddit/Music Threads Guidelines**\n",
    "\n",
    "**Conditions:**  \n",
    "- Description: 100‚Äì300 characters  \n",
    "- Posts must be HOT & relevant to music  \n",
    "- Downloadable visuals (images/videos)  \n",
    "- Only include posts with ‚â•100 shares  \n",
    "- High engagement / top-performing posts  \n",
    "\n",
    "**Essentials:**  \n",
    "- Keyword: `music`  \n",
    "- Focus: short text or image, simplicity, relevance  \n",
    "\n",
    "**Relevance Categories:**  \n",
    "- Music theory, instruments, production, teaching  \n",
    "- Historical highlights, facts, memes, communities  \n",
    "- Target: anyone learning, playing, or producing music  \n",
    "\n",
    "**Threads Post Rules:**  \n",
    "- Video under 15 seconds  \n",
    "- If both image & video exist, use only video  \n",
    "\n",
    "\n",
    "\n",
    "## ‚≠ê **Reddit Post Data Fields**\n",
    "\n",
    "The main data fields to extract from the Reddit Post :\n",
    "| Field Name         | Python Data Type | Description |\n",
    "|--------------------|-----------------|-------------|\n",
    "| <span style=\"color:green\">**post_title**</span>     | `str`            | Title of the post. |\n",
    "| **post_link**      | `str`            | Direct URL to the post. |\n",
    "| **post_id**        | `str`            | Unique identifier for the post. |\n",
    "| <span style=\"color:red\">**num_votes**</span>      | `int`            | Total number of upvotes the post received. |\n",
    "| <span style=\"color:red\">**num_comments**</span>   | `int`            | Total number of comments on the post. |\n",
    "| **text_length**    | `int`            | Character count of the post‚Äôs text description. |\n",
    "| <span style=\"color:green\">**post_description**</span> | `str`          | Full text description or caption of the post. |\n",
    "| **post_date**      | `date`           | Calendar date when the post was published. |\n",
    "| **post_time**      | `str`            | Time (with timezone) when the post was published. |\n",
    "| <span style=\"color:green\">**post_visual**</span>    | `list[str]`      | Direct URLs to visual content (images or videos). |\n",
    "| **visual_type**    | `str`            | Type of visual content: `\"IMAGE\"`, `\"VIDEO\"`,`\"CAROUSEL\"`, or `\"NONE\"`. |\n",
    "| **visual_count**   | `int`            | Number of visual items in the post. |\n",
    "| <span style=\"color:blue\">**filter**</span>         | `str`            | Reddit search filter used. Must be one of: `\"Relevance\"`, `\"Hot\"`, `\"Top\"`, `\"New\"`, `\"Comment count\"`. |\n",
    "| <span style=\"color:blue\">**keyword**</span>        | `str`            | Search keyword or query. |\n",
    "| <span style=\"color:blue\">**limit**</span>          | `int`            | Number of posts requested. |\n",
    "| <span style=\"color:blue\">**period**</span>         | `str`            | Time filter used when searching posts. Must be one of: `\"All time\"`, `\"Past year\"`, `\"Past month\"`, `\"Past week\"`, `\"Today\"`, `\"Past hour\"`. |\n",
    "| <span style=\"color:red\">**time_ago**</span>       | `str`            | Relative time since the post was published (e.g., `\"3 hours ago\"`). |\n",
    "\n",
    "\n",
    "\n",
    "## ‚≠ê **POST LINK - EXPLORING**\n",
    "\n",
    "| Link             | Keyword       | Filter       |\n",
    "|------------------------|-----------------------|-----------------------|\n",
    "| https://www.reddit.com/search/?q=music             |  ```music```  | ```Relevance``` by default |\n",
    "| https://www.reddit.com/search/?q=***keyword***             |  ```keyword```  | ```Relevance``` by default |\n",
    "| https://www.reddit.com/search/?q=music&type=posts&sort=hot             |  ```music``` |  ```HOT```  |\n",
    "| https://www.reddit.com/search/?q=music&type=posts&sort=top             |  ```music``` |  ```Top```  |\n",
    "| https://www.reddit.com/search/?q=***keyword***&type=posts&sort=***filter***             |  ```keyword``` |  ```filter```  |\n",
    "\n",
    "\n",
    "## ‚≠ê **FILTER**\n",
    "\n",
    "| Filter       | What it does                                             | Use it when                                      |\n",
    "|--------------|----------------------------------------------------------|-------------------------------------------------|\n",
    "| ```Relevance```    | Shows posts most related to your search terms           | You want posts that best match your search query |\n",
    "| ```Hot```          | Shows currently trending posts based on upvotes, recency, and engagement | You want popular and active posts right now    |\n",
    "| ```Top```          | Shows posts with the highest score for a given time period | You want the most upvoted or \"best\" content for a topic |\n",
    "| ```New```          | Shows posts in chronological order, newest first       | You want fresh content or to track recent activity |\n",
    "| ```Comments Count```     | Sorts posts by number of comments (descending)          | You want posts with lots of community discussion |\n",
    "\n",
    "## ‚≠ê **PERIOD**\n",
    "\n",
    "| Filter       | What it does                                             | Use it when                                      |\n",
    "|--------------|----------------------------------------------------------|-------------------------------------------------|\n",
    "| ```All time```    | Shows posts most related to your search terms           | You want posts that best match your search query |\n",
    "| ```Past year```          | Shows currently trending posts based on upvotes, recency, and engagement | You want popular and active posts right now    |\n",
    "| ```Past month```          | Shows posts with the highest score for a given time period | You want the most upvoted or \"best\" content for a topic |\n",
    "| ```Past week```          | Shows posts in chronological order, newest first       | You want fresh content or to track recent activity |\n",
    "| ```Past hour```     | Sorts posts by number of comments (descending)          | You want posts with lots of community discussion |\n",
    "\n",
    "\n",
    "## ‚≠ê **DOWNLOAD AUTOMATICALLY**\n",
    "\n",
    "| Visual Type | File Naming Formula | Example File Names |\n",
    "|-------------|-------------------|------------------|\n",
    "| ```CAROUSEL```    | **keyword_filter_period_**`<post_number>_<type>_<sequence>` | **keyword_filter_period_**`1_img_01`<br>**keyword_filter_period_**`1_img_02`<br>**keyword_filter_period_**`1_img_03` |\n",
    "| ```IMAGE```       | **keyword_filter_period_**`<post_number>_<type>` | **keyword_filter_period_**`2_img`<br>**keyword_filter_period_**`3_img`<br>**keyword_filter_period_**`4_img` |\n",
    "| ```VIDEO```       | **keyword_filter_period_**`<post_number>_<type>` | **keyword_filter_period_**`5_vid`<br>**keyword_filter_period_**`6_vid`<br>**keyword_filter_period_**`7_vid` |\n",
    "\n",
    "\n",
    "## ‚≠ê **HOW TO USE - STEPS**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 00   | Run the code |\n",
    "| 01   | Input what you want to search (example: `\"music\"`) |\n",
    "| 02   | Input the Reddit search filter: `\"Relevance\"`, `\"Hot\"`, `\"Top\"`, `\"New\"`, `\"Comment count\"` |\n",
    "| 03   | Input the time filter: `\"All time\"`, `\"Past year\"`, `\"Past month\"`, `\"Past week\"`, `\"Today\"`, `\"Past hour\"` |\n",
    "| 04   | Input the limit of how many posts you want to extract (example: `\"17\"`) |\n",
    "| 05   | Use the CSV file for data analysis and access the post visual content files in `../data/visuals/` |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309219d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ‚öôÔ∏è INSTALL BEFORE RUNNING CODE\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0eb49",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1Ô∏è‚É£ INSTALL using **GIT BASH**\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d4bd0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in c:\\users\\sboub\\anaconda3\\lib\\site-packages (1.56.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\sboub\\anaconda3\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: selenium in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.38.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: selenium in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.38.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%playwright` not found.\n"
     ]
    }
   ],
   "source": [
    "#pip install requests beautifulsoup4 lxml\n",
    "%pip install playwright pandas beautifulsoup4 lxml\n",
    "\n",
    "#pip install selenium beautifulsoup4 requests webdriver-manager\n",
    "%pip install selenium beautifulsoup4 requests webdriver-manager\n",
    "\n",
    "# install selenium\n",
    "%pip install selenium pandas\n",
    "\n",
    "# install chromium\n",
    "%playwright install chromium\n",
    "\n",
    "# install fake-useragent\n",
    "%pip install fake-useragent\n",
    "\n",
    "# install requests & beautifulsoup\n",
    "%pip install requests beautifulsoup4 fake-useragent pandas\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628cc80",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2Ô∏è‚É£ INSTALL using **NOTEBOOK**\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5370ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\sboub\\anaconda3\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: selenium in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.38.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: playwright in c:\\users\\sboub\\anaconda3\\lib\\site-packages (1.56.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%playwright` not found.\n"
     ]
    }
   ],
   "source": [
    "# Install core packages\n",
    "%pip install requests beautifulsoup4 lxml pandas fake-useragent\n",
    "\n",
    "# Install Selenium and WebDriver manager\n",
    "%pip install selenium webdriver-manager\n",
    "\n",
    "# Install Playwright and Chromium browser\n",
    "%pip install playwright\n",
    "%playwright install chromium\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3bc28",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ü§ñ REDDIT\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5046db",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1Ô∏è‚É£ FIX NAMING OF COLUMNS\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f23219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT POST EXTRACTOR - i.redd.it ONLY!\n",
      "==================================================\n",
      "\n",
      "Search:  'anime romance'\n",
      "Files:   anime_romance_main.csv ‚Üí anime_romance_{filter}.csv\n",
      "\n",
      "Filters:\n",
      "  [1] hot      ‚Üí print_art_hot.csv\n",
      "  [2] top      ‚Üí print_art_top.csv\n",
      "  [3] new      ‚Üí print_art_new.csv\n",
      "  [4] comments ‚Üí print_art_comments.csv\n",
      "  [5] relevance‚Üí print_art_relevance.csv\n",
      "\n",
      "üî• Scraping 10 'anime romance' posts (TOP)\n",
      "üì• anime_romance_main.csv ‚Üí üì§ anime_romance_top.csv\n",
      "üì° Fetching real Reddit posts...\n",
      "üîç Fetching 10 top posts for 'anime romance'...\n",
      "‚úÖ Found 10/10 top posts\n",
      "‚úÖ Saved 10 REAL posts to ../data/reddit/anime_romance_main.csv\n",
      "üìñ Loaded 10 posts from ../data/reddit/anime_romance_main.csv\n",
      "üöÄ Processing 10 TOP posts ‚Üí ../data/reddit/anime_romance_top.csv\n",
      "üìÅ Input:  ../data/reddit/anime_romance_main.csv\n",
      "================================================================================\n",
      "üîç 1/10: NETFLIX Secret genre code list updated 2018...\n",
      "   üîó Post ID: 8khygi\n",
      "   üó≥Ô∏è Votes: 36316\n",
      "   ‚úÖ JSON selftext found\n",
      "   üìù Text length: 852 chars\n",
      "   ‚úÖ NONE (0)\n",
      "üîç 2/10: LPT List of Netflix secret codes for searching the...\n",
      "   üîó Post ID: wloiih\n",
      "   üó≥Ô∏è Votes: 22857\n",
      "   ‚úÖ JSON selftext found\n",
      "   üìù Text length: 1714 chars\n",
      "   ‚úÖ NONE (0)\n",
      "üîç 3/10: Every anime romance starter pack...\n",
      "   üîó Post ID: rjxygo\n",
      "   üó≥Ô∏è Votes: 20376\n",
      "   üìù Text length: 0 chars\n",
      "   üñºÔ∏è Single: i.redd.it/rjbbhcbwfi681.jpg\n",
      "   ‚úÖ 1 i.redd.it URLS!\n",
      "   ‚úÖ IMAGE (1)\n",
      "üîç 4/10: Every other romance anime...\n",
      "   üîó Post ID: uwukhs\n",
      "   üó≥Ô∏è Votes: 17868\n",
      "   üìù Text length: 0 chars\n",
      "   üñºÔ∏è Single: i.redd.it/t5cwjvdj8g191.gif\n",
      "   ‚úÖ 1 i.redd.it URLS!\n",
      "   ‚úÖ IMAGE (1)\n",
      "üîç 5/10: I'm a 23 year old, straight man who has recently f...\n",
      "   üîó Post ID: gz26ts\n",
      "   üó≥Ô∏è Votes: 17601\n",
      "   ‚úÖ JSON selftext found\n",
      "   üìù Text length: 1997 chars\n",
      "   ‚úÖ NONE (0)\n",
      "üîç 6/10: watching romance anime be like...\n",
      "   üîó Post ID: 1jgel9m\n",
      "   üó≥Ô∏è Votes: 17160\n",
      "   üìù Text length: 0 chars\n",
      "   üñºÔ∏è Single: i.redd.it/erulflxx21qe1.jpeg\n",
      "   ‚úÖ 1 i.redd.it URLS!\n",
      "   ‚úÖ IMAGE (1)\n",
      "üîç 7/10: Beginner Anime Chart (Revised Edition)...\n",
      "   üîó Post ID: 1e6ijsn\n",
      "   üó≥Ô∏è Votes: 15087\n",
      "   üìù Text length: 0 chars\n",
      "   üñºÔ∏è Single: i.redd.it/jqkc7e61nbdd1.png\n",
      "   ‚úÖ 1 i.redd.it URLS!\n",
      "   ‚úÖ IMAGE (1)\n",
      "üîç 8/10: [WP] A necromancer discovers that spells to animat...\n",
      "   üîó Post ID: a0rrkt\n",
      "   üó≥Ô∏è Votes: 14930\n",
      "   ‚úÖ JSON selftext found\n",
      "   üìù Text length: 71 chars\n",
      "   ‚úÖ NONE (0)\n",
      "üîç 9/10: She beat the system...\n",
      "   üîó Post ID: 1opc657\n",
      "   üó≥Ô∏è Votes: 13894\n",
      "   üìù Text length: 0 chars\n",
      "   üñºÔ∏è Single: i.redd.it/v2q3ptlzmhzf1.jpeg\n",
      "   ‚úÖ 1 i.redd.it URLS!\n",
      "   ‚úÖ IMAGE (1)\n",
      "üîç 10/10: Brad Pitt movies featuring him consuming 200+ calo...\n",
      "   üîó Post ID: b3fgxb\n",
      "   üó≥Ô∏è Votes: 13827\n",
      "   ‚úÖ JSON selftext found\n",
      "   üìù Text length: 1732 chars\n",
      "   üì∑ Preview ‚Üí i.redd.it conversion\n",
      "   ‚úÖ NONE (0)\n",
      "\n",
      "üéâ SAVED 10 posts ‚Üí ../data/reddit/anime_romance_top.csv\n",
      "üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,text_length,post_description,post_visual,visual_type,visual_count\n",
      "üìä STATS: 10 posts | 5 images | 0 carousels | 0 videos | 637 chars avg\n",
      "\n",
      "‚úÖ DONE! Output: ../data/reddit/anime_romance_top.csv\n",
      "üéâ ONLY i.redd.it/xxx.png - NO preview.redd.it EVER!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [\n",
    "            f'{keyword.title()} guitar solo!',\n",
    "            f'New {keyword} album üî•',\n",
    "            f'Best {keyword} live concert',\n",
    "            f'{keyword.title()} drum cover',\n",
    "            f'{keyword.title()} masterpiece'\n",
    "        ][:limit],\n",
    "        'post_link': [\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1abc123/title1/',\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1def456/title2/',\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1ghi789/title3/',\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1jkl012/title4/',\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1mno345/title5/'\n",
    "        ][:limit]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50):\n",
    "    \"\"\"Fetch REAL Reddit posts using search API with different filters\"\"\"\n",
    "    print(f\"üîç Fetching {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    \n",
    "    filter_map = {\n",
    "        'relevance': 'relevance',\n",
    "        'top': 'top',\n",
    "        'hot': 'hot', \n",
    "        'comments': 'comments',\n",
    "        'new': 'new'\n",
    "    }\n",
    "    sort_filter = filter_map.get(filter, 'hot')\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&type=link&sort={sort_filter}&limit={min(limit,100)}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            for post in data['data']['children']:\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter\n",
    "                })\n",
    "        \n",
    "        actual_posts = min(len(posts), limit)\n",
    "        print(f\"‚úÖ Found {actual_posts}/{limit} {filter} posts\")\n",
    "        return pd.DataFrame(posts[:limit])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    # ‚úÖ DIRECT i.redd.it - KEEP AS-IS\n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    # üî• EXTRACT media_id FROM ANY preview/external-preview ‚Üí i.redd.it ONLY\n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)  # Any 13-char media_id\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50):\n",
    "    \"\"\"\n",
    "    üî• i.redd.it/xxx.png ONLY - NO preview.redd.it EVER!\n",
    "    CSV columns: post_title,post_link,post_id,num_votes,num_comments,filter,text_length,post_description,post_visual,visual_type,visual_count\n",
    "    \"\"\"\n",
    "    \n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}.csv\"\n",
    "    \n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(\"üì° Fetching real Reddit posts...\")\n",
    "        df = fetch_reddit_posts_search(keyword, filter, limit)\n",
    "        if df is not None and not df.empty:\n",
    "            os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "            df.to_csv(INPUT_FILE, index=False)\n",
    "            print(f\"‚úÖ Saved {len(df)} REAL posts to {INPUT_FILE}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Using sample data (search failed)\")\n",
    "            create_sample_main_data(keyword_clean, limit)\n",
    "    \n",
    "    df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    print(f\"üìñ Loaded {len(df)} posts from {INPUT_FILE}\")\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual)\n",
    "        visual_lower = visual_str.lower()\n",
    "        \n",
    "        if any(x in visual_lower for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        \n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        \n",
    "        if 'i.redd.it' in visual_lower or '.jpg' in visual_lower or '.png' in visual_lower or '.gif' in visual_lower:\n",
    "            return 'IMAGE', 1\n",
    "        \n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = str(description)\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        \"\"\"üî• i.redd.it/xxx.png ONLY - Converts ALL preview/external-preview\"\"\"\n",
    "        visual_urls = []\n",
    "        \n",
    "        try:\n",
    "            # 1. VIDEOS FIRST (direct playable)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                video_info = post_info['media']['reddit_video']\n",
    "                fallback_url = video_info.get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    print(f\"   ‚úÖ VIDEO: {fallback_url.split('/')[-1]}\")\n",
    "                    return visual_urls\n",
    "            \n",
    "            if post_info.get('url') and any(domain in post_info['url'].lower() \n",
    "                                        for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                print(f\"   üé• External video\")\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 2. GALLERY ‚Üí ONLY i.redd.it\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data:\n",
    "                print(f\"   üéâ GALLERY ({len(gallery_data.get('items', []))} items)\")\n",
    "                items = gallery_data.get('items', [])\n",
    "                \n",
    "                for item_idx, item in enumerate(items, 1):\n",
    "                    try:\n",
    "                        if isinstance(item, dict) and 'media_id' in item:\n",
    "                            media_id = item['media_id']\n",
    "                            viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                            if viewable_url not in visual_urls:\n",
    "                                visual_urls.append(viewable_url)\n",
    "                                print(f\"   üì∑ Gallery {item_idx}: i.redd.it/{media_id}.png\")\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 3. SINGLE IMAGE ‚Üí ONLY i.redd.it\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                visual_urls.append(viewable_url)\n",
    "                print(f\"   üñºÔ∏è Single: i.redd.it/{viewable_url.split('/')[-1]}\")\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 4. PREVIEW ‚Üí ONLY i.redd.it (source + resolutions)\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                print(f\"   üì∑ Preview ‚Üí i.redd.it conversion\")\n",
    "                for img_idx, img in enumerate(post_info['preview']['images'], 1):\n",
    "                    try:\n",
    "                        source_url = img.get('source', {}).get('url', '')\n",
    "                        if source_url:\n",
    "                            viewable_url = get_viewable_image_url(source_url)\n",
    "                            if viewable_url and 'i.redd.it' in viewable_url and viewable_url not in visual_urls:\n",
    "                                visual_urls.append(viewable_url)\n",
    "                                print(f\"   üì∑ Preview {img_idx}: i.redd.it/{viewable_url.split('/')[-1]}\")\n",
    "                                continue\n",
    "                        \n",
    "                        resolutions = img.get('resolutions', [])\n",
    "                        for res in resolutions[-2:]:  # Highest quality\n",
    "                            res_url = res.get('url', '')\n",
    "                            if res_url:\n",
    "                                viewable_url = get_viewable_image_url(res_url)\n",
    "                                if viewable_url and 'i.redd.it' in viewable_url and viewable_url not in visual_urls:\n",
    "                                    visual_urls.append(viewable_url)\n",
    "                                    print(f\"   üì∑ Preview {img_idx}: i.redd.it/{viewable_url.split('/')[-1]}\")\n",
    "                                    break\n",
    "                    except:\n",
    "                        continue\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 5. THUMBNAIL ‚Üí ONLY i.redd.it\n",
    "            if post_info.get('thumbnail') and post_info['thumbnail'] != 'self' and not visual_urls:\n",
    "                thumb_url = get_viewable_image_url(post_info['thumbnail'])\n",
    "                if thumb_url and 'i.redd.it' in thumb_url:\n",
    "                    visual_urls.append(thumb_url)\n",
    "                    print(f\"   üîç Thumbnail: i.redd.it/{thumb_url.split('/')[-1]}\")\n",
    "                \n",
    "        except Exception as visual_error:\n",
    "            print(f\"   ‚ö†Ô∏è Visual error: {str(visual_error)[:40]}\")\n",
    "        \n",
    "        return visual_urls\n",
    "    \n",
    "    print(f\"üöÄ Processing {len(df)} {filter.upper()} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìÅ Input:  {INPUT_FILE}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_link = row['post_link']\n",
    "        post_title = row['post_title']\n",
    "        \n",
    "        print(f\"üîç {idx+1}/{len(df)}: {post_title[:50]}...\")\n",
    "        \n",
    "        post_id = extract_post_id(post_link)\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå Invalid link\")\n",
    "            new_data.append(post_data)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            json_url = f\"https://www.reddit.com/comments/{post_id}.json\"\n",
    "            response = session.get(json_url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data['post_id'] = post_id\n",
    "                    post_data['num_votes'] = str(post_info.get('score', row.get('num_votes', 'N/A')))\n",
    "                    print(f\"   üó≥Ô∏è Votes: {post_data['num_votes']}\")\n",
    "                    \n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    description = 'N/A'\n",
    "                    \n",
    "                    text_body = soup.find('div', {'data-post-click-location': 'text-body'})\n",
    "                    if text_body:\n",
    "                        p_tags = text_body.find_all('p')\n",
    "                        if p_tags:\n",
    "                            description = '\\n'.join([p.get_text(strip=True) for p in p_tags])[:2000]\n",
    "                            print(f\"   ‚úÖ HTML description: {len(p_tags)} paragraphs\")\n",
    "                    \n",
    "                    if description == 'N/A':\n",
    "                        selftext = post_info.get('selftext', '')\n",
    "                        if selftext.strip():\n",
    "                            description = selftext[:2000]\n",
    "                            print(f\"   ‚úÖ JSON selftext found\")\n",
    "                    \n",
    "                    text_length = calculate_text_length(description)\n",
    "                    post_data['text_length'] = text_length\n",
    "                    post_data['post_description'] = description\n",
    "                    print(f\"   üìù Text length: {text_length} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    \n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        print(f\"   ‚úÖ {len(visual_urls)} i.redd.it URLS!\")\n",
    "                    else:\n",
    "                        post_data['post_visual'] = 'N/A'\n",
    "                    \n",
    "                    visual_type, visual_count = get_visual_type_count(post_data['post_visual'])\n",
    "                    post_data['visual_type'] = visual_type\n",
    "                    post_data['visual_count'] = visual_count\n",
    "                    print(f\"   ‚úÖ {visual_type} ({visual_count})\")\n",
    "            \n",
    "            else:\n",
    "                print(f\"   ‚ùå HTTP {response.status_code}\")\n",
    "                raise Exception(f\"HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error: {str(e)[:50]}\")\n",
    "            post_data['post_id'] = post_id\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.0)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'text_length', 'post_description', 'post_visual', \n",
    "        'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    \n",
    "    text_lengths = new_df['text_length'].fillna(0).astype(int)\n",
    "    image_posts = len(new_df[new_df['visual_type'] == 'IMAGE'])\n",
    "    carousel_posts = len(new_df[new_df['visual_type'] == 'CAROUSEL'])\n",
    "    video_posts = len(new_df[new_df['visual_type'] == 'VIDEO'])\n",
    "    print(f\"üìä STATS: {len(new_df)} posts | {image_posts} images | {carousel_posts} carousels | {video_posts} videos | {text_lengths.mean():.0f} chars avg\")\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - i.redd.it ONLY!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    keyword = input(\"Enter keyword (spaces OK): \").strip()\n",
    "    if not keyword:\n",
    "        keyword = 'music'\n",
    "    \n",
    "    num_posts = input(\"How many posts to scrape? [10]: \").strip()\n",
    "    try:\n",
    "        limit = int(num_posts) if num_posts else 10\n",
    "        limit = min(limit, 100)\n",
    "    except:\n",
    "        limit = 10\n",
    "        print(\"‚ö†Ô∏è Using 10 posts\")\n",
    "    \n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    print(f\"\\nSearch:  '{keyword}'\")\n",
    "    print(f\"Files:   {keyword_clean}_main.csv ‚Üí {keyword_clean}_{{filter}}.csv\")\n",
    "    \n",
    "    print(\"\\nFilters:\")\n",
    "    print(\"  [1] hot      ‚Üí print_art_hot.csv\")\n",
    "    print(\"  [2] top      ‚Üí print_art_top.csv\") \n",
    "    print(\"  [3] new      ‚Üí print_art_new.csv\")\n",
    "    print(\"  [4] comments ‚Üí print_art_comments.csv\")\n",
    "    print(\"  [5] relevance‚Üí print_art_relevance.csv\")\n",
    "    \n",
    "    choice = input(f\"\\nChoose filter (1-5) [1]: \").strip()\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' posts ({filter.upper()})\")\n",
    "    print(f\"üì• {keyword_clean}_main.csv ‚Üí üì§ {keyword_clean}_{filter}.csv\")\n",
    "    \n",
    "    result = extract_post_details_complete(keyword, filter, limit)\n",
    "    \n",
    "    print(f\"\\n‚úÖ DONE! Output: ../data/reddit/{keyword_clean}_{filter}.csv\")\n",
    "    print(\"üéâ ONLY i.redd.it/xxx.png - NO preview.redd.it EVER!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d762c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2Ô∏è‚É£ FIX ADD POST_DATE & POST_TIME\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53396e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT POST EXTRACTOR - FULL PROGRESS TRACKING!\n",
      "============================================================\n",
      "\n",
      "Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "üî• Scraping 13 'oban star racers' TOP posts...\n",
      "üì° Fetching EXACTLY 13 top posts...\n",
      "üîç Fetching UP TO 13 top posts for 'oban star racers'...\n",
      "   üì° API: oban%20star%20racers&sort=top&limit=100&t=month&ty...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 13/13 top posts loaded!\n",
      "‚úÖ Saved 13 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 13 posts ‚Üí ../data/reddit/oban_star_racers_top.csv\n",
      "====================================================================================================\n",
      "üîç [ 1/13] [OT] According to a press release on the RLL website release...\n",
      "   üîó [ 1/13] Post ID: 1p5f58b\n",
      "   üìÖ [ 1/13] Monday, November 24, 2025 | üïê 12:56:55 PM \n",
      "   üñºÔ∏è [ 1/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 1/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/13] According to the RLL Racing website who published the announ...\n",
      "   üîó [ 2/13] Post ID: 1p5evj3\n",
      "   üìÖ [ 2/13] Monday, November 24, 2025 | üïê 12:41:56 PM \n",
      "   üñºÔ∏è [ 2/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 2/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/13] IAE 2955 Ship Value Quiksheet...\n",
      "   üîó [ 3/13] Post ID: 1ovlk3e\n",
      "   üìÖ [ 3/13] Thursday, November 13, 2025 | üïê 12:44:24 AM \n",
      "   üìù [ 3/13] 402 chars\n",
      "   üñºÔ∏è [ 3/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 3/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/13] Now this is podracing!...\n",
      "   üîó [ 4/13] Post ID: 1oxclc3\n",
      "   üìÖ [ 4/13] Saturday, November 15, 2025 | üïê 12:50:43 AM \n",
      "   üìù [ 4/13] 46 chars\n",
      "   üñºÔ∏è [ 4/13] CAROUSEL (3) - 3 URLs!\n",
      "   ‚úÖ [ 4/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/13] Annan STEAM-peliavaimia ilmaisiksi, tule hakemaan omasi!...\n",
      "   üîó [ 5/13] Post ID: 1oypk3n\n",
      "   üìÖ [ 5/13] Sunday, November 16, 2025 | üïê 05:09:06 PM \n",
      "   üìù [ 5/13] 1920 chars\n",
      "   ‚ûñ [ 5/13] No visuals\n",
      "   ‚úÖ [ 5/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/13] Crash 4 It‚Äôs About Time Trials, The 11th Dimension: Snow Way...\n",
      "   üîó [ 6/13] Post ID: 1pfthbc\n",
      "   üìÖ [ 6/13] Saturday, December 06, 2025 | üïê 05:35:25 PM \n",
      "   üìù [ 6/13] 127 chars\n",
      "   üñºÔ∏è [ 6/13] VIDEO (1) - 1 URLs!\n",
      "   ‚úÖ [ 6/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/13] My Epic Universe VIP exclusive tour review...\n",
      "   üîó [ 7/13] Post ID: 1p1c6xs\n",
      "   üìÖ [ 7/13] Wednesday, November 19, 2025 | üïê 05:25:35 PM \n",
      "   üìù [ 7/13] 1982 chars\n",
      "   üñºÔ∏è [ 7/13] CAROUSEL (11) - 11 URLs!\n",
      "   ‚úÖ [ 7/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/13] My Top 50 PS1 Games Ranked...\n",
      "   üîó [ 8/13] Post ID: 1p8bp2b\n",
      "   üìÖ [ 8/13] Thursday, November 27, 2025 | üïê 09:04:45 PM \n",
      "   üìù [ 8/13] 1951 chars\n",
      "   ‚ûñ [ 8/13] No visuals\n",
      "   ‚úÖ [ 8/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/13] Last Week's Request Trends...\n",
      "   üîó [ 9/13] Post ID: 1oteiak\n",
      "   üìÖ [ 9/13] Monday, November 10, 2025 | üïê 03:00:16 PM \n",
      "   üìù [ 9/13] 1998 chars\n",
      "   üñºÔ∏è [ 9/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 9/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/13] Since I dug my ferrari box out for the 5 pack post a couple ...\n",
      "   üîó [10/13] Post ID: 1oy8vic\n",
      "   üìÖ [10/13] Sunday, November 16, 2025 | üïê 02:30:53 AM \n",
      "   üñºÔ∏è [10/13] CAROUSEL (18) - 18 URLs!\n",
      "   ‚úÖ [10/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [11/13] Laitan STEAM-peliavaimia jakoon j√§lleen, ennakkoilmoitus!...\n",
      "   üîó [11/13] Post ID: 1ox7k01\n",
      "   üìÖ [11/13] Friday, November 14, 2025 | üïê 09:26:27 PM \n",
      "   üìù [11/13] 1924 chars\n",
      "   ‚ûñ [11/13] No visuals\n",
      "   ‚úÖ [11/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [12/13] Today‚Äôs Game, what‚Äôs everyone‚Äôs favourite PS1 Star Wars Game...\n",
      "   üîó [12/13] Post ID: 1pdscj6\n",
      "   üìÖ [12/13] Thursday, December 04, 2025 | üïê 06:58:19 AM \n",
      "   üñºÔ∏è [12/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [12/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [13/13] My Epic Universe VIP exclusive tour review...\n",
      "   üîó [13/13] Post ID: 1p1jiyx\n",
      "   üìÖ [13/13] Wednesday, November 19, 2025 | üïê 09:54:12 PM \n",
      "   üìù [13/13] 1981 chars\n",
      "   üñºÔ∏è [13/13] CAROUSEL (12) - 12 URLs!\n",
      "   ‚úÖ [13/13] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 13/13 posts ‚Üí ../data/reddit/oban_star_racers_top.csv\n",
      "\n",
      "‚úÖ DONE! 13 posts saved ‚Üí ../data/reddit/oban_star_racers_top.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50):\n",
    "    \"\"\"üî• FIXED: Uses reference code's URL - gets 20-100+ posts!\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t=month&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: {search_url.split('q=')[1][:50]}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50):\n",
    "    \"\"\"MAIN FUNCTION - FULL PROGRESS TRACKING!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                        'post_date': format_post_date(post_info.get('created_utc'))[0],\n",
    "                        'post_time': format_post_date(post_info.get('created_utc'))[1]\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_data['post_date']} | üïê {post_data['post_time'][:12]}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                        \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'post_date', 'post_time', 'text_length', 'post_description', \n",
    "        'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - FULL PROGRESS TRACKING!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    limit_input = input(\"How many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(\"\\nFilters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit)\n",
    "    \n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb1de7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3Ô∏è‚É£ FIX ADD TIME_AGO\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb656832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT POST EXTRACTOR - WITH TIME_AGO COLUMN!\n",
      "============================================================\n",
      "\n",
      "Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "üî• Scraping 13 'oban star racers' TOP posts...\n",
      "üì° Fetching EXACTLY 13 top posts...\n",
      "üîç Fetching UP TO 13 top posts for 'oban star racers'...\n",
      "   üì° API: oban%20star%20racers&sort=top&limit=100&t=month&ty...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 13/13 top posts loaded!\n",
      "‚úÖ Saved 13 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 13 posts ‚Üí ../data/reddit/oban_star_racers_top.csv\n",
      "====================================================================================================\n",
      "üîç [ 1/13] [OT] According to a press release on the RLL website release...\n",
      "   üîó [ 1/13] Post ID: 1p5f58b\n",
      "   üìÖ [ 1/13] Monday, November 24, 2025 | üïê 12:56:55 PM  | ‚è∞ 2 weeks 2 days ago\n",
      "   üñºÔ∏è [ 1/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 1/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/13] According to the RLL Racing website who published the announ...\n",
      "   üîó [ 2/13] Post ID: 1p5evj3\n",
      "   üìÖ [ 2/13] Monday, November 24, 2025 | üïê 12:41:56 PM  | ‚è∞ 2 weeks 2 days ago\n",
      "   üñºÔ∏è [ 2/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 2/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/13] IAE 2955 Ship Value Quiksheet...\n",
      "   üîó [ 3/13] Post ID: 1ovlk3e\n",
      "   üìÖ [ 3/13] Thursday, November 13, 2025 | üïê 12:44:24 AM  | ‚è∞ 4 weeks ago\n",
      "   üìù [ 3/13] 402 chars\n",
      "   üñºÔ∏è [ 3/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 3/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/13] Now this is podracing!...\n",
      "   üîó [ 4/13] Post ID: 1oxclc3\n",
      "   üìÖ [ 4/13] Saturday, November 15, 2025 | üïê 12:50:43 AM  | ‚è∞ 3 weeks 5 days ago\n",
      "   üìù [ 4/13] 46 chars\n",
      "   üñºÔ∏è [ 4/13] CAROUSEL (3) - 3 URLs!\n",
      "   ‚úÖ [ 4/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/13] Annan STEAM-peliavaimia ilmaisiksi, tule hakemaan omasi!...\n",
      "   üîó [ 5/13] Post ID: 1oypk3n\n",
      "   üìÖ [ 5/13] Sunday, November 16, 2025 | üïê 05:09:06 PM  | ‚è∞ 3 weeks 3 days ago\n",
      "   üìù [ 5/13] 1920 chars\n",
      "   ‚ûñ [ 5/13] No visuals\n",
      "   ‚úÖ [ 5/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/13] Crash 4 It‚Äôs About Time Trials, The 11th Dimension: Snow Way...\n",
      "   üîó [ 6/13] Post ID: 1pfthbc\n",
      "   üìÖ [ 6/13] Saturday, December 06, 2025 | üïê 05:35:25 PM  | ‚è∞ 4 days ago\n",
      "   üìù [ 6/13] 127 chars\n",
      "   üñºÔ∏è [ 6/13] VIDEO (1) - 1 URLs!\n",
      "   ‚úÖ [ 6/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/13] My Epic Universe VIP exclusive tour review...\n",
      "   üîó [ 7/13] Post ID: 1p1c6xs\n",
      "   üìÖ [ 7/13] Wednesday, November 19, 2025 | üïê 05:25:35 PM  | ‚è∞ 3 weeks ago\n",
      "   üìù [ 7/13] 1982 chars\n",
      "   üñºÔ∏è [ 7/13] CAROUSEL (11) - 11 URLs!\n",
      "   ‚úÖ [ 7/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/13] My Top 50 PS1 Games Ranked...\n",
      "   üîó [ 8/13] Post ID: 1p8bp2b\n",
      "   üìÖ [ 8/13] Thursday, November 27, 2025 | üïê 09:04:45 PM  | ‚è∞ 1 week 6 days ago\n",
      "   üìù [ 8/13] 1951 chars\n",
      "   ‚ûñ [ 8/13] No visuals\n",
      "   ‚úÖ [ 8/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/13] Last Week's Request Trends...\n",
      "   üîó [ 9/13] Post ID: 1oteiak\n",
      "   üìÖ [ 9/13] Monday, November 10, 2025 | üïê 03:00:16 PM  | ‚è∞ 1 month 2 days ago\n",
      "   üìù [ 9/13] 1998 chars\n",
      "   üñºÔ∏è [ 9/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 9/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/13] Since I dug my ferrari box out for the 5 pack post a couple ...\n",
      "   üîó [10/13] Post ID: 1oy8vic\n",
      "   üìÖ [10/13] Sunday, November 16, 2025 | üïê 02:30:53 AM  | ‚è∞ 3 weeks 3 days ago\n",
      "   üñºÔ∏è [10/13] CAROUSEL (18) - 18 URLs!\n",
      "   ‚úÖ [10/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [11/13] Laitan STEAM-peliavaimia jakoon j√§lleen, ennakkoilmoitus!...\n",
      "   üîó [11/13] Post ID: 1ox7k01\n",
      "   üìÖ [11/13] Friday, November 14, 2025 | üïê 09:26:27 PM  | ‚è∞ 3 weeks 5 days ago\n",
      "   üìù [11/13] 1924 chars\n",
      "   ‚ûñ [11/13] No visuals\n",
      "   ‚úÖ [11/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [12/13] Today‚Äôs Game, what‚Äôs everyone‚Äôs favourite PS1 Star Wars Game...\n",
      "   üîó [12/13] Post ID: 1pdscj6\n",
      "   üìÖ [12/13] Thursday, December 04, 2025 | üïê 06:58:19 AM  | ‚è∞ 6 days ago\n",
      "   üñºÔ∏è [12/13] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [12/13] COMPLETE ‚úì\n",
      "\n",
      "üîç [13/13] My Epic Universe VIP exclusive tour review...\n",
      "   üîó [13/13] Post ID: 1p1jiyx\n",
      "   üìÖ [13/13] Wednesday, November 19, 2025 | üïê 09:54:12 PM  | ‚è∞ 3 weeks ago\n",
      "   üìù [13/13] 1981 chars\n",
      "   üñºÔ∏è [13/13] CAROUSEL (12) - 12 URLs!\n",
      "   ‚úÖ [13/13] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 13/13 posts ‚Üí ../data/reddit/oban_star_racers_top.csv\n",
      "üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\n",
      "\n",
      "‚úÖ DONE! 13 posts saved ‚Üí ../data/reddit/oban_star_racers_top.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50):\n",
    "    \"\"\"üî• FIXED: Uses reference code's URL - gets 20-100+ posts!\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t=month&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: {search_url.split('q=')[1][:50]}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• NEW: Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        # Parse the full datetime string\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        # Calculate components\n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        # Build readable string\n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:  # Only show hours if no larger units\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50):\n",
    "    \"\"\"MAIN FUNCTION - FULL PROGRESS + TIME_AGO COLUMN!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                        \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - WITH TIME_AGO COLUMN!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    limit_input = input(\"How many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(\"\\nFilters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit)\n",
    "    \n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c6f5f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4Ô∏è‚É£ FIX ADD PERIOD FILTER\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbde2a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT POST EXTRACTOR - EXACT HTML PERIOD FILTER!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: Past year ‚Üí API t=year\n",
      "\n",
      "üî• Scraping 2 'oban star racers' TOP posts...\n",
      "   ‚è∞ Time filter: Past year\n",
      "üì° Fetching EXACTLY 2 top posts (Period: Past year)...\n",
      "üîç Fetching UP TO 2 top posts for 'oban star racers'...\n",
      "   ‚è∞ Time filter: Past year\n",
      "   üì° API: q=oban%20star%20racers&sort=top&t=year...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 2/2 top posts loaded!\n",
      "‚úÖ Saved 2 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 2 posts ‚Üí ../data/reddit/oban_star_racers_top_past_year.csv\n",
      "====================================================================================================\n",
      "üîç [ 1/2] I may have missed my calling, but I got a cool watch....\n",
      "   üîó [ 1/2] Post ID: 1mfi7na\n",
      "   üìÖ [ 1/2] Saturday, August 02, 2025 | üïê 07:06:03 AM  | ‚è∞ 4 months 1 week 4 days ago\n",
      "   üìù [ 1/2] 997 chars\n",
      "   üñºÔ∏è [ 1/2] CAROUSEL (3) - 3 URLs!\n",
      "   ‚úÖ [ 1/2] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/2] Extreme Target Haul! 47 games total...\n",
      "   üîó [ 2/2] Post ID: 1im9xrb\n",
      "   üìÖ [ 2/2] Monday, February 10, 2025 | üïê 05:20:18 PM  | ‚è∞ 10 months 2 days ago\n",
      "   üìù [ 2/2] 247 chars\n",
      "   üñºÔ∏è [ 2/2] CAROUSEL (4) - 4 URLs!\n",
      "   ‚úÖ [ 2/2] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 2/2 posts ‚Üí ../data/reddit/oban_star_racers_top_past_year.csv\n",
      "üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,period_filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\n",
      "\n",
      "‚úÖ DONE! 2 posts saved ‚Üí ../data/reddit/oban_star_racers_top_past_year.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - FULL PROGRESS + TIME_AGO + EXACT HTML PERIOD!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,period_filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - EXACT HTML DROPDOWN MATCH!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - EXACT HTML PERIOD FILTER!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    limit_input = input(\"How many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # üî• EXACT HTML DROPDOWN for Relevance/Top/Comments\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}_{period_filename}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f60e6e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5Ô∏è‚É£ STABLE SCRIPT V1 (IMAGES no file format)\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48730ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT POST EXTRACTOR - EXACT HTML PERIOD FILTER!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: Past year ‚Üí API t=year\n",
      "\n",
      "üî• Scraping 15 'oban star racers' TOP posts...\n",
      "   ‚è∞ Time filter: Past year\n",
      "üì° Fetching EXACTLY 15 top posts (Period: Past year)...\n",
      "üîç Fetching UP TO 15 top posts for 'oban star racers'...\n",
      "   ‚è∞ Time filter: Past year\n",
      "   üì° API: q=oban%20star%20racers&sort=top&t=year...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 15/15 top posts loaded!\n",
      "‚úÖ Saved 15 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 15 posts ‚Üí ../data/reddit/oban_star_racers_top_past_year.csv\n",
      "====================================================================================================\n",
      "üîç [ 1/15] I may have missed my calling, but I got a cool watch....\n",
      "   üîó [ 1/15] Post ID: 1mfi7na\n",
      "   üìÖ [ 1/15] Saturday, August 02, 2025 | üïê 07:06:03 AM  | ‚è∞ 4 months 1 week 4 days ago\n",
      "   üìù [ 1/15] 997 chars\n",
      "   üñºÔ∏è [ 1/15] CAROUSEL (3) - 3 URLs!\n",
      "   ‚úÖ [ 1/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/15] Extreme Target Haul! 47 games total...\n",
      "   üîó [ 2/15] Post ID: 1im9xrb\n",
      "   üìÖ [ 2/15] Monday, February 10, 2025 | üïê 05:20:18 PM  | ‚è∞ 10 months 2 days ago\n",
      "   üìù [ 2/15] 247 chars\n",
      "   üñºÔ∏è [ 2/15] CAROUSEL (4) - 4 URLs!\n",
      "   ‚úÖ [ 2/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/15] [toys] How LEGO lost its innocence and became an arms manufa...\n",
      "   üîó [ 3/15] Post ID: 1idosk4\n",
      "   üìÖ [ 3/15] Thursday, January 30, 2025 | üïê 03:58:12 PM  | ‚è∞ 10 months 2 weeks 6 days ago\n",
      "   üìù [ 3/15] 1832 chars\n",
      "   ‚ûñ [ 3/15] No visuals\n",
      "   ‚úÖ [ 3/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/15] GameCube Dream Kiosk Acquired!...\n",
      "   üîó [ 4/15] Post ID: 1krn97i\n",
      "   üìÖ [ 4/15] Wednesday, May 21, 2025 | üïê 04:41:21 AM  | ‚è∞ 6 months 3 weeks ago\n",
      "   üñºÔ∏è [ 4/15] CAROUSEL (4) - 4 URLs!\n",
      "   ‚úÖ [ 4/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/15] [OT] According to a press release on the RLL website release...\n",
      "   üîó [ 5/15] Post ID: 1p5f58b\n",
      "   üìÖ [ 5/15] Monday, November 24, 2025 | üïê 12:56:55 PM  | ‚è∞ 2 weeks 2 days ago\n",
      "   üñºÔ∏è [ 5/15] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 5/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/15] Post Your Tested Games ‚Äì Compatibility or Performance on Swi...\n",
      "   üîó [ 6/15] Post ID: 1l49teu\n",
      "   üìÖ [ 6/15] Thursday, June 05, 2025 | üïê 10:32:54 PM  | ‚è∞ 6 months 1 week 6 days ago\n",
      "   üìù [ 6/15] 1959 chars\n",
      "   ‚ûñ [ 6/15] No visuals\n",
      "   ‚úÖ [ 6/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/15] Which great 3DS game is missing?...\n",
      "   üîó [ 7/15] Post ID: 1l1lynh\n",
      "   üìÖ [ 7/15] Monday, June 02, 2025 | üïê 06:06:55 PM  | ‚è∞ 6 months 1 week 2 days ago\n",
      "   üñºÔ∏è [ 7/15] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 7/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/15] Star Wars Episode 1 racer...\n",
      "   üîó [ 8/15] Post ID: 1ij8vaj\n",
      "   üìÖ [ 8/15] Thursday, February 06, 2025 | üïê 07:01:05 PM  | ‚è∞ 10 months 1 week 6 days ago\n",
      "   üñºÔ∏è [ 8/15] CAROUSEL (8) - 8 URLs!\n",
      "   ‚úÖ [ 8/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/15] I think I robbed GameStop...\n",
      "   üîó [ 9/15] Post ID: 1nl5c4q\n",
      "   üìÖ [ 9/15] Friday, September 19, 2025 | üïê 04:38:05 PM  | ‚è∞ 2 months 3 weeks 5 days ago\n",
      "   üìù [ 9/15] 642 chars\n",
      "   üñºÔ∏è [ 9/15] IMAGE (1) - 1 URLs!\n",
      "   ‚úÖ [ 9/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/15] My Sandershelf ‚ô°...\n",
      "   üîó [10/15] Post ID: 1hol6am\n",
      "   üìÖ [10/15] Sunday, December 29, 2024 | üïê 02:48:46 AM  | ‚è∞ 11 months 2 weeks 3 days ago\n",
      "   üìù [10/15] 605 chars\n",
      "   üñºÔ∏è [10/15] CAROUSEL (14) - 14 URLs!\n",
      "   ‚úÖ [10/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [11/15] All my beautiful hounds....\n",
      "   üîó [11/15] Post ID: 1iqosx1\n",
      "   üìÖ [11/15] Sunday, February 16, 2025 | üïê 10:39:53 AM  | ‚è∞ 9 months 3 weeks 3 days ago\n",
      "   üìù [11/15] 211 chars\n",
      "   üñºÔ∏è [11/15] CAROUSEL (3) - 3 URLs!\n",
      "   ‚úÖ [11/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [12/15] PC Ports, Decompilations, Recompilations Remakes, Demakes, F...\n",
      "   üîó [12/15] Post ID: 1hlbp7j\n",
      "   üìÖ [12/15] Tuesday, December 24, 2024 | üïê 12:27:58 PM  | ‚è∞ 11 months 3 weeks 1 day ago\n",
      "   üìù [12/15] 1213 chars\n",
      "   ‚ûñ [12/15] No visuals\n",
      "   ‚úÖ [12/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [13/15] Vilho always finds the weirdest places to chill at...\n",
      "   üîó [13/15] Post ID: 1ij5vc1\n",
      "   üìÖ [13/15] Thursday, February 06, 2025 | üïê 04:59:52 PM  | ‚è∞ 10 months 1 week 6 days ago\n",
      "   üìù [13/15] 51 chars\n",
      "   üñºÔ∏è [13/15] CAROUSEL (6) - 6 URLs!\n",
      "   ‚úÖ [13/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [14/15] Comparing 3 monthly retro game subscription boxes...\n",
      "   üîó [14/15] Post ID: 1oikhws\n",
      "   üìÖ [14/15] Tuesday, October 28, 2025 | üïê 09:28:53 PM  | ‚è∞ 1 month 1 week 1 day ago\n",
      "   üìù [14/15] 1789 chars\n",
      "   üñºÔ∏è [14/15] CAROUSEL (13) - 13 URLs!\n",
      "   ‚úÖ [14/15] COMPLETE ‚úì\n",
      "\n",
      "üîç [15/15] A discourse on The Flintstones Movie, and how it was misunde...\n",
      "   üîó [15/15] Post ID: 1lf5tfl\n",
      "   üìÖ [15/15] Thursday, June 19, 2025 | üïê 10:42:18 AM  | ‚è∞ 5 months 3 weeks 6 days ago\n",
      "   üìù [15/15] 1971 chars\n",
      "   ‚ûñ [15/15] No visuals\n",
      "   ‚úÖ [15/15] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 15/15 posts ‚Üí ../data/reddit/oban_star_racers_top_past_year.csv\n",
      "üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,period_filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\n",
      "\n",
      "‚úÖ DONE! 15 posts saved ‚Üí ../data/reddit/oban_star_racers_top_past_year.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - FULL PROGRESS + TIME_AGO + EXACT HTML PERIOD!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,period_filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - EXACT HTML DROPDOWN MATCH!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - EXACT HTML PERIOD FILTER!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    limit_input = input(\"How many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # üî• EXACT HTML DROPDOWN for Relevance/Top/Comments\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}_{period_filename}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe601b",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ FIX REORDER THE INPUT KEYWORD - FILTER - PERIOD - LIMIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f50a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT POST EXTRACTOR - EXACT HTML PERIOD FILTER!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: Past year ‚Üí API t=year\n",
      "\n",
      "üî• Scraping 5 'oban star racers' RELEVANCE posts...\n",
      "   ‚è∞ Time filter: Past year\n",
      "üì° Fetching EXACTLY 5 relevance posts (Period: Past year)...\n",
      "üîç Fetching UP TO 5 relevance posts for 'oban star racers'...\n",
      "   ‚è∞ Time filter: Past year\n",
      "   üì° API: q=oban%20star%20racers&sort=relevance&t=year...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 5/5 relevance posts loaded!\n",
      "‚úÖ Saved 5 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 5 posts ‚Üí ../data/reddit/oban_star_racers_relevance_past_year.csv\n",
      "====================================================================================================\n",
      "üîç [ 1/5] Oban Star Racers Chance to Shine Update...\n",
      "   üîó [ 1/5] Post ID: 1oiumb2\n",
      "   üìÖ [ 1/5] Wednesday, October 29, 2025 | üïê 04:51:36 AM  | ‚è∞ 1 month 1 week ago\n",
      "   üìù [ 1/5] 1994 chars\n",
      "   üñºÔ∏è [ 1/5] CAROUSEL (14) - 14 URLs!\n",
      "   ‚úÖ [ 1/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/5] 20 Years Ago Today on December 2, 2005, the official trailer...\n",
      "   üîó [ 2/5] Post ID: 1pcl5km\n",
      "   üìÖ [ 2/5] Tuesday, December 02, 2025 | üïê 10:30:11 PM  | ‚è∞ 1 week 1 day ago\n",
      "   üìù [ 2/5] 9 chars\n",
      "   üñºÔ∏è [ 2/5] VIDEO (1) - 1 URLs!\n",
      "   ‚úÖ [ 2/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/5] Oban Star Racers Chance to Shine ‚ù§Ô∏è‚Äçüî• Alwas Traps ü™§ Please S...\n",
      "   üîó [ 3/5] Post ID: 1pdpc6z\n",
      "   üìÖ [ 3/5] Thursday, December 04, 2025 | üïê 04:24:36 AM  | ‚è∞ 6 days ago\n",
      "   üìù [ 3/5] 1089 chars\n",
      "   üñºÔ∏è [ 3/5] CAROUSEL (7) - 7 URLs!\n",
      "   ‚úÖ [ 3/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/5] Oban Star Racers Chance to Shine Sneak Peak of Alwas...\n",
      "   üîó [ 4/5] Post ID: 1oivknc\n",
      "   üìÖ [ 4/5] Wednesday, October 29, 2025 | üïê 05:43:51 AM  | ‚è∞ 1 month 1 week ago\n",
      "   üìù [ 4/5] 225 chars\n",
      "   üñºÔ∏è [ 4/5] CAROUSEL (2) - 2 URLs!\n",
      "   ‚úÖ [ 4/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/5] Oban Star-Racers extended opening intro (2005)....\n",
      "   üîó [ 5/5] Post ID: 1pf2m6v\n",
      "   üìÖ [ 5/5] Friday, December 05, 2025 | üïê 07:24:56 PM  | ‚è∞ 5 days ago\n",
      "   üìù [ 5/5] 104 chars\n",
      "   üñºÔ∏è [ 5/5] VIDEO (1) - 1 URLs!\n",
      "   ‚úÖ [ 5/5] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 5/5 posts ‚Üí ../data/reddit/oban_star_racers_relevance_past_year.csv\n",
      "üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,period_filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\n",
      "\n",
      "‚úÖ DONE! 5 posts saved ‚Üí ../data/reddit/oban_star_racers_relevance_past_year.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - FULL PROGRESS + TIME_AGO + EXACT HTML PERIOD!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,period_filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - NEW ORDER: keyword ‚Üí filter ‚Üí period ‚Üí limit\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - EXACT HTML PERIOD FILTER!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}_{period_filename}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7963b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 7Ô∏è‚É£ FIX POST_VISUAL the extraction of images in high quality but without file format\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6bfd43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT POST EXTRACTOR + AUTO VISUAL DOWNLOAD!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: Past year ‚Üí API t=year\n",
      "\n",
      "üî• Scraping 10 'oban star racers' TOP posts + AUTO DOWNLOAD!\n",
      "   ‚è∞ Time filter: Past year\n",
      "üì° Fetching EXACTLY 10 top posts (Period: Past year)...\n",
      "üîç Fetching UP TO 10 top posts for 'oban star racers'...\n",
      "   ‚è∞ Time filter: Past year\n",
      "   üì° API: q=oban%20star%20racers&sort=top&t=year...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 10/10 top posts loaded!\n",
      "‚úÖ Saved 10 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/oban_star_racers_top_past_year.csv\n",
      "üíæ DOWNLOADING to ‚Üí ../data/visuals/oban_star_racers_top_past_year\n",
      "====================================================================================================\n",
      "üîç [ 1/10] I may have missed my calling, but I got a cool watch....\n",
      "   üîó [ 1/10] Post ID: 1mfi7na\n",
      "   üìÖ [ 1/10] Saturday, August 02, 2025 | üïê 07:06:03 AM  | ‚è∞ 4 months 1 week 4 days ago\n",
      "   üìù [ 1/10] 997 chars\n",
      "   üñºÔ∏è [ 1/10] VIDEO (1) - 48 URLs!\n",
      "   üíæ Downloading ‚Üí ../data/visuals/oban_star_racers_top_past_year/...\n",
      "   üíæ [IMAGE] oban_star_racers_top_past_year_1_img (1459.2KB)\n",
      "   ‚úÖ [ 1/10] 1 files saved!\n",
      "   üéâ [ 1/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/10] Extreme Target Haul! 47 games total...\n",
      "   üîó [ 2/10] Post ID: 1im9xrb\n",
      "   üìÖ [ 2/10] Monday, February 10, 2025 | üïê 05:20:18 PM  | ‚è∞ 10 months 2 days ago\n",
      "   üìù [ 2/10] 247 chars\n",
      "   üñºÔ∏è [ 2/10] VIDEO (1) - 64 URLs!\n",
      "   üíæ Downloading ‚Üí ../data/visuals/oban_star_racers_top_past_year/...\n",
      "   üíæ [IMAGE] oban_star_racers_top_past_year_2_img (2580.1KB)\n",
      "   ‚úÖ [ 2/10] 1 files saved!\n",
      "   üéâ [ 2/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/10] [toys] How LEGO lost its innocence and became an arms manufa...\n",
      "   üîó [ 3/10] Post ID: 1idosk4\n",
      "   üìÖ [ 3/10] Thursday, January 30, 2025 | üïê 03:58:12 PM  | ‚è∞ 10 months 2 weeks 6 days ago\n",
      "   üìù [ 3/10] 1832 chars\n",
      "   ‚ûñ [ 3/10] No visuals\n",
      "   üéâ [ 3/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/10] GameCube Dream Kiosk Acquired!...\n",
      "   üîó [ 4/10] Post ID: 1krn97i\n",
      "   üìÖ [ 4/10] Wednesday, May 21, 2025 | üïê 04:41:21 AM  | ‚è∞ 6 months 3 weeks ago\n",
      "   üñºÔ∏è [ 4/10] VIDEO (1) - 64 URLs!\n",
      "   üíæ Downloading ‚Üí ../data/visuals/oban_star_racers_top_past_year/...\n",
      "   üíæ [IMAGE] oban_star_racers_top_past_year_4_img (2100.1KB)\n",
      "   ‚úÖ [ 4/10] 1 files saved!\n",
      "   üéâ [ 4/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/10] [OT] According to a press release on the RLL website release...\n",
      "   üîó [ 5/10] Post ID: 1p5f58b\n",
      "   üìÖ [ 5/10] Monday, November 24, 2025 | üïê 12:56:55 PM  | ‚è∞ 2 weeks 2 days ago\n",
      "   üñºÔ∏è [ 5/10] IMAGE (1) - 1 URLs!\n",
      "   üíæ Downloading ‚Üí ../data/visuals/oban_star_racers_top_past_year/...\n",
      "   üíæ [IMAGE] oban_star_racers_top_past_year_5_img (2026.6KB)\n",
      "   ‚úÖ [ 5/10] 1 files saved!\n",
      "   üéâ [ 5/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/10] Post Your Tested Games ‚Äì Compatibility or Performance on Swi...\n",
      "   üîó [ 6/10] Post ID: 1l49teu\n",
      "   üìÖ [ 6/10] Thursday, June 05, 2025 | üïê 10:32:54 PM  | ‚è∞ 6 months 1 week 6 days ago\n",
      "   üìù [ 6/10] 1959 chars\n",
      "   ‚ûñ [ 6/10] No visuals\n",
      "   üéâ [ 6/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/10] Which great 3DS game is missing?...\n",
      "   üîó [ 7/10] Post ID: 1l1lynh\n",
      "   üìÖ [ 7/10] Monday, June 02, 2025 | üïê 06:06:55 PM  | ‚è∞ 6 months 1 week 2 days ago\n",
      "   üñºÔ∏è [ 7/10] IMAGE (1) - 1 URLs!\n",
      "   üíæ Downloading ‚Üí ../data/visuals/oban_star_racers_top_past_year/...\n",
      "   üíæ [IMAGE] oban_star_racers_top_past_year_7_img (2580.0KB)\n",
      "   ‚úÖ [ 7/10] 1 files saved!\n",
      "   üéâ [ 7/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/10] Star Wars Episode 1 racer...\n",
      "   üîó [ 8/10] Post ID: 1ij8vaj\n",
      "   üìÖ [ 8/10] Thursday, February 06, 2025 | üïê 07:01:05 PM  | ‚è∞ 10 months 1 week 6 days ago\n",
      "   üñºÔ∏è [ 8/10] VIDEO (1) - 128 URLs!\n",
      "   üíæ Downloading ‚Üí ../data/visuals/oban_star_racers_top_past_year/...\n",
      "   üíæ [IMAGE] oban_star_racers_top_past_year_8_img (446.2KB)\n",
      "   ‚úÖ [ 8/10] 1 files saved!\n",
      "   üéâ [ 8/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/10] I think I robbed GameStop...\n",
      "   üîó [ 9/10] Post ID: 1nl5c4q\n",
      "   üìÖ [ 9/10] Friday, September 19, 2025 | üïê 04:38:05 PM  | ‚è∞ 2 months 3 weeks 5 days ago\n",
      "   üìù [ 9/10] 642 chars\n",
      "   üñºÔ∏è [ 9/10] IMAGE (1) - 1 URLs!\n",
      "   üíæ Downloading ‚Üí ../data/visuals/oban_star_racers_top_past_year/...\n",
      "   üíæ [IMAGE] oban_star_racers_top_past_year_9_img (5407.2KB)\n",
      "   ‚úÖ [ 9/10] 1 files saved!\n",
      "   üéâ [ 9/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/10] My Sandershelf ‚ô°...\n",
      "   üîó [10/10] Post ID: 1hol6am\n",
      "   üìÖ [10/10] Sunday, December 29, 2024 | üïê 02:48:46 AM  | ‚è∞ 11 months 2 weeks 4 days ago\n",
      "   üìù [10/10] 605 chars\n",
      "   üñºÔ∏è [10/10] VIDEO (1) - 224 URLs!\n",
      "   üíæ Downloading ‚Üí ../data/visuals/oban_star_racers_top_past_year/...\n",
      "   üíæ [IMAGE] oban_star_racers_top_past_year_10_img (1448.2KB)\n",
      "   ‚úÖ [10/10] 1 files saved!\n",
      "   üéâ [10/10] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/oban_star_racers_top_past_year.csv\n",
      "üíæ ALL VISUALS ‚Üí ../data/visuals/oban_star_racers_top_past_year/\n",
      "üìã NEW column: downloaded_files\n",
      "\n",
      "‚úÖ DONE! 10 posts + visuals ‚Üí ../data/visuals/oban_star_racers_top_past_year/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD FUNCTION\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals with EXACT naming convention to ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Try each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            if '_img_' in base_filename:\n",
    "                filename = f\"{base_filename}_img_{seq_str}\"\n",
    "            elif '_vid_' in base_filename:\n",
    "                filename = f\"{base_filename}_vid_{seq_str}\"\n",
    "            else:\n",
    "                filename = f\"{base_filename}_img_{seq_str}\"\n",
    "            \n",
    "            filepath = os.path.join(visual_folder, filename)\n",
    "            \n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"   üìÅ SKIP {filename}\")\n",
    "                downloaded_files.append(filename)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                resp = requests.get(url, headers=headers_browser, timeout=15, stream=True)\n",
    "                if resp.status_code == 200:\n",
    "                    content_type = resp.headers.get('content-type', '').lower()\n",
    "                    size = len(resp.content)\n",
    "                    \n",
    "                    if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}] {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Next sequence\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è  Empty/Invalid: {size}B ({content_type[:20]})\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Error: {str(e)[:30]}\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Try each URL once\n",
    "        for url in visual_urls:\n",
    "            try:\n",
    "                resp = requests.get(url, headers=headers_browser, timeout=15, stream=True)\n",
    "                if resp.status_code == 200:\n",
    "                    content_type = resp.headers.get('content-type', '').lower()\n",
    "                    size = len(resp.content)\n",
    "                    \n",
    "                    if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                        if 'video' in content_type:\n",
    "                            filename = f\"{base_filename}_vid\"\n",
    "                        else:\n",
    "                            filename = f\"{base_filename}_img\"\n",
    "                        \n",
    "                        filepath = os.path.join(visual_folder, filename)\n",
    "                        \n",
    "                        if os.path.exists(filepath):\n",
    "                            print(f\"   üìÅ SKIP {filename}\")\n",
    "                            downloaded_files.append(filename)\n",
    "                            break\n",
    "                        \n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}] {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è  Empty/Invalid: {size}B ({content_type[:20]})\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Error: {str(e)[:30]}\")\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - FULL PROGRESS + TIME_AGO + AUTO DOWNLOAD TO ../data/visuals/\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + AUTO DOWNLOAD\n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        \n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                        print(f\"   üíæ Downloading ‚Üí {VISUALS_FOLDER}/...\")\n",
    "                        \n",
    "                        # üî• AUTOMATIC DOWNLOAD with EXACT naming!\n",
    "                        downloaded_files = download_visual_auto(\n",
    "                            post_number, vtype, visual_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                        post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                        \n",
    "                        print(f\"   ‚úÖ [{progress}] {len(downloaded_files)} files saved!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.0)  # Increased delay for downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üìã NEW column: downloaded_files\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - NOW WITH AUTO DOWNLOAD!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR + AUTO VISUAL DOWNLOAD!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts + AUTO DOWNLOAD!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fb79b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 8Ô∏è‚É£ FIX POST_VISUAL file format downloaded & fix direct links & remove broken links\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef4b7521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT!)\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: All time ‚Üí API t=all\n",
      "\n",
      "üî• Scraping 10 'oban star racers' TOP posts...\n",
      "‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions!\n",
      "   ‚è∞ Time filter: All time\n",
      "üì° Fetching EXACTLY 10 top posts (Period: All time)...\n",
      "üîç Fetching UP TO 10 top posts for 'oban star racers'...\n",
      "   ‚è∞ Time filter: All time\n",
      "   üì° API: q=oban%20star%20racers&sort=top&t=all...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 10/10 top posts loaded!\n",
      "‚úÖ Saved 10 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/oban_star_racers_top_all_time.csv\n",
      "üíæ DOWNLOADING to ‚Üí ../data/visuals/oban_star_racers_top_all_time\n",
      "====================================================================================================\n",
      "üîç [ 1/10] [OC] My new in box Star Wars Episode 1 racer Nintendo 64 con...\n",
      "   üîó [ 1/10] Post ID: usbmw2\n",
      "   üìÖ [ 1/10] Wednesday, May 18, 2022 | üïê 02:22:36 PM  | ‚è∞ 3 years 6 months 1 week ago\n",
      "   üñºÔ∏è [ 1/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_top_all_time_1_img.jpg (5858.2KB)\n",
      "   ‚úÖ [ 1/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 1/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/10] The pod racer crowds from Star Wars Episode 1 were actually ...\n",
      "   üîó [ 2/10] Post ID: m37xzg\n",
      "   üìÖ [ 2/10] Friday, March 12, 2021 | üïê 03:56:58 AM  | ‚è∞ 4 years 9 months 3 weeks 5 days ago\n",
      "   üñºÔ∏è [ 2/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_top_all_time_2_img.jpg (163.0KB)\n",
      "   ‚úÖ [ 2/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 2/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/10] An interesting glitch I found on Star Wars Racer back when I...\n",
      "   üîó [ 3/10] Post ID: hemi28\n",
      "   üìÖ [ 3/10] Tuesday, June 23, 2020 | üïê 10:37:12 PM  | ‚è∞ 5 years 5 months 2 weeks 1 day ago\n",
      "   üñºÔ∏è [ 3/10] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_top_all_time_3_vid.mp4 (9175.5KB)\n",
      "   ‚úÖ [ 3/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 3/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/10] 48 Hours, 250+ Steam Keys to Giveaway...\n",
      "   üîó [ 4/10] Post ID: oan4md\n",
      "   üìÖ [ 4/10] Wednesday, June 30, 2021 | üïê 04:07:03 AM  | ‚è∞ 4 years 5 months ago\n",
      "   üìù [ 4/10] 1973 chars\n",
      "   ‚ûñ [ 4/10] No visuals\n",
      "   üéâ [ 4/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/10] Which one of these deserves a modern remake?...\n",
      "   üîó [ 5/10] Post ID: 1fbd8oy\n",
      "   üìÖ [ 5/10] Saturday, September 07, 2024 | üïê 08:09:12 PM  | ‚è∞ 1 year 3 months 1 week 4 days ago\n",
      "   üìù [ 5/10] 193 chars\n",
      "   üñºÔ∏è [ 5/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_top_all_time_5_img.jpg (3622.6KB)\n",
      "   ‚úÖ [ 5/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 5/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/10] [Star Wars Episode 1: Racer] Good job, Tim...\n",
      "   üîó [ 6/10] Post ID: 8rzk6i\n",
      "   üìÖ [ 6/10] Monday, June 18, 2018 | üïê 03:38:01 PM  | ‚è∞ 7 years 5 months 2 days ago\n",
      "   ‚ûñ [ 6/10] No visuals\n",
      "   üéâ [ 6/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/10] Star Wars Episode I Racer coming to Switch and PS4...\n",
      "   üîó [ 7/10] Post ID: fpe4ui\n",
      "   üìÖ [ 7/10] Thursday, March 26, 2020 | üïê 05:07:38 PM  | ‚è∞ 5 years 8 months 2 weeks 6 days ago\n",
      "   ‚ûñ [ 7/10] No visuals\n",
      "   üéâ [ 7/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/10] So you're a gamer? Name every game....\n",
      "   üîó [ 8/10] Post ID: j068u1\n",
      "   üìÖ [ 8/10] Saturday, September 26, 2020 | üïê 03:42:54 PM  | ‚è∞ 5 years 2 months 1 week 4 days ago\n",
      "   üìù [ 8/10] 1926 chars\n",
      "   ‚ûñ [ 8/10] No visuals\n",
      "   üéâ [ 8/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/10] A very long and updated List of recommendable Coop Titles fo...\n",
      "   üîó [ 9/10] Post ID: dlx6od\n",
      "   üìÖ [ 9/10] Wednesday, October 23, 2019 | üïê 12:15:05 PM  | ‚è∞ 6 years 1 month 2 weeks ago\n",
      "   üìù [ 9/10] 1961 chars\n",
      "   ‚ûñ [ 9/10] No visuals\n",
      "   üéâ [ 9/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/10] In 1982 a man claiming to be a professional race car driver ...\n",
      "   üîó [10/10] Post ID: m0j25x\n",
      "   üìÖ [10/10] Monday, March 08, 2021 | üïê 05:11:30 PM  | ‚è∞ 4 years 9 months 4 weeks 2 days ago\n",
      "   üìù [10/10] 1900 chars\n",
      "   ‚ûñ [10/10] No visuals\n",
      "   üéâ [10/10] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/oban_star_racers_top_all_time.csv\n",
      "üíæ ALL VISUALS ‚Üí ../data/visuals/oban_star_racers_top_all_time/\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 10 posts + visuals ‚Üí ../data/visuals/oban_star_racers_top_all_time/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# üî• NEW: Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS ONLY\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    working_urls = []\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "    \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUTO DOWNLOAD\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                        \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE', \n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                        \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                            \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT!)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79527be9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 9Ô∏è‚É£ STABLE SCRIPT V2 (IMAGES HIGH QUALITY)\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d160e0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT!)\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: All time ‚Üí API t=all\n",
      "\n",
      "üî• Scraping 10 'luffy funny video' RELEVANCE posts...\n",
      "‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions!\n",
      "   ‚è∞ Time filter: All time\n",
      "üì° Fetching EXACTLY 10 relevance posts (Period: All time)...\n",
      "üîç Fetching UP TO 10 relevance posts for 'luffy funny video'...\n",
      "   ‚è∞ Time filter: All time\n",
      "   üì° API: q=luffy%20funny%20video&sort=relevance&t=all...\n",
      "‚ùå Search failed: HTTP 403\n",
      "‚ö†Ô∏è  Search failed ‚Üí Using sample data\n",
      "‚úÖ Created sample MAIN data (10 posts): ../data/reddit/luffy_funny_video_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/luffy_funny_video_relevance_all_time.csv\n",
      "üíæ DOWNLOADING to ‚Üí ../data/visuals/luffy_funny_video_relevance_all_time\n",
      "====================================================================================================\n",
      "üîç [ 1/10] Luffy_Funny_Video post 1...\n",
      "   ‚ùå [ 1/10] Invalid link - SKIPPED\n",
      "üîç [ 2/10] Luffy_Funny_Video post 2...\n",
      "   ‚ùå [ 2/10] Invalid link - SKIPPED\n",
      "üîç [ 3/10] Luffy_Funny_Video post 3...\n",
      "   ‚ùå [ 3/10] Invalid link - SKIPPED\n",
      "üîç [ 4/10] Luffy_Funny_Video post 4...\n",
      "   ‚ùå [ 4/10] Invalid link - SKIPPED\n",
      "üîç [ 5/10] Luffy_Funny_Video post 5...\n",
      "   ‚ùå [ 5/10] Invalid link - SKIPPED\n",
      "üîç [ 6/10] Luffy_Funny_Video post 6...\n",
      "   ‚ùå [ 6/10] Invalid link - SKIPPED\n",
      "üîç [ 7/10] Luffy_Funny_Video post 7...\n",
      "   ‚ùå [ 7/10] Invalid link - SKIPPED\n",
      "üîç [ 8/10] Luffy_Funny_Video post 8...\n",
      "   ‚ùå [ 8/10] Invalid link - SKIPPED\n",
      "üîç [ 9/10] Luffy_Funny_Video post 9...\n",
      "   ‚ùå [ 9/10] Invalid link - SKIPPED\n",
      "üîç [10/10] Luffy_Funny_Video post 10...\n",
      "   ‚ùå [10/10] Invalid link - SKIPPED\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/luffy_funny_video_relevance_all_time.csv\n",
      "üíæ ALL VISUALS ‚Üí ../data/visuals/luffy_funny_video_relevance_all_time/\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 10 posts + visuals ‚Üí ../data/visuals/luffy_funny_video_relevance_all_time/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# üî• NEW: Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS ONLY\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    working_urls = []\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "    \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUTO DOWNLOAD\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                        \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE', \n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                        \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                            \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT!)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47279513",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üîü STABLE SCRIPT V3 (IMAGES & GIFT & VIDEOS without audio)\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba6ac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT!)\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: All time ‚Üí API t=all\n",
      "\n",
      "üî• Scraping 10 'one piece funny video' RELEVANCE posts...\n",
      "‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions!\n",
      "   ‚è∞ Time filter: All time\n",
      "üì° Fetching EXACTLY 10 relevance posts (Period: All time)...\n",
      "üîç Fetching UP TO 10 relevance posts for 'one piece funny video'...\n",
      "   ‚è∞ Time filter: All time\n",
      "   üì° API: q=one%20piece%20funny%20video&sort=relevance&t=all...\n",
      "‚ùå Search failed: HTTP 403\n",
      "‚ö†Ô∏è  Search failed ‚Üí Using sample data\n",
      "‚úÖ Created sample MAIN data (10 posts): ../data/reddit/one_piece_funny_video_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/one_piece_funny_video_relevance_all_time.csv\n",
      "üíæ DOWNLOADING to ‚Üí ../data/visuals/one_piece_funny_video_relevance_all_time\n",
      "====================================================================================================\n",
      "üîç [ 1/10] One_Piece_Funny_Video post 1...\n",
      "   ‚ùå [ 1/10] Invalid link - SKIPPED\n",
      "üîç [ 2/10] One_Piece_Funny_Video post 2...\n",
      "   ‚ùå [ 2/10] Invalid link - SKIPPED\n",
      "üîç [ 3/10] One_Piece_Funny_Video post 3...\n",
      "   ‚ùå [ 3/10] Invalid link - SKIPPED\n",
      "üîç [ 4/10] One_Piece_Funny_Video post 4...\n",
      "   ‚ùå [ 4/10] Invalid link - SKIPPED\n",
      "üîç [ 5/10] One_Piece_Funny_Video post 5...\n",
      "   ‚ùå [ 5/10] Invalid link - SKIPPED\n",
      "üîç [ 6/10] One_Piece_Funny_Video post 6...\n",
      "   ‚ùå [ 6/10] Invalid link - SKIPPED\n",
      "üîç [ 7/10] One_Piece_Funny_Video post 7...\n",
      "   ‚ùå [ 7/10] Invalid link - SKIPPED\n",
      "üîç [ 8/10] One_Piece_Funny_Video post 8...\n",
      "   ‚ùå [ 8/10] Invalid link - SKIPPED\n",
      "üîç [ 9/10] One_Piece_Funny_Video post 9...\n",
      "   ‚ùå [ 9/10] Invalid link - SKIPPED\n",
      "üîç [10/10] One_Piece_Funny_Video post 10...\n",
      "   ‚ùå [10/10] Invalid link - SKIPPED\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/one_piece_funny_video_relevance_all_time.csv\n",
      "üíæ ALL VISUALS ‚Üí ../data/visuals/one_piece_funny_video_relevance_all_time/\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 10 posts + visuals ‚Üí ../data/visuals/one_piece_funny_video_relevance_all_time/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# üî• NEW: Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS ONLY\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    working_urls = []\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "    \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUTO DOWNLOAD\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                        \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE', \n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                        \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                            \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT!)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4d77c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üåü11üåü FIX DOWNLOAD AUDIO .m4a\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cb35cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (VIDEO+AUDIO + PROPER .EXT!)\n",
      "======================================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "üî• Scraping 5 'oban star racers' HOT posts...\n",
      "‚úÖ WORKING LINKS ONLY + VIDEO WITH AUDIO (.mp4) + PROPER .png/.gif extensions!\n",
      "‚úÖ pip install yt-dlp  # REQUIRED for video+audio!\n",
      "üì° Fetching EXACTLY 5 hot posts (Period: All time)...\n",
      "üîç Fetching UP TO 5 hot posts for 'oban star racers'...\n",
      "   üì° API: q=oban%20star%20racers&sort=hot&t=month...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 5/5 hot posts loaded!\n",
      "‚úÖ Saved 5 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 5 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ DOWNLOADING to ‚Üí ../data/visuals/oban_star_racers_hot_all_time\n",
      "====================================================================================================\n",
      "üîç [ 1/5] Respect Ondai (Oban Star Racers)...\n",
      "   üîó [ 1/5] Post ID: 1pjk4g8\n",
      "   üìÖ [ 1/5] Thursday, December 11, 2025 | üïê 01:59:47 AM  | ‚è∞ 1 hour ago\n",
      "   üìù [ 1/5] 1630 chars\n",
      "   ‚ûñ [ 1/5] No visuals\n",
      "   üéâ [ 1/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/5] Oban Star-Racers extended opening intro (2005)....\n",
      "   üîó [ 2/5] Post ID: 1pf2m6v\n",
      "   üìÖ [ 2/5] Friday, December 05, 2025 | üïê 07:24:56 PM  | ‚è∞ 5 days ago\n",
      "   üìù [ 2/5] 104 chars\n",
      "   üñºÔ∏è [ 2/5] Testing 1 candidate URLs...\n",
      "   üé¨ DETECTED REDDIT VIDEO POST ‚Üí Using yt-dlp (video+audio)\n",
      "   üé• yt-dlp: PERFECT video + audio ‚Üí oban_star_racers_hot_all_time_2_vid_with_audio.mp4\n",
      "   ‚ùå yt-dlp failed: WARNING: You have requested merging of multiple formats but ffmpeg is not installed. The formats won\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_2_vid.mp4\n",
      "   ‚úÖ [ 2/5] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 2/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/5] Oban Star Racers Chance to Shine ‚ù§Ô∏è‚Äçüî• Alwas Traps ü™§ Please S...\n",
      "   üîó [ 3/5] Post ID: 1pdpc6z\n",
      "   üìÖ [ 3/5] Thursday, December 04, 2025 | üïê 04:24:36 AM  | ‚è∞ 6 days ago\n",
      "   üìù [ 3/5] 1089 chars\n",
      "   üñºÔ∏è [ 3/5] Testing 112 candidate URLs...\n",
      "   üé¨ DETECTED REDDIT VIDEO POST ‚Üí Using yt-dlp (video+audio)\n",
      "   üé• yt-dlp: PERFECT video + audio ‚Üí oban_star_racers_hot_all_time_3_vid_with_audio.mp4\n",
      "   ‚ùå yt-dlp failed: ERROR: [generic] Unable to download webpage: HTTP Error 403: Blocked (caused by <HTTPError 403: Bloc\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_13.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_29.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_45.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_61.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_77.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.png...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_95.jpg\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.png...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_111.jpg\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.jpeg...\n",
      "   ‚úÖ [ 3/5] CAROUSEL (7) - 7 WORKING URLs!\n",
      "   üíæ 7 files saved!\n",
      "   üéâ [ 3/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/5] 20 Years Ago Today on December 2, 2005, the official trailer...\n",
      "   üîó [ 4/5] Post ID: 1pcl5km\n",
      "   üìÖ [ 4/5] Tuesday, December 02, 2025 | üïê 10:30:11 PM  | ‚è∞ 1 week 1 day ago\n",
      "   üìù [ 4/5] 9 chars\n",
      "   üñºÔ∏è [ 4/5] Testing 1 candidate URLs...\n",
      "   üé¨ DETECTED REDDIT VIDEO POST ‚Üí Using yt-dlp (video+audio)\n",
      "   üé• yt-dlp: PERFECT video + audio ‚Üí oban_star_racers_hot_all_time_4_vid_with_audio.mp4\n",
      "   ‚ùå yt-dlp failed: WARNING: You have requested merging of multiple formats but ffmpeg is not installed. The formats won\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_4_vid.mp4\n",
      "   ‚úÖ [ 4/5] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 4/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/5] Star Citizen The Cup - Origin M50 Turbo, Loved By Racers!...\n",
      "   üîó [ 5/5] Post ID: 1pgpnyd\n",
      "   üìÖ [ 5/5] Sunday, December 07, 2025 | üïê 07:33:10 PM  | ‚è∞ 3 days ago\n",
      "   üñºÔ∏è [ 5/5] Testing 1 candidate URLs...\n",
      "   ‚ùå [ 5/5] No working URLs found\n",
      "   üéâ [ 5/5] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 5/5 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ ALL VISUALS ‚Üí ../data/visuals/oban_star_racers_hot_all_time/\n",
      "‚úÖ post_visual = WORKING LINKS ONLY + VIDEO WITH AUDIO!\n",
      "\n",
      "‚úÖ DONE! 5 posts + visuals ‚Üí ../data/visuals/oban_star_racers_hot_all_time/\n",
      "üéµ ALL VIDEOS NOW HAVE AUDIO! Test with VLC üé•üîä\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• NEW: yt-dlp for PERFECT video+audio\n",
    "import ffmpeg  # üî• NEW: Fallback audio merging\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "# üî• NEW: PERFECT VIDEO+ AUDIO DOWNLOAD with yt-dlp\n",
    "def download_video_with_audio_yt_dlp(post_link, base_filename, visual_folder):\n",
    "    \"\"\"üöÄ Downloads SINGLE MP4 with VIDEO + AUDIO using yt-dlp (no ffmpeg needed)\"\"\"\n",
    "    try:\n",
    "        video_path = os.path.join(visual_folder, f\"{base_filename}_vid_with_audio.mp4\")\n",
    "        if os.path.exists(video_path):\n",
    "            print(f\"   üìÅ SKIP video (exists): {os.path.basename(video_path)}\")\n",
    "            return [os.path.basename(video_path)]\n",
    "        \n",
    "        print(f\"   üé• yt-dlp: PERFECT video + audio ‚Üí {os.path.basename(video_path)}\")\n",
    "        \n",
    "        cmd = [\n",
    "            'yt-dlp',\n",
    "            '--merge-output-format', 'mp4',\n",
    "            '-f', 'best[ext=mp4][height<=1080]/bestvideo[height<=1080]+bestaudio/best',\n",
    "            '--no-playlist',\n",
    "            '-o', video_path,\n",
    "            post_link\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "        if result.returncode == 0 and os.path.exists(video_path) and os.path.getsize(video_path) > 10000:\n",
    "            size_kb = os.path.getsize(video_path) / 1024\n",
    "            print(f\"   üíæ ‚úÖ VIDEO+AUDIO ({size_kb:.1f}KB): {os.path.basename(video_path)}\")\n",
    "            return [os.path.basename(video_path)]\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "        return []\n",
    "\n",
    "# üî• ENHANCED: All possible media URLs (video/audio/images)\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS + AUDIO (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "# üî• Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS + yt-dlp VIDEO+AUDIO\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder, post_link):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí yt-dlp for videos!\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    # üî• SPECIAL CASE: REDDIT VIDEO POST ‚Üí yt-dlp MAGIC (video + audio combined!)\n",
    "    if 'v.redd.it' in post_link or any('v.redd.it' in url for url in visual_urls):\n",
    "        print(f\"   üé¨ DETECTED REDDIT VIDEO POST ‚Üí Using yt-dlp (video+audio)\")\n",
    "        yt_dlp_files = download_video_with_audio_yt_dlp(post_link, base_filename, visual_folder)\n",
    "        if yt_dlp_files:\n",
    "            downloaded_files.extend(yt_dlp_files)\n",
    "            return downloaded_files, [f\"yt-dlp:{post_link}\"]\n",
    "    \n",
    "    working_urls = []\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "    \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + VIDEO+AUDIO!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be', 'yt-dlp']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS + VIDEO+AUDIO!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                        \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS + yt-dlp!\n",
    "                        vtype = 'CAROUSEL' if len(all_candidate_urls) > 5 else 'IMAGE'\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, vtype, all_candidate_urls, base_filename, VISUALS_FOLDER, row['post_link']\n",
    "                        )\n",
    "                        \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls or downloaded_files:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls) if working_urls else 'yt-dlp_success'\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                            \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY + VIDEO WITH AUDIO!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS + VIDEO+AUDIO!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (VIDEO+AUDIO + PROPER .EXT!)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY + VIDEO WITH AUDIO (.mp4) + PROPER .png/.gif extensions!\")\n",
    "    print(f\"‚úÖ pip install yt-dlp  # REQUIRED for video+audio!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n",
    "    print(f\"üéµ ALL VIDEOS NOW HAVE AUDIO! Test with VLC üé•üîä\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a3a00",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üéº12üéº ADDED CODE OF DOWNLOAD AUDIO .m4a TO MAIN \n",
    "and save them in ../data/audio/\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49e2dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT + AUDIO!)\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "üî• Scraping 5 'oban star racers' HOT posts...\n",
      "‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions + AUDIO!\n",
      "üì° Fetching EXACTLY 5 hot posts (Period: All time)...\n",
      "üîç Fetching UP TO 5 hot posts for 'oban star racers'...\n",
      "   üì° API: q=oban%20star%20racers&sort=hot&t=month...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 5/5 hot posts loaded!\n",
      "‚úÖ Saved 5 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 5 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ DOWNLOADING visuals to ‚Üí ../data/visuals/oban_star_racers_hot_all_time\n",
      "üéµ DOWNLOADING audio to ‚Üí ../data/audio/oban_star_racers_hot_all_time\n",
      "====================================================================================================\n",
      "üîç [ 1/5] Respect Ondai (Oban Star Racers)...\n",
      "   üîó [ 1/5] Post ID: 1pjk4g8\n",
      "   üìÖ [ 1/5] Thursday, December 11, 2025 | üïê 01:59:47 AM  | ‚è∞ 1 hour ago\n",
      "   üìù [ 1/5] 1630 chars\n",
      "   ‚ûñ [ 1/5] No visuals\n",
      "   üéâ [ 1/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/5] Oban Star-Racers extended opening intro (2005)....\n",
      "   üîó [ 2/5] Post ID: 1pf2m6v\n",
      "   üìÖ [ 2/5] Friday, December 05, 2025 | üïê 07:24:56 PM  | ‚è∞ 5 days ago\n",
      "   üìù [ 2/5] 104 chars\n",
      "   üñºÔ∏è [ 2/5] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_hot_all_time_2_vid.mp4 (10595.4KB)\n",
      "   ‚úÖ [ 2/5] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 2/5] Extracting audio...\n",
      "   ‚ûñ [ 2/5] No audio (not video post)\n",
      "   üéâ [ 2/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/5] Oban Star Racers Chance to Shine ‚ù§Ô∏è‚Äçüî• Alwas Traps ü™§ Please S...\n",
      "   üîó [ 3/5] Post ID: 1pdpc6z\n",
      "   üìÖ [ 3/5] Thursday, December 04, 2025 | üïê 04:24:36 AM  | ‚è∞ 6 days ago\n",
      "   üìù [ 3/5] 1089 chars\n",
      "   üñºÔ∏è [ 3/5] Testing 112 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_3_img_13.gif (481.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_3_img_29.gif (760.8KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_3_img_45.gif (728.4KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_3_img_61.gif (2640.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_3_img_77.gif (454.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_3_img_95.jpg (110.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_3_img_111.jpg (143.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.jpeg...\n",
      "   ‚úÖ [ 3/5] CAROUSEL (7) - 7 WORKING URLs!\n",
      "   üíæ 7 files saved!\n",
      "   üéâ [ 3/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/5] 20 Years Ago Today on December 2, 2005, the official trailer...\n",
      "   üîó [ 4/5] Post ID: 1pcl5km\n",
      "   üìÖ [ 4/5] Tuesday, December 02, 2025 | üïê 10:30:11 PM  | ‚è∞ 1 week 1 day ago\n",
      "   üìù [ 4/5] 9 chars\n",
      "   üñºÔ∏è [ 4/5] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_hot_all_time_4_vid.mp4 (13218.3KB)\n",
      "   ‚úÖ [ 4/5] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 4/5] Extracting audio...\n",
      "   ‚ûñ [ 4/5] No audio (not video post)\n",
      "   üéâ [ 4/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/5] Star Citizen The Cup - Origin M50 Turbo, Loved By Racers!...\n",
      "   üîó [ 5/5] Post ID: 1pgpnyd\n",
      "   üìÖ [ 5/5] Sunday, December 07, 2025 | üïê 07:33:10 PM  | ‚è∞ 3 days ago\n",
      "   üñºÔ∏è [ 5/5] Testing 1 candidate URLs...\n",
      "   ‚ùå [ 5/5] No working URLs found\n",
      "   üéâ [ 5/5] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 5/5 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ ALL VISUALS ‚Üí ../data/visuals/oban_star_racers_hot_all_time/\n",
      "üéµ ALL AUDIO ‚Üí ../data/audio/oban_star_racers_hot_all_time/\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 5 posts + visuals + audio ‚Üí ../data/visuals/oban_star_racers_hot_all_time/\n",
      "üéµ Audio files ‚Üí ../data/audio/oban_star_racers_hot_all_time/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• ADDED for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "          \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "      \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "          \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "          \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "      \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "      \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "      \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "      \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "      \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "      \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "      \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "      \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "      \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "# üî• NEW: Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "          \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "              \n",
    "                return url, content_type, file_ext\n",
    "          \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "          \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "              \n",
    "                return url, content_type, file_ext\n",
    "              \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• FROM REFERENCE: Reddit audio downloader\n",
    "def download_reddit_audio_only(post_url, output_folder=None):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY (highest quality M4A) from Reddit video post\"\"\"\n",
    "    # Extract post ID for folder naming\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # Use provided folder or create post-specific folder\n",
    "    if output_folder:\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        audio_path = os.path.join(output_folder, f\"{post_id}_audio.m4a\")\n",
    "    else:\n",
    "        folder = f\"audio_{post_id}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        audio_path = os.path.join(folder, f\"{post_id}_audio.m4a\")\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio', # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality (smaller than MP3)\n",
    "        '--audio-quality', '0', # Highest quality (lossless)\n",
    "        '--embed-metadata', # Title, uploader info\n",
    "        '-o', audio_path, # Exact filename\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            return audio_path\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS ONLY\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "          \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "              \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "              \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "              \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "              \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                      \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "              \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "              \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "              \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "              \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                      \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUTO DOWNLOAD + AUDIO\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"  # üî• NEW AUDIO FOLDER\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")  # üî• NEW\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)  # üî• NEW\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "          \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "          \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "          \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "          \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "          \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "              \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + AUDIO!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "      \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• NEW AUDIO COLUMN\n",
    "        }\n",
    "      \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "      \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "      \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                  \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                  \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                  \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                  \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                  \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                  \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                      \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                      \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                          \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                  \n",
    "                    # üî• NEW: DOWNLOAD AUDIO for VIDEO posts using yt-dlp\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        audio_path = download_reddit_audio_only(post_data['post_link'], AUDIO_FOLDER)\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio saved: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio (not video post)\")\n",
    "                  \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "              \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "      \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• NEW AUDIO COLUMN\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/\")  # üî• NEW\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS + AUDIO!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT + AUDIO!)\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions + AUDIO!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c789e41",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üéº13üéº RENAME DOWNLOADED AUDIO .m4a \n",
    "and save them in ../data/audio/\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30f9c317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "üî• Scraping 5 'oban star racers' HOT posts...\n",
      "‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\n",
      "üì° Fetching EXACTLY 5 hot posts (Period: All time)...\n",
      "üîç Fetching UP TO 5 hot posts for 'oban star racers'...\n",
      "   üì° API: q=oban%20star%20racers&sort=hot&t=month...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 5/5 hot posts loaded!\n",
      "‚úÖ Saved 5 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 5 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ DOWNLOADING visuals to ‚Üí ../data/visuals/oban_star_racers_hot_all_time\n",
      "üéµ DOWNLOADING audio to ‚Üí ../data/audio/oban_star_racers_hot_all_time\n",
      "====================================================================================================\n",
      "üîç [ 1/5] Respect Ondai (Oban Star Racers)...\n",
      "   üîó [ 1/5] Post ID: 1pjk4g8\n",
      "   üìÖ [ 1/5] Thursday, December 11, 2025 | üïê 01:59:47 AM  | ‚è∞ 1 hour ago\n",
      "   üìù [ 1/5] 1630 chars\n",
      "   ‚ûñ [ 1/5] No visuals\n",
      "   üéâ [ 1/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/5] Oban Star-Racers extended opening intro (2005)....\n",
      "   üîó [ 2/5] Post ID: 1pf2m6v\n",
      "   üìÖ [ 2/5] Friday, December 05, 2025 | üïê 07:24:56 PM  | ‚è∞ 5 days ago\n",
      "   üìù [ 2/5] 104 chars\n",
      "   üñºÔ∏è [ 2/5] Testing 1 candidate URLs...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_2_vid.mp4\n",
      "   ‚úÖ [ 2/5] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 2/5] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_hot_all_time_2_audio.m4a\n",
      "   ‚ùå yt-dlp failed: WARNING: tnmouowugf5g1: writing DASH m4a. Only some players support this container. Install ffmpeg t\n",
      "   ‚ûñ [ 2/5] No audio extracted\n",
      "   üéâ [ 2/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/5] Oban Star Racers Chance to Shine ‚ù§Ô∏è‚Äçüî• Alwas Traps ü™§ Please S...\n",
      "   üîó [ 3/5] Post ID: 1pdpc6z\n",
      "   üìÖ [ 3/5] Thursday, December 04, 2025 | üïê 04:24:36 AM  | ‚è∞ 6 days ago\n",
      "   üìù [ 3/5] 1089 chars\n",
      "   üñºÔ∏è [ 3/5] Testing 112 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_13.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_29.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_45.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_61.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.webm...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_77.gif\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.png...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_95.jpg\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.png...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_3_img_111.jpg\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.jpeg...\n",
      "   ‚úÖ [ 3/5] CAROUSEL (7) - 7 WORKING URLs!\n",
      "   üíæ 7 files saved!\n",
      "   üéâ [ 3/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/5] 20 Years Ago Today on December 2, 2005, the official trailer...\n",
      "   üîó [ 4/5] Post ID: 1pcl5km\n",
      "   üìÖ [ 4/5] Tuesday, December 02, 2025 | üïê 10:30:11 PM  | ‚è∞ 1 week 1 day ago\n",
      "   üìù [ 4/5] 9 chars\n",
      "   üñºÔ∏è [ 4/5] Testing 1 candidate URLs...\n",
      "   üìÅ SKIP oban_star_racers_hot_all_time_4_vid.mp4\n",
      "   ‚úÖ [ 4/5] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 4/5] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_hot_all_time_4_audio.m4a\n",
      "   ‚ùå yt-dlp failed: WARNING: j13z2wtnzu4g1: writing DASH m4a. Only some players support this container. Install ffmpeg t\n",
      "   ‚ûñ [ 4/5] No audio extracted\n",
      "   üéâ [ 4/5] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/5] Star Citizen The Cup - Origin M50 Turbo, Loved By Racers!...\n",
      "   üîó [ 5/5] Post ID: 1pgpnyd\n",
      "   üìÖ [ 5/5] Sunday, December 07, 2025 | üïê 07:33:10 PM  | ‚è∞ 3 days ago\n",
      "   üñºÔ∏è [ 5/5] Testing 1 candidate URLs...\n",
      "   ‚ùå [ 5/5] No working URLs found\n",
      "   üéâ [ 5/5] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 5/5 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ ALL VISUALS ‚Üí ../data/visuals/oban_star_racers_hot_all_time/\n",
      "üéµ ALL AUDIO ‚Üí ../data/audio/oban_star_racers_hot_all_time/ (keyword_filter_period_postnumber_audio.m4a)\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 5 posts + visuals + audio ‚Üí ../data/\n",
      "üéµ Audio files ‚Üí ../data/audio/oban_star_racers_hot_all_time/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "         \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "     \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "         \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "         \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "     \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "     \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "     \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "     \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "     \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "     \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "     \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "     \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "     \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• FIXED: PERFECT AUDIO NAMING FORMAT\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• EXACT REQUIRED NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "         \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "             \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "             \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + PERFECT AUDIO NAMING\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "     \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "     \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "     \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "         \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + PERFECT AUDIO!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "     \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• PERFECT NAMING COLUMN\n",
    "        }\n",
    "     \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "     \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "     \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                 \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                 \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                 \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                 \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                 \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                 \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                     \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                     \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                         \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                 \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                 \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "     \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9af7c1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üéº14üéº COMBINE AUDIO + VIDEO and replace the same video without audio \n",
    "and save them in ../data/visuals/\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39290b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ffmpeg\\bin\\ffmpeg.EXE\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "print(shutil.which(\"ffmpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bafe8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (16254322.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 1\u001b[1;36m\u001b[0m\n",
      "\u001b[1;33m    pip install moviepy\u001b[0m\n",
      "\u001b[1;37m        ^\u001b[0m\n",
      "\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install moviepy\n",
    "notepad combine_moviepy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6c11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Video: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_vid.mp4\n",
      "üìÅ Audio: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\audio\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_audio.m4a\n",
      "üîÑ Merging...\n",
      "‚ùå Error: ffmpeg version 8.0.1-essentials_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with gcc 15.2.0 (Rev8, Built by MSYS2 project)\n",
      "  configuration: --enable-gpl --enable-version3 \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# üî• 100% Python - NO install needed - Uses built-in subprocess\n",
    "def easy_video_merge():\n",
    "    \"\"\"‚úÖ Works instantly from notebooks/ - oban_star_racers_hot_all_time\"\"\"\n",
    "    \n",
    "    # Your exact paths\n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-music\")\n",
    "    video_path = base_dir / \"data\" / \"visuals\" / \"oban_star_racers_hot_all_time\" / \"oban_star_racers_hot_all_time_2_vid.mp4\"\n",
    "    audio_path = base_dir / \"data\" / \"audio\" / \"oban_star_racers_hot_all_time\" / \"oban_star_racers_hot_all_time_2_audio.m4a\"\n",
    "    \n",
    "    print(f\"üìÅ Video: {video_path}\")\n",
    "    print(f\"üìÅ Audio: {audio_path}\")\n",
    "    \n",
    "    if not video_path.exists():\n",
    "        print(\"‚ùå Video missing!\")\n",
    "        return\n",
    "    if not audio_path.exists():\n",
    "        print(\"‚ùå Audio missing!\")\n",
    "        return\n",
    "    \n",
    "    # üî• ONE LINE FFmpeg (Windows built-in works)\n",
    "    cmd = [\n",
    "        r\"C:\\ffmpeg\\bin\\ffmpeg.exe\", \"-y\",           # Overwrite\n",
    "        \"-i\", str(video_path),                      # Input video\n",
    "        \"-i\", str(audio_path),                      # Input audio  \n",
    "        \"-c:v\", \"copy\",                             # Fast video copy\n",
    "        \"-c:a\", \"aac\",                              # Audio encode\n",
    "        \"-shortest\",                                # Match shortest duration\n",
    "        str(video_path)                             # Output = overwrite video\n",
    "    ]\n",
    "    \n",
    "    print(\"üîÑ Merging...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ SUCCESS! {video_path.name} = {video_path.stat().st_size / 1e6:.1f} MB üé•üîä\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.stderr[:200]}\")\n",
    "\n",
    "# üöÄ RUN NOW\n",
    "easy_video_merge()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d9e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Video: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_vid.mp4\n",
      "üìÅ Audio: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\audio\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_audio.m4a\n",
      "üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_vid.merged.mp4\n",
      "üìù Log: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\ffmpeg_debug.log\n",
      "üîÑ Merging... (check log for progress)\n",
      "‚úÖ SUCCESS! oban_star_racers_hot_all_time_2_vid.merged.mp4 (12.6 MB) üé•üîä\n",
      "üîç Full log saved: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\ffmpeg_debug.log\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def easy_video_merge_fixed():\n",
    "    \"\"\"‚úÖ Complete fixed version - Handles errors, logs everything, safe overwrite\"\"\"\n",
    "    \n",
    "    # Your exact paths\n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-music\")\n",
    "    video_path = base_dir / \"data\" / \"visuals\" / \"oban_star_racers_hot_all_time\" / \"oban_star_racers_hot_all_time_2_vid.mp4\"\n",
    "    audio_path = base_dir / \"data\" / \"audio\" / \"oban_star_racers_hot_all_time\" / \"oban_star_racers_hot_all_time_2_audio.m4a\"\n",
    "    output_path = video_path.with_suffix(\".merged.mp4\")  # Safe new output\n",
    "    \n",
    "    print(f\"üìÅ Video: {video_path}\")\n",
    "    print(f\"üìÅ Audio: {audio_path}\")\n",
    "    print(f\"üìÅ Output: {output_path}\")\n",
    "    \n",
    "    if not video_path.exists():\n",
    "        print(\"‚ùå Video missing!\")\n",
    "        return\n",
    "    if not audio_path.exists():\n",
    "        print(\"‚ùå Audio missing!\")\n",
    "        return\n",
    "    \n",
    "    # Create log file\n",
    "    log_file = base_dir / \"ffmpeg_debug.log\"\n",
    "    print(f\"üìù Log: {log_file}\")\n",
    "    \n",
    "    # üî• FIXED FFmpeg command with explicit stream mapping\n",
    "    cmd = [\n",
    "        r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "        \"-y\",  # Overwrite\n",
    "        \"-i\", str(video_path),\n",
    "        \"-i\", str(audio_path),\n",
    "        \"-map\", \"0:v:0\",   # First video stream\n",
    "        \"-map\", \"1:a:0\",   # First audio stream\n",
    "        \"-c:v\", \"copy\",    # Fast video copy\n",
    "        \"-c:a\", \"aac\",     # Re-encode audio to AAC\n",
    "        \"-shortest\",       # Match shortest duration\n",
    "        \"-progress\", str(log_file),  # Real-time progress to log\n",
    "        str(output_path)   # Output file\n",
    "    ]\n",
    "    \n",
    "    print(\"üîÑ Merging... (check log for progress)\")\n",
    "    \n",
    "    # Run with full stderr logging\n",
    "    with open(log_file, \"w\") as log:\n",
    "        result = subprocess.run(\n",
    "            cmd, \n",
    "            stderr=subprocess.STDOUT,  # Capture ALL output\n",
    "            stdout=log, \n",
    "            text=True,\n",
    "            cwd=base_dir\n",
    "        )\n",
    "    \n",
    "    # Check result\n",
    "    if result.returncode == 0 and output_path.exists():\n",
    "        size_mb = output_path.stat().st_size / 1e6\n",
    "        print(f\"‚úÖ SUCCESS! {output_path.name} ({size_mb:.1f} MB) üé•üîä\")\n",
    "        print(\"üîç Full log saved:\", log_file)\n",
    "    else:\n",
    "        print(\"‚ùå FAILED! Check full log:\")\n",
    "        with open(log_file, \"r\") as f:\n",
    "            print(f.read()[:1000])  # First 1000 chars\n",
    "        print(\"üí° Run manual test in CMD to debug further\")\n",
    "\n",
    "# üöÄ RUN NOW - SAFE VERSION\n",
    "easy_video_merge_fixed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for video/audio pairs...\n",
      "üìä Found 3 video files\n",
      "\n",
      "üîÑ [1/3] oban_star_racers_hot_all_time_2\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\audio\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_vid.merged.mp4\n",
      "   ‚úÖ Already exists - SKIPPING\n",
      "\n",
      "üîÑ [2/3] oban_star_racers_hot_all_time_4\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_4_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\audio\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_4_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_4_vid.merged.mp4\n",
      "   ‚úÖ SUCCESS! (16.4 MB)\n",
      "\n",
      "üîÑ [3/3] oban_star_racers_top_all_time_3\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\visuals\\oban_star_racers_top_all_time\\oban_star_racers_top_all_time_3_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\audio\\oban_star_racers_top_all_time\\oban_star_racers_top_all_time_3_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-music\\data\\visuals\\oban_star_racers_top_all_time\\oban_star_racers_top_all_time_3_vid.merged.mp4\n",
      "   ‚ùå Audio missing - SKIPPING\n",
      "\n",
      "üéâ SUMMARY:\n",
      "   ‚úÖ Success: 2\n",
      "   ‚ùå Failed:  1\n",
      "   üìÅ Total:   3\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def auto_video_merge_all():\n",
    "    \"\"\"üöÄ Automatically merge ALL video+audio pairs in your reddit-music folders\"\"\"\n",
    "    \n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-music\")\n",
    "    visuals_dir = base_dir / \"data\" / \"visuals\"\n",
    "    audio_dir = base_dir / \"data\" / \"audio\"\n",
    "    \n",
    "    print(\"üîç Scanning for video/audio pairs...\")\n",
    "    \n",
    "    # Find ALL video files matching pattern\n",
    "    video_files = list(visuals_dir.rglob(\"*_vid.mp4\"))\n",
    "    print(f\"üìä Found {len(video_files)} video files\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        # Extract keyword_filter_period_postnumber from filename\n",
    "        video_name = video_path.stem  # removes .mp4\n",
    "        if not video_name.endswith(\"_vid\"):\n",
    "            continue\n",
    "            \n",
    "        keyword_filter_period_postnumber = video_name[:-4]  # remove _vid\n",
    "        \n",
    "        # Construct matching audio path\n",
    "        audio_path = audio_dir / video_path.relative_to(visuals_dir).parent / f\"{keyword_filter_period_postnumber}_audio.m4a\"\n",
    "        output_path = video_path.with_suffix(\".merged.mp4\")\n",
    "        \n",
    "        print(f\"\\nüîÑ [{success_count + fail_count + 1}/{len(video_files)}] {keyword_filter_period_postnumber}\")\n",
    "        print(f\"   üìÅ Video:  {video_path}\")\n",
    "        print(f\"   üìÅ Audio:  {audio_path}\")\n",
    "        print(f\"   üìÅ Output: {output_path}\")\n",
    "        \n",
    "        # Skip if already merged\n",
    "        if output_path.exists():\n",
    "            print(\"   ‚úÖ Already exists - SKIPPING\")\n",
    "            success_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not audio_path.exists():\n",
    "            print(\"   ‚ùå Audio missing - SKIPPING\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Merge command\n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "            \"-y\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-i\", str(audio_path),\n",
    "            \"-map\", \"0:v:0\",\n",
    "            \"-map\", \"1:a:0\",\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-shortest\",\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                size_mb = output_path.stat().st_size / 1e6\n",
    "                print(f\"   ‚úÖ SUCCESS! ({size_mb:.1f} MB)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ‚ùå FAILED: {result.stderr[:200]}\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   ‚è∞ TIMEOUT - Killed\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}\")\n",
    "    print(f\"   ‚ùå Failed:  {fail_count}\")\n",
    "    print(f\"   üìÅ Total:   {len(video_files)}\")\n",
    "\n",
    "# üöÄ RUN ALL AUTOMATICALLY\n",
    "auto_video_merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c4d3f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üß† STABLE SCRIPT V4 üß†\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: green;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70407d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: All time ‚Üí API t=all\n",
      "\n",
      "üî• Scraping 10 'one piece funny video' TOP posts...\n",
      "‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\n",
      "   ‚è∞ Time filter: All time\n",
      "üì° Fetching EXACTLY 10 top posts (Period: All time)...\n",
      "üîç Fetching UP TO 10 top posts for 'one piece funny video'...\n",
      "   ‚è∞ Time filter: All time\n",
      "   üì° API: q=one%20piece%20funny%20video&sort=top&t=all...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 10/10 top posts loaded!\n",
      "‚úÖ Saved 10 REAL posts ‚Üí ../data/reddit/one_piece_funny_video_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/one_piece_funny_video_top_all_time.csv\n",
      "üíæ DOWNLOADING visuals to ‚Üí ../data/visuals/one_piece_funny_video_top_all_time\n",
      "üéµ DOWNLOADING audio to ‚Üí ../data/audio/one_piece_funny_video_top_all_time\n",
      "====================================================================================================\n",
      "üîç [ 1/10] üì£ Apollo will close down on June 30th. Reddit‚Äôs recent decis...\n",
      "   üîó [ 1/10] Post ID: 144f6xm\n",
      "   üìÖ [ 1/10] Thursday, June 08, 2023 | üïê 07:18:52 PM  | ‚è∞ 2 years 6 months 2 weeks 6 days ago\n",
      "   üìù [ 1/10] 1809 chars\n",
      "   ‚ûñ [ 1/10] No visuals\n",
      "   üéâ [ 1/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/10] Megathread: Joe Biden Projected to Defeat President Donald T...\n",
      "   üîó [ 2/10] Post ID: jptq5n\n",
      "   üìÖ [ 2/10] Saturday, November 07, 2020 | üïê 05:28:04 PM  | ‚è∞ 5 years 1 month 4 weeks 4 days ago\n",
      "   üìù [ 2/10] 1033 chars\n",
      "   ‚ûñ [ 2/10] No visuals\n",
      "   üéâ [ 2/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/10] One minute standing applause in the European Parliament when...\n",
      "   üîó [ 3/10] Post ID: t46o18\n",
      "   üìÖ [ 3/10] Tuesday, March 01, 2022 | üïê 01:18:35 PM  | ‚è∞ 3 years 9 months 1 day ago\n",
      "   üñºÔ∏è [ 3/10] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [ 3/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéµ [ 3/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí one_piece_funny_video_top_all_time_3_audio.m4a\n",
      "   ‚úÖ Audio saved: one_piece_funny_video_top_all_time_3_audio.m4a (459.6KB)\n",
      "   ‚úÖ [ 3/10] Audio: one_piece_funny_video_top_all_time_3_audio.m4a\n",
      "   üéâ [ 3/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/10] This guy made a video bypassing a lock, the company responds...\n",
      "   üîó [ 4/10] Post ID: 1l262s8\n",
      "   üìÖ [ 4/10] Tuesday, June 03, 2025 | üïê 09:49:54 AM  | ‚è∞ 6 months 1 week 1 day ago\n",
      "   üñºÔ∏è [ 4/10] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [ 4/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéµ [ 4/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí one_piece_funny_video_top_all_time_4_audio.m4a\n",
      "   ‚úÖ Audio saved: one_piece_funny_video_top_all_time_4_audio.m4a (1875.3KB)\n",
      "   ‚úÖ [ 4/10] Audio: one_piece_funny_video_top_all_time_4_audio.m4a\n",
      "   üéâ [ 4/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/10] I just deleted thousands of hours of work from my old job...\n",
      "   üîó [ 5/10] Post ID: r5tn55\n",
      "   üìÖ [ 5/10] Tuesday, November 30, 2021 | üïê 07:19:42 PM  | ‚è∞ 4 years 1 day ago\n",
      "   üìù [ 5/10] 1980 chars\n",
      "   ‚ûñ [ 5/10] No visuals\n",
      "   üéâ [ 5/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/10] Our baby announcement photo. My wife looked so obnoxiously t...\n",
      "   üîó [ 6/10] Post ID: kgvne6\n",
      "   üìÖ [ 6/10] Sunday, December 20, 2020 | üïê 04:15:32 PM  | ‚è∞ 4 years 11 months 2 weeks 3 days ago\n",
      "   üñºÔ∏è [ 6/10] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [ 6/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [ 6/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/10] Upvote the Downvote: Tell Congress to use the CRA to save ne...\n",
      "   üîó [ 7/10] Post ID: 80jsi9\n",
      "   üìÖ [ 7/10] Tuesday, February 27, 2018 | üïê 06:00:05 AM  | ‚è∞ 7 years 9 months 3 weeks 2 days ago\n",
      "   üìù [ 7/10] 1867 chars\n",
      "   ‚ûñ [ 7/10] No visuals\n",
      "   üéâ [ 7/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/10] Net Neutrality Videos &amp; Discussion Megathread...\n",
      "   üîó [ 8/10] Post ID: 7ejban\n",
      "   üìÖ [ 8/10] Tuesday, November 21, 2017 | üïê 06:04:02 PM  | ‚è∞ 8 years 1 day ago\n",
      "   üìù [ 8/10] 1257 chars\n",
      "   ‚ûñ [ 8/10] No visuals\n",
      "   üéâ [ 8/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/10] Megathread: House Votes to Impeach President Donald J. Trump...\n",
      "   üîó [ 9/10] Post ID: ecm1zg\n",
      "   üìÖ [ 9/10] Thursday, December 19, 2019 | üïê 02:36:13 AM  | ‚è∞ 5 years 11 months 3 weeks ago\n",
      "   üìù [ 9/10] 1145 chars\n",
      "   ‚ûñ [ 9/10] No visuals\n",
      "   üéâ [ 9/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/10] TIFU by spending the last year on reddit talking to myself a...\n",
      "   üîó [10/10] Post ID: bbgmzp\n",
      "   üìÖ [10/10] Wednesday, April 10, 2019 | üïê 04:14:58 AM  | ‚è∞ 6 years 8 months 1 week 1 day ago\n",
      "   üìù [10/10] 1987 chars\n",
      "   ‚ûñ [10/10] No visuals\n",
      "   üéâ [10/10] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/one_piece_funny_video_top_all_time.csv\n",
      "üíæ ALL VISUALS ‚Üí ../data/visuals/one_piece_funny_video_top_all_time/\n",
      "üéµ ALL AUDIO ‚Üí ../data/audio/one_piece_funny_video_top_all_time/ (keyword_filter_period_postnumber_audio.m4a)\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 10 posts + visuals + audio ‚Üí ../data/\n",
      "üéµ Audio files ‚Üí ../data/audio/one_piece_funny_video_top_all_time/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "         \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "     \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "         \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "         \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "     \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "     \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "     \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "     \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "     \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "     \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "     \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "     \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "     \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• FIXED: PERFECT AUDIO NAMING FORMAT\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• EXACT REQUIRED NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "         \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "             \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "             \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + PERFECT AUDIO NAMING\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "     \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "     \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "     \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "         \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + PERFECT AUDIO!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "     \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• PERFECT NAMING COLUMN\n",
    "        }\n",
    "     \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "     \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "     \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                 \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                 \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                 \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                 \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                 \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                 \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                     \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                     \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                         \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                 \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                 \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "     \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35728298",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üóÉÔ∏è STEP 01: EXTRACT ALL DATA & CONTENT \n",
    "mute videos in ../data/visuals/\n",
    "\n",
    "audios in ../data/audio/\n",
    "\n",
    "images and gifs in ../data/visuals/\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: hotpink;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68597ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "üî• Scraping 20 'music' HOT posts...\n",
      "‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\n",
      "üì° Fetching EXACTLY 20 hot posts (Period: All time)...\n",
      "üîç Fetching UP TO 20 hot posts for 'music'...\n",
      "   üì° API: q=music&sort=hot&t=month...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 20/20 hot posts loaded!\n",
      "‚úÖ Saved 20 REAL posts ‚Üí ../data/reddit/music_main.csv\n",
      "\n",
      "üöÄ PROCESSING 20 posts ‚Üí ../data/reddit/music_hot_all_time.csv\n",
      "üíæ DOWNLOADING visuals to ‚Üí ../data/visuals/music_hot_all_time\n",
      "üéµ DOWNLOADING audio to ‚Üí ../data/audio/music_hot_all_time\n",
      "====================================================================================================\n",
      "üîç [ 1/20] Music makes him horny...\n",
      "   üîó [ 1/20] Post ID: 1pjcwal\n",
      "   üìÖ [ 1/20] Wednesday, December 10, 2025 | üïê 09:02:39 PM  | ‚è∞ 10 hours ago\n",
      "   üñºÔ∏è [ 1/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [ 1/20] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéµ [ 1/20] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí music_hot_all_time_1_audio.m4a\n",
      "   ‚úÖ Audio saved: music_hot_all_time_1_audio.m4a (462.2KB)\n",
      "   ‚úÖ [ 1/20] Audio: music_hot_all_time_1_audio.m4a\n",
      "   üéâ [ 1/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/20] We try a DIABOLICAL music quiz | 1 Second Game Music Quiz...\n",
      "   üîó [ 2/20] Post ID: 1pjcleb\n",
      "   üìÖ [ 2/20] Wednesday, December 10, 2025 | üïê 08:51:52 PM  | ‚è∞ 10 hours ago\n",
      "   üñºÔ∏è [ 2/20] Testing 1 candidate URLs...\n",
      "   ‚ùå [ 2/20] No working URLs found\n",
      "   üéâ [ 2/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/20] MUSIC IS HOPDE MUSIC IS LOVE...\n",
      "   üîó [ 3/20] Post ID: 1pjgq3i\n",
      "   üìÖ [ 3/20] Wednesday, December 10, 2025 | üïê 11:32:06 PM  | ‚è∞ 8 hours ago\n",
      "   ‚ûñ [ 3/20] No visuals\n",
      "   üéâ [ 3/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/20] „ÄåNintendo Music 2025 „Äú‰ªäÂπ¥„ÅÆÊåØ„ÇäËøî„Çä„Äú„Äç„ÇíÂÖ¨Èñã‰∏≠„ÄÇ„ÄéNintendo Music„Äè„ÅßÊ•ΩÊõ≤ÈÖç‰ø°‰∏≠„ÅÆ„Ç≤...\n",
      "   üîó [ 4/20] Post ID: 1pjnlfw\n",
      "   üìÖ [ 4/20] Thursday, December 11, 2025 | üïê 04:44:52 AM  | ‚è∞ 3 hours ago\n",
      "   ‚ûñ [ 4/20] No visuals\n",
      "   üéâ [ 4/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/20] Gay music...\n",
      "   üîó [ 5/20] Post ID: 1pj7evn\n",
      "   üìÖ [ 5/20] Wednesday, December 10, 2025 | üïê 05:43:02 PM  | ‚è∞ 14 hours ago\n",
      "   üìù [ 5/20] 351 chars\n",
      "   üñºÔ∏è [ 5/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [ 5/20] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [ 5/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/20] New music video...\n",
      "   üîó [ 6/20] Post ID: 1pjp8ww\n",
      "   üìÖ [ 6/20] Thursday, December 11, 2025 | üïê 06:10:38 AM  | ‚è∞ 1 hour ago\n",
      "   üìù [ 6/20] 9 chars\n",
      "   üñºÔ∏è [ 6/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [ 6/20] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [ 6/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/20] Sisyphus music stand...\n",
      "   üîó [ 7/20] Post ID: 1pjc66c\n",
      "   üìÖ [ 7/20] Wednesday, December 10, 2025 | üïê 08:35:46 PM  | ‚è∞ 11 hours ago\n",
      "   üñºÔ∏è [ 7/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [ 7/20] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéµ [ 7/20] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí music_hot_all_time_7_audio.m4a\n",
      "   ‚úÖ Audio saved: music_hot_all_time_7_audio.m4a (132.4KB)\n",
      "   ‚úÖ [ 7/20] Audio: music_hot_all_time_7_audio.m4a\n",
      "   üéâ [ 7/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/20] I like musicals, but I don‚Äôt like people who like musicals....\n",
      "   üîó [ 8/20] Post ID: 1pjgkvh\n",
      "   üìÖ [ 8/20] Wednesday, December 10, 2025 | üïê 11:26:20 PM  | ‚è∞ 8 hours ago\n",
      "   üìù [ 8/20] 1541 chars\n",
      "   ‚ûñ [ 8/20] No visuals\n",
      "   üéâ [ 8/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/20] Best intro music...\n",
      "   üîó [ 9/20] Post ID: 1pjfaqk\n",
      "   üìÖ [ 9/20] Wednesday, December 10, 2025 | üïê 10:35:10 PM  | ‚è∞ 9 hours ago\n",
      "   üìù [ 9/20] 212 chars\n",
      "   üñºÔ∏è [ 9/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [ 9/20] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [ 9/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/20] Character's music taste?...\n",
      "   üîó [10/20] Post ID: 1pj8pbs\n",
      "   üìÖ [10/20] Wednesday, December 10, 2025 | üïê 06:29:32 PM  | ‚è∞ 13 hours ago\n",
      "   üìù [10/20] 167 chars\n",
      "   üñºÔ∏è [10/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [10/20] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [10/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [11/20] My Local Public Classical Music Station Has Not Yet Begun Ch...\n",
      "   üîó [11/20] Post ID: 1pjkn6z\n",
      "   üìÖ [11/20] Thursday, December 11, 2025 | üïê 02:23:41 AM  | ‚è∞ 5 hours ago\n",
      "   üìù [11/20] 103 chars\n",
      "   ‚ûñ [11/20] No visuals\n",
      "   üéâ [11/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [12/20] THEME MUSIC...\n",
      "   üîó [12/20] Post ID: 1pjjipb\n",
      "   üìÖ [12/20] Thursday, December 11, 2025 | üïê 01:31:48 AM  | ‚è∞ 6 hours ago\n",
      "   üñºÔ∏è [12/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [12/20] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [12/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [13/20] Every music Leewufufu without Mark...\n",
      "   üîó [13/20] Post ID: 1pjmumd\n",
      "   üìÖ [13/20] Thursday, December 11, 2025 | üïê 04:07:36 AM  | ‚è∞ 3 hours ago\n",
      "   üñºÔ∏è [13/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [13/20] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [13/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [14/20] Night Music by Yioshka...\n",
      "   üîó [14/20] Post ID: 1pjgpke\n",
      "   üìÖ [14/20] Wednesday, December 10, 2025 | üïê 11:31:32 PM  | ‚è∞ 8 hours ago\n",
      "   üñºÔ∏è [14/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [14/20] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [14/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [15/20] Favorite music video?...\n",
      "   üîó [15/20] Post ID: 1pjf49d\n",
      "   üìÖ [15/20] Wednesday, December 10, 2025 | üïê 10:28:09 PM  | ‚è∞ 9 hours ago\n",
      "   üìù [15/20] 84 chars\n",
      "   üñºÔ∏è [15/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [15/20] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [15/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [16/20] Indian Christmas music...\n",
      "   üîó [16/20] Post ID: 1pjk0gc\n",
      "   üìÖ [16/20] Thursday, December 11, 2025 | üïê 01:54:27 AM  | ‚è∞ 5 hours ago\n",
      "   üñºÔ∏è [16/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [16/20] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéµ [16/20] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí music_hot_all_time_16_audio.m4a\n",
      "   ‚úÖ Audio saved: music_hot_all_time_16_audio.m4a (241.6KB)\n",
      "   ‚úÖ [16/20] Audio: music_hot_all_time_16_audio.m4a\n",
      "   üéâ [16/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [17/20] Gym music...\n",
      "   üîó [17/20] Post ID: 1piv1j1\n",
      "   üìÖ [17/20] Wednesday, December 10, 2025 | üïê 07:00:55 AM  | ‚è∞ 1 day ago\n",
      "   üñºÔ∏è [17/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [17/20] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéµ [17/20] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí music_hot_all_time_17_audio.m4a\n",
      "   ‚úÖ Audio saved: music_hot_all_time_17_audio.m4a (4044.5KB)\n",
      "   ‚úÖ [17/20] Audio: music_hot_all_time_17_audio.m4a\n",
      "   üéâ [17/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [18/20] Back when you owned the music and didn't rent......\n",
      "   üîó [18/20] Post ID: 1pjgg4g\n",
      "   üìÖ [18/20] Wednesday, December 10, 2025 | üïê 11:20:55 PM  | ‚è∞ 8 hours ago\n",
      "   üñºÔ∏è [18/20] Testing 1 candidate URLs...\n",
      "   ‚ö†Ô∏è  Download error: 'str' object has no attribute \n",
      "   ‚úÖ [18/20] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 0 files saved!\n",
      "   üéâ [18/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [19/20] Yt music recap...\n",
      "   üîó [19/20] Post ID: 1pjky1y\n",
      "   üìÖ [19/20] Thursday, December 11, 2025 | üïê 02:38:02 AM  | ‚è∞ 5 hours ago\n",
      "   üñºÔ∏è [19/20] Testing 32 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8ijwgshdbh6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/8ijwgshdbh6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/8ijwgshdbh6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/8ijwgshdbh6g1.gif...\n",
      "   üíæ [IMAGE].png music_hot_all_time_19_img_14.png (1192.8KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/8ijwgshdbh6g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/8ijwgshdbh6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/lspl5lhdbh6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/lspl5lhdbh6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/lspl5lhdbh6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/lspl5lhdbh6g1.gif...\n",
      "   üíæ [IMAGE].png music_hot_all_time_19_img_30.png (1390.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/lspl5lhdbh6g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/lspl5lhdbh6g1.jpeg...\n",
      "   ‚úÖ [19/20] CAROUSEL (2) - 2 WORKING URLs!\n",
      "   üíæ 2 files saved!\n",
      "   üéâ [19/20] COMPLETE ‚úì\n",
      "\n",
      "üîç [20/20] stills from my new music video...\n",
      "   üîó [20/20] Post ID: 1pjb1u6\n",
      "   üìÖ [20/20] Wednesday, December 10, 2025 | üïê 07:53:52 PM  | ‚è∞ 11 hours ago\n",
      "   üìù [20/20] 241 chars\n",
      "   üñºÔ∏è [20/20] Testing 176 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/p9x70zl9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/p9x70zl9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/p9x70zl9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/p9x70zl9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/p9x70zl9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_15.jpg (206.3KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/p9x70zl9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/nq5n15m9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/nq5n15m9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/nq5n15m9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/nq5n15m9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/nq5n15m9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_31.jpg (171.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/nq5n15m9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/spha5zl9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/spha5zl9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/spha5zl9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/spha5zl9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/spha5zl9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_47.jpg (203.7KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/spha5zl9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/x6smeyl9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/x6smeyl9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/x6smeyl9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/x6smeyl9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/x6smeyl9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_63.jpg (236.7KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/x6smeyl9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/t7drgzl9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/t7drgzl9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/t7drgzl9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/t7drgzl9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/t7drgzl9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_79.jpg (198.4KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/t7drgzl9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ifsusyl9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ifsusyl9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ifsusyl9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ifsusyl9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ifsusyl9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_95.jpg (120.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/ifsusyl9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/21pay6m9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/21pay6m9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/21pay6m9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/21pay6m9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/21pay6m9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_111.jpg (226.0KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/21pay6m9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/y1bguzl9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/y1bguzl9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/y1bguzl9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/y1bguzl9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/y1bguzl9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_127.jpg (201.0KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/y1bguzl9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/34qf70m9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/34qf70m9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/34qf70m9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/34qf70m9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/34qf70m9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_143.jpg (159.7KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/34qf70m9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/j6u4g0m9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/j6u4g0m9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/j6u4g0m9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/j6u4g0m9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/j6u4g0m9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_159.jpg (254.0KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/j6u4g0m9bf6g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fntgazl9bf6g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/fntgazl9bf6g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/fntgazl9bf6g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/fntgazl9bf6g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/fntgazl9bf6g1.png...\n",
      "   üíæ [IMAGE].jpg music_hot_all_time_20_img_175.jpg (124.3KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/fntgazl9bf6g1.jpeg...\n",
      "   ‚úÖ [20/20] CAROUSEL (11) - 11 WORKING URLs!\n",
      "   üíæ 11 files saved!\n",
      "   üéâ [20/20] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 20/20 posts ‚Üí ../data/reddit/music_hot_all_time.csv\n",
      "üíæ ALL VISUALS ‚Üí ../data/visuals/music_hot_all_time/\n",
      "üéµ ALL AUDIO ‚Üí ../data/audio/music_hot_all_time/ (keyword_filter_period_postnumber_audio.m4a)\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 20 posts + visuals + audio ‚Üí ../data/\n",
      "üéµ Audio files ‚Üí ../data/audio/music_hot_all_time/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "         \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "     \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "         \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "         \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "     \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "     \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "     \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "     \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "     \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "     \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "     \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "     \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "     \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• FIXED: PERFECT AUDIO NAMING FORMAT\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• EXACT REQUIRED NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "         \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "             \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "             \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + PERFECT AUDIO NAMING\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "     \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "     \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "     \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "         \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + PERFECT AUDIO!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "     \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• PERFECT NAMING COLUMN\n",
    "        }\n",
    "     \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "     \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "     \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                 \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                 \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                 \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                 \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                 \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                 \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                     \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                     \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                         \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                 \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                 \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "     \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bead4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üéûÔ∏è STEP 02: AUTOMATED MERGING OF AUDIOS AND VIDEOS \n",
    "and save them in ../data/visuals/\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: hotpink;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95668355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for video/audio pairs...\n",
      "üìä Found 10 video files\n",
      "\n",
      "üîÑ [1/10] luffy_funny_videos_relevance_all_time_15\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\luffy_funny_videos_relevance_all_time\\luffy_funny_videos_relevance_all_time_15_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\luffy_funny_videos_relevance_all_time\\luffy_funny_videos_relevance_all_time_15_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\luffy_funny_videos_relevance_all_time\\luffy_funny_videos_relevance_all_time_15_video.mp4\n",
      "   ‚úÖ SUCCESS! (0.7 MB)\n",
      "\n",
      "üîÑ [2/10] music_hot_all_time_16\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\music_hot_all_time\\music_hot_all_time_16_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\music_hot_all_time\\music_hot_all_time_16_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\music_hot_all_time\\music_hot_all_time_16_video.mp4\n",
      "   ‚úÖ SUCCESS! (1.9 MB)\n",
      "\n",
      "üîÑ [3/10] music_hot_all_time_17\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\music_hot_all_time\\music_hot_all_time_17_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\music_hot_all_time\\music_hot_all_time_17_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\music_hot_all_time\\music_hot_all_time_17_video.mp4\n",
      "   ‚úÖ SUCCESS! (19.7 MB)\n",
      "\n",
      "üîÑ [4/10] music_hot_all_time_1\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\music_hot_all_time\\music_hot_all_time_1_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\music_hot_all_time\\music_hot_all_time_1_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\music_hot_all_time\\music_hot_all_time_1_video.mp4\n",
      "   ‚úÖ SUCCESS! (3.7 MB)\n",
      "\n",
      "üîÑ [5/10] music_hot_all_time_7\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\music_hot_all_time\\music_hot_all_time_7_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\music_hot_all_time\\music_hot_all_time_7_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\music_hot_all_time\\music_hot_all_time_7_video.mp4\n",
      "   ‚úÖ SUCCESS! (2.3 MB)\n",
      "\n",
      "üîÑ [6/10] oban_star_racers_hot_all_time_2\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_2_video.mp4\n",
      "   ‚úÖ Already exists - SKIPPING\n",
      "\n",
      "üîÑ [7/10] oban_star_racers_hot_all_time_4\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_4_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_4_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\oban_star_racers_hot_all_time\\oban_star_racers_hot_all_time_4_video.mp4\n",
      "   ‚úÖ Already exists - SKIPPING\n",
      "\n",
      "üîÑ [8/10] one_piece_funny_video_top_all_time_3\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\one_piece_funny_video_top_all_time\\one_piece_funny_video_top_all_time_3_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\one_piece_funny_video_top_all_time\\one_piece_funny_video_top_all_time_3_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\one_piece_funny_video_top_all_time\\one_piece_funny_video_top_all_time_3_video.mp4\n",
      "   ‚úÖ SUCCESS! (17.0 MB)\n",
      "\n",
      "üîÑ [9/10] one_piece_funny_video_top_all_time_4\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\one_piece_funny_video_top_all_time\\one_piece_funny_video_top_all_time_4_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\one_piece_funny_video_top_all_time\\one_piece_funny_video_top_all_time_4_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\one_piece_funny_video_top_all_time\\one_piece_funny_video_top_all_time_4_video.mp4\n",
      "   ‚úÖ SUCCESS! (30.4 MB)\n",
      "\n",
      "üîÑ [10/10] promised_neverland_top_all_time_5\n",
      "   üìÅ Video:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\promised_neverland_top_all_time\\promised_neverland_top_all_time_5_vid.mp4\n",
      "   üìÅ Audio:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\audio\\promised_neverland_top_all_time\\promised_neverland_top_all_time_5_audio.m4a\n",
      "   üìÅ Output: C:\\Users\\sboub\\Documents\\GitHub\\reddit-posts-scraper\\data\\visuals\\promised_neverland_top_all_time\\promised_neverland_top_all_time_5_video.mp4\n",
      "   ‚úÖ Already exists - SKIPPING\n",
      "\n",
      "üéâ SUMMARY:\n",
      "   ‚úÖ Success: 10\n",
      "   ‚ùå Failed:  0\n",
      "   üìÅ Total:   10\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def auto_video_merge_all():\n",
    "    \"\"\"üöÄ Automatically merge ALL video+audio pairs - outputs *_video.mp4\"\"\"\n",
    "    \n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-posts-scraper\")\n",
    "    visuals_dir = base_dir / \"data\" / \"visuals\"\n",
    "    audio_dir = base_dir / \"data\" / \"audio\"\n",
    "    \n",
    "    print(\"üîç Scanning for video/audio pairs...\")\n",
    "    \n",
    "    # Find ALL video files matching pattern *_postnumber_vid.mp4\n",
    "    video_files = list(visuals_dir.rglob(\"*_vid.mp4\"))\n",
    "    print(f\"üìä Found {len(video_files)} video files\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        # Extract keyword_filter_period_postnumber from *_vid.mp4\n",
    "        video_name = video_path.stem  # removes .mp4\n",
    "        if not video_name.endswith(\"_vid\"):\n",
    "            continue\n",
    "            \n",
    "        keyword_filter_period_postnumber = video_name[:-4]  # remove _vid\n",
    "        \n",
    "        # Construct matching audio path: same name + _audio.m4a\n",
    "        audio_path = audio_dir / video_path.relative_to(visuals_dir).parent / f\"{keyword_filter_period_postnumber}_audio.m4a\"\n",
    "        \n",
    "        # üî• NEW OUTPUT: keyword_filter_period_postnumber_video.mp4\n",
    "        output_path = video_path.parent / f\"{keyword_filter_period_postnumber}_video.mp4\"\n",
    "        \n",
    "        print(f\"\\nüîÑ [{success_count + fail_count + 1}/{len(video_files)}] {keyword_filter_period_postnumber}\")\n",
    "        print(f\"   üìÅ Video:  {video_path}\")\n",
    "        print(f\"   üìÅ Audio:  {audio_path}\")\n",
    "        print(f\"   üìÅ Output: {output_path}\")\n",
    "        \n",
    "        # Skip if already merged\n",
    "        if output_path.exists():\n",
    "            print(\"   ‚úÖ Already exists - SKIPPING\")\n",
    "            success_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not audio_path.exists():\n",
    "            print(\"   ‚ùå Audio missing - SKIPPING\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Merge command\n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "            \"-y\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-i\", str(audio_path),\n",
    "            \"-map\", \"0:v:0\",\n",
    "            \"-map\", \"1:a:0\",\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-shortest\",\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                size_mb = output_path.stat().st_size / 1e6\n",
    "                print(f\"   ‚úÖ SUCCESS! ({size_mb:.1f} MB)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ‚ùå FAILED: {result.stderr[:200]}\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   ‚è∞ TIMEOUT - Killed\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}\")\n",
    "    print(f\"   ‚ùå Failed:  {fail_count}\")\n",
    "    print(f\"   üìÅ Total:   {len(video_files)}\")\n",
    "\n",
    "# üöÄ RUN ALL AUTOMATICALLY\n",
    "auto_video_merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e96337",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# FEEDBACK\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: red;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8629194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for: your_keyword_here_hot_day (limit: 10)\n",
      "üìä Found 0 matching video files\n",
      "‚ùå No matching files found!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# User inputs - MODIFY THESE\n",
    "KEYWORD = \"your_keyword_here\"  # e.g. \"lofi\" or \"synthwave\"\n",
    "FILTER = \"hot\"  # \"hot\", \"new\", \"top\"\n",
    "PERIOD = \"day\"  # \"hour\", \"day\", \"week\", \"month\", \"year\", \"all\"\n",
    "LIMIT = 10  # Number of posts to process\n",
    "\n",
    "def auto_video_merge_selected():\n",
    "    \"\"\"üöÄ Merge video+audio pairs for specific keyword/filter/period/limit\"\"\"\n",
    "    \n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-music\")\n",
    "    visuals_dir = base_dir / \"data\" / \"visuals\"\n",
    "    audio_dir = base_dir / \"data\" / \"audio\"\n",
    "    \n",
    "    # Construct the naming pattern: keyword_filter_period\n",
    "    pattern_name = f\"{KEYWORD}_{FILTER}_{PERIOD}\"\n",
    "    \n",
    "    print(f\"üîç Scanning for: {pattern_name} (limit: {LIMIT})\")\n",
    "    \n",
    "    # Find video files matching EXACT pattern: keyword_filter_period_*_vid.mp4\n",
    "    video_pattern = f\"*{pattern_name}*_vid.mp4\"\n",
    "    video_files = list(visuals_dir.rglob(video_pattern))\n",
    "    \n",
    "    # Sort by postnumber (natural sort) and limit\n",
    "    video_files.sort(key=lambda p: int(p.stem.split('_')[-2]) if p.stem.split('_')[-1] == 'vid' else 0)\n",
    "    video_files = video_files[:LIMIT]\n",
    "    \n",
    "    print(f\"üìä Found {len(video_files)} matching video files\")\n",
    "    \n",
    "    if not video_files:\n",
    "        print(\"‚ùå No matching files found!\")\n",
    "        return\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        # Extract keyword_filter_period_postnumber from *_vid.mp4\n",
    "        video_name = video_path.stem  # removes .mp4\n",
    "        if not video_name.endswith(\"_vid\"):\n",
    "            continue\n",
    "            \n",
    "        keyword_filter_period_postnumber = video_name[:-4]  # remove _vid\n",
    "        \n",
    "        # Construct matching audio path: same name + _audio.m4a\n",
    "        audio_path = audio_dir / video_path.relative_to(visuals_dir).parent / f\"{keyword_filter_period_postnumber}_audio.m4a\"\n",
    "        \n",
    "        # üî• NEW OUTPUT: keyword_filter_period_postnumber_video.mp4\n",
    "        output_path = video_path.parent / f\"{keyword_filter_period_postnumber}_video.mp4\"\n",
    "        \n",
    "        print(f\"\\nüîÑ [{success_count + fail_count + 1}/{len(video_files)}] {keyword_filter_period_postnumber}\")\n",
    "        print(f\"   üìÅ Video:  {video_path}\")\n",
    "        print(f\"   üìÅ Audio:  {audio_path}\")\n",
    "        print(f\"   üìÅ Output: {output_path}\")\n",
    "        \n",
    "        # Skip if already merged\n",
    "        if output_path.exists():\n",
    "            print(\"   ‚úÖ Already exists - SKIPPING\")\n",
    "            success_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not audio_path.exists():\n",
    "            print(\"   ‚ùå Audio missing - SKIPPING\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Merge command\n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "            \"-y\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-i\", str(audio_path),\n",
    "            \"-map\", \"0:v:0\",\n",
    "            \"-map\", \"1:a:0\",\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-shortest\",\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                size_mb = output_path.stat().st_size / 1e6\n",
    "                print(f\"   ‚úÖ SUCCESS! ({size_mb:.1f} MB)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ‚ùå FAILED: {result.stderr[:200]}\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   ‚è∞ TIMEOUT - Killed\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ SUMMARY for {pattern_name}:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}\")\n",
    "    print(f\"   ‚ùå Failed:  {fail_count}\")\n",
    "    print(f\"   üìÅ Total:   {len(video_files)}\")\n",
    "\n",
    "# üöÄ RUN with your parameters\n",
    "auto_video_merge_selected()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4fd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
