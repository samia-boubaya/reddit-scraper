{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d29526d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üëÄ README\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5a37d",
   "metadata": {},
   "source": [
    "## ‚≠ê **REDDIT POSTS SCRAPER** \n",
    "\n",
    "A Python-based scraper to extract high-engagement Reddit posts related to **music**. Automatically collects post data, including title, description, votes, comments, visuals (images/videos), and metadata. Ideal for content analysis, research, or curating trending music posts. Supports filtering by **Hot, Top, New, Comments, and Relevance**, with time-based searches like **past hour**, **week**, **month**, or **all-time**.  \n",
    "\n",
    "## üî∞ **Features**\n",
    "- Extract post metadata: title, link, ID, votes, comments, post date & time.\n",
    "- Download visuals (image, video, carousel) with automatic file naming.\n",
    "- Filter posts by engagement, relevance, and time period.\n",
    "- Focused on **music-related content**, including theory, instruments, production, memes, and historical highlights.\n",
    "\n",
    "Perfect for researchers, content creators, or music enthusiasts looking to track trends and high-performing posts on Reddit.\n",
    "\n",
    "\n",
    "## üéµ **Reddit/Music Threads Guidelines**\n",
    "\n",
    "**Conditions:**  \n",
    "- Description: 100‚Äì300 characters  \n",
    "- Posts must be HOT & relevant to music  \n",
    "- Downloadable visuals (images/videos)  \n",
    "- Only include posts with ‚â•100 shares  \n",
    "- High engagement / top-performing posts  \n",
    "\n",
    "**Essentials:**  \n",
    "- Keyword: `music`  \n",
    "- Focus: short text or image, simplicity, relevance  \n",
    "\n",
    "**Relevance Categories:**  \n",
    "- Music theory, instruments, production, teaching  \n",
    "- Historical highlights, facts, memes, communities  \n",
    "- Target: anyone learning, playing, or producing music  \n",
    "\n",
    "**Threads Post Rules:**  \n",
    "- Video under 15 seconds  \n",
    "- If both image & video exist, use only video  \n",
    "\n",
    "\n",
    "\n",
    "## üìã **Reddit Post Data Fields**\n",
    "\n",
    "The main data fields to extract from the Reddit Post :\n",
    "| Field Name         | Python Data Type | Description |\n",
    "|--------------------|-----------------|-------------|\n",
    "| <span style=\"color:green\">**post_title**</span>     | `str`            | Title of the post. |\n",
    "| **post_link**      | `str`            | Direct URL to the post. |\n",
    "| **post_id**        | `str`            | Unique identifier for the post. |\n",
    "| <span style=\"color:red\">**num_votes**</span>      | `int`            | Total number of upvotes the post received. |\n",
    "| <span style=\"color:red\">**num_comments**</span>   | `int`            | Total number of comments on the post. |\n",
    "| **text_length**    | `int`            | Character count of the post‚Äôs text description. |\n",
    "| <span style=\"color:green\">**post_description**</span> | `str`          | Full text description or caption of the post. |\n",
    "| **post_date**      | `date`           | Calendar date when the post was published. |\n",
    "| **post_time**      | `str`            | Time (with timezone) when the post was published. |\n",
    "| <span style=\"color:green\">**post_visual**</span>    | `list[str]`      | Direct URLs to visual content (images or videos). |\n",
    "| **visual_type**    | `str`            | Type of visual content: `\"IMAGE\"`, `\"VIDEO\"`,`\"CAROUSEL\"`, or `\"NONE\"`. |\n",
    "| **visual_count**   | `int`            | Number of visual items in the post. |\n",
    "| <span style=\"color:blue\">**filter**</span>         | `str`            | Reddit search filter used. Must be one of: `\"Relevance\"`, `\"Hot\"`, `\"Top\"`, `\"New\"`, `\"Comment count\"`. |\n",
    "| <span style=\"color:blue\">**keyword**</span>        | `str`            | Search keyword or query. |\n",
    "| <span style=\"color:blue\">**limit**</span>          | `int`            | Number of posts requested. |\n",
    "| <span style=\"color:blue\">**period**</span>         | `str`            | Time filter used when searching posts. Must be one of: `\"All time\"`, `\"Past year\"`, `\"Past month\"`, `\"Past week\"`, `\"Today\"`, `\"Past hour\"`. |\n",
    "| <span style=\"color:red\">**time_ago**</span>       | `str`            | Relative time since the post was published (e.g., `\"3 hours ago\"`). |\n",
    "\n",
    "\n",
    "\n",
    "## üîó **POST LINK - EXPLORING**\n",
    "\n",
    "| Link             | Keyword       | Filter       |\n",
    "|------------------------|-----------------------|-----------------------|\n",
    "| https://www.reddit.com/search/?q=music             |  ```music```  | ```Relevance``` by default |\n",
    "| https://www.reddit.com/search/?q=***keyword***             |  ```keyword```  | ```Relevance``` by default |\n",
    "| https://www.reddit.com/search/?q=music&type=posts&sort=hot             |  ```music``` |  ```HOT```  |\n",
    "| https://www.reddit.com/search/?q=music&type=posts&sort=top             |  ```music``` |  ```Top```  |\n",
    "| https://www.reddit.com/search/?q=***keyword***&type=posts&sort=***filter***             |  ```keyword``` |  ```filter```  |\n",
    "\n",
    "\n",
    "## üîç **SEARCH FILTER**\n",
    "\n",
    "| Filter       | What it does                                             | Use it when                                      |\n",
    "|--------------|----------------------------------------------------------|-------------------------------------------------|\n",
    "| ```Relevance```    | Shows posts most related to your search terms           | You want posts that best match your search query |\n",
    "| ```Hot```          | Shows currently trending posts based on upvotes, recency, and engagement | You want popular and active posts right now    |\n",
    "| ```Top```          | Shows posts with the highest score for a given time period | You want the most upvoted or \"best\" content for a topic |\n",
    "| ```New```          | Shows posts in chronological order, newest first       | You want fresh content or to track recent activity |\n",
    "| ```Comments Count```     | Sorts posts by number of comments (descending)          | You want posts with lots of community discussion |\n",
    "\n",
    "## üîç **PERIOD FILTER**\n",
    "\n",
    "| Filter       | What it does                                             | Use it when                                      |\n",
    "|--------------|----------------------------------------------------------|-------------------------------------------------|\n",
    "| ```All time```    | Shows posts most related to your search terms           | You want posts that best match your search query |\n",
    "| ```Past year```          | Shows currently trending posts based on upvotes, recency, and engagement | You want popular and active posts right now    |\n",
    "| ```Past month```          | Shows posts with the highest score for a given time period | You want the most upvoted or \"best\" content for a topic |\n",
    "| ```Past week```          | Shows posts in chronological order, newest first       | You want fresh content or to track recent activity |\n",
    "| ```Past hour```     | Sorts posts by number of comments (descending)          | You want posts with lots of community discussion |\n",
    "\n",
    "\n",
    "## ‚è¨ **DOWNLOAD AUTOMATICALLY**\n",
    "\n",
    "| Visual Type | File Naming Formula | Example File Names |\n",
    "|-------------|-------------------|------------------|\n",
    "| ```CAROUSEL```    | **keyword_filter_period_**`<post_number>_<filter>_<sequence>` | **keyword_filter_period_**`1_img_01`<br>**keyword_filter_period_**`1_img_02`<br>**keyword_filter_period_**`1_img_03` |\n",
    "| ```IMAGE```       | **keyword_filter_period_**`<post_number>_<filter>` | **keyword_filter_period_**`2_img`<br>**keyword_filter_period_**`3_img`<br>**keyword_filter_period_**`4_img` |\n",
    "| ```VIDEO```       | **keyword_filter_period_**`<post_number>_<filter>` | **keyword_filter_period_**`5_vid`<br>**keyword_filter_period_**`6_vid`<br>**keyword_filter_period_**`7_vid` |\n",
    "| ```GIF```       | **keyword_filter_period_**`<post_number>_<filter>` | **keyword_filter_period_**`5_gif`<br>**keyword_filter_period_**`6_gif`<br>**keyword_filter_period_**`7_gif` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098ff7e",
   "metadata": {},
   "source": [
    "---\n",
    "# üîß **HOW TO USE - STEPS**\n",
    "\n",
    "| Step | Instruction |\n",
    "|------|-------------|\n",
    "| **01**  | Run the code |\n",
    "| **02**  | Input what you want to search (example: `\"music\"`) |\n",
    "| **03**  | Input the Reddit search filter: `\"Relevance\"`, `\"Hot\"`, `\"Top\"`, `\"New\"`, `\"Comment count\"` |\n",
    "| **04**  | Input the time filter: `\"All time\"`, `\"Past year\"`, `\"Past month\"`, `\"Past week\"`, `\"Today\"`, `\"Past hour\"` |\n",
    "| **05**  | Input the limit of how many posts you want to extract (example: `\"17\"`) |\n",
    "| **06**  |Wait for the automatic download of visuals then check all saved media in `../data/visuals/` and use the CSV for analysis  |\n",
    "\n",
    "\n",
    "## üîé **SEARCH FILTER 5 CHOICES**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **01**   | `\"Hot\"` |\n",
    "| **02**   | `\"Top\"` |\n",
    "| **03**   | `\"New\"` |\n",
    "| **04**   | `\"Comments Count\"` |\n",
    "| **05**   | `\"Relevance\"` |\n",
    "\n",
    "\n",
    "## ‚è≥ **PERIOD FILTER 6 CHOICES**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **01**   | `\"All time\"` |\n",
    "| **02**   | `\"Past year\"` |\n",
    "| **03**   | `\"Past month\"` |\n",
    "| **04**   | `\"Past week\"` |\n",
    "| **05**   | `\"Today\"` |\n",
    "| **06**   | `\"Past hour\"` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309219d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ‚öôÔ∏è INSTALL BEFORE RUNNING CODE\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0eb49",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1Ô∏è‚É£ INSTALL using **GIT BASH**\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests beautifulsoup4 lxml\n",
    "%pip install playwright pandas beautifulsoup4 lxml\n",
    "\n",
    "#pip install selenium beautifulsoup4 requests webdriver-manager\n",
    "%pip install selenium beautifulsoup4 requests webdriver-manager\n",
    "\n",
    "# install selenium\n",
    "%pip install selenium pandas\n",
    "\n",
    "# install chromium\n",
    "%playwright install chromium\n",
    "\n",
    "# install fake-useragent\n",
    "%pip install fake-useragent\n",
    "\n",
    "# install requests & beautifulsoup\n",
    "%pip install requests beautifulsoup4 fake-useragent pandas\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628cc80",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2Ô∏è‚É£ INSTALL using **NOTEBOOK**\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5370ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core packages\n",
    "%pip install requests beautifulsoup4 lxml pandas fake-useragent\n",
    "\n",
    "# Install Selenium and WebDriver manager\n",
    "%pip install selenium webdriver-manager\n",
    "\n",
    "# Install Playwright and Chromium browser\n",
    "%pip install playwright\n",
    "%playwright install chromium\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c4d3f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üß∞ V4 - STABLE SCRIPT\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70407d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "         \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "     \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "         \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "         \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "     \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "     \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "     \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "     \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "     \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "     \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "     \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "     \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "     \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• AUDIO NAMING FORMAT\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "         \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "             \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "             \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUDIO NAMING\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "     \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "     \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "     \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "         \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + AUDIO NAMING\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "     \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'\n",
    "        }\n",
    "     \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "     \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "     \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                 \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                 \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                 \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                 \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                 \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                 \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                     \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                     \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                         \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                 \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ example oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                 \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "     \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538dbf35",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üß∞ V5 - STABLE SCRIPT  \n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: hotpink;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecf481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "üî• Scraping 10 'oban star racers' HOT posts...\n",
      "‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\n",
      "üì° Fetching EXACTLY 10 hot posts (Period: All time)...\n",
      "üîç Fetching UP TO 10 hot posts for 'oban star racers'...\n",
      "   üì° API: q=oban%20star%20racers&sort=hot&t=month...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 10/10 hot posts loaded!\n",
      "‚úÖ Saved 10 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  IMAGES/GIFs ‚Üí ../data/visuals/\n",
      "üéµ AUDIO ‚Üí ../data/audio/\n",
      "====================================================================================================\n",
      "üîç [ 1/10] NEW OFICIAL OBAN STAR RACERS COMIC...\n",
      "   üîó [ 1/10] Post ID: 1pk06te\n",
      "   üìÖ [ 1/10] Thursday, December 11, 2025 | üïê 04:23:24 PM  | ‚è∞ 6 hours ago\n",
      "   üìù [ 1/10] 160 chars\n",
      "   üñºÔ∏è [ 1/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].png oban_star_racers_hot_all_time_1_img.png (597.3KB)\n",
      "   ‚úÖ [ 1/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 1/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/10] Respect Ondai (Oban Star Racers)...\n",
      "   üîó [ 2/10] Post ID: 1pjk4g8\n",
      "   üìÖ [ 2/10] Thursday, December 11, 2025 | üïê 01:59:47 AM  | ‚è∞ 21 hours ago\n",
      "   üìù [ 2/10] 1630 chars\n",
      "   ‚ûñ [ 2/10] No visuals\n",
      "   üéâ [ 2/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/10] Oban Star-Racers extended opening intro (2005)....\n",
      "   üîó [ 3/10] Post ID: 1pf2m6v\n",
      "   üìÖ [ 3/10] Friday, December 05, 2025 | üïê 07:24:56 PM  | ‚è∞ 6 days ago\n",
      "   üìù [ 3/10] 104 chars\n",
      "   üñºÔ∏è [ 3/10] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_hot_all_time_3_vid.mp4 (10595.4KB)\n",
      "   ‚úÖ [ 3/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 3/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_hot_all_time_3_audio.m4a\n",
      "   ‚úÖ Audio saved: oban_star_racers_hot_all_time_3_audio.m4a (1652.4KB)\n",
      "   ‚úÖ [ 3/10] Audio: oban_star_racers_hot_all_time_3_audio.m4a\n",
      "   üéâ [ 3/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/10] Oban Star Racers Chance to Shine ‚ù§Ô∏è‚Äçüî• Alwas Traps ü™§ Please S...\n",
      "   üîó [ 4/10] Post ID: 1pdpc6z\n",
      "   üìÖ [ 4/10] Thursday, December 04, 2025 | üïê 04:24:36 AM  | ‚è∞ 1 week ago\n",
      "   üìù [ 4/10] 1089 chars\n",
      "   üñºÔ∏è [ 4/10] Testing 112 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_13.gif (481.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_29.gif (760.8KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_45.gif (728.4KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_61.gif (2640.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_77.gif (454.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_4_img_95.jpg (110.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_4_img_111.jpg (143.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.jpeg...\n",
      "   ‚úÖ [ 4/10] CAROUSEL (7) - 7 WORKING URLs!\n",
      "   üíæ 7 files saved!\n",
      "   üéâ [ 4/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/10] 20 Years Ago Today on December 2, 2005, the official trailer...\n",
      "   üîó [ 5/10] Post ID: 1pcl5km\n",
      "   üìÖ [ 5/10] Tuesday, December 02, 2025 | üïê 10:30:11 PM  | ‚è∞ 1 week 2 days ago\n",
      "   üìù [ 5/10] 9 chars\n",
      "   üñºÔ∏è [ 5/10] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_hot_all_time_5_vid.mp4 (13218.3KB)\n",
      "   ‚úÖ [ 5/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 5/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_hot_all_time_5_audio.m4a\n",
      "   ‚úÖ Audio saved: oban_star_racers_hot_all_time_5_audio.m4a (2758.7KB)\n",
      "   ‚úÖ [ 5/10] Audio: oban_star_racers_hot_all_time_5_audio.m4a\n",
      "   üéâ [ 5/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/10] Kurakasis: Star Wars game annoucment incoming that ends with...\n",
      "   üîó [ 6/10] Post ID: 1pjvv8j\n",
      "   üìÖ [ 6/10] Thursday, December 11, 2025 | üïê 01:05:00 PM  | ‚è∞ 9 hours ago\n",
      "   üìù [ 6/10] 98 chars\n",
      "   ‚ûñ [ 6/10] No visuals\n",
      "   üéâ [ 6/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/10] There‚Äôs another Star Wars game in development ending with ‚ÄòR...\n",
      "   üîó [ 7/10] Post ID: 1pjva8a\n",
      "   üìÖ [ 7/10] Thursday, December 11, 2025 | üïê 12:31:45 PM  | ‚è∞ 10 hours ago\n",
      "   ‚ûñ [ 7/10] No visuals\n",
      "   üéâ [ 7/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/10] Fimbul Mamba Cafe Racer with nose art - Star Atlas, by Gary ...\n",
      "   üîó [ 8/10] Post ID: 1pjrlmt\n",
      "   üìÖ [ 8/10] Thursday, December 11, 2025 | üïê 08:30:58 AM  | ‚è∞ 14 hours ago\n",
      "   üñºÔ∏è [ 8/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_8_img.jpg (1296.5KB)\n",
      "   ‚úÖ [ 8/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 8/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/10] Kurakasis: There‚Äôs another Star Wars game announcement comin...\n",
      "   üîó [ 9/10] Post ID: 1pjxg5d\n",
      "   üìÖ [ 9/10] Thursday, December 11, 2025 | üïê 02:25:13 PM  | ‚è∞ 8 hours ago\n",
      "   ‚ûñ [ 9/10] No visuals\n",
      "   üéâ [ 9/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/10] Star Citizen The Cup - Origin M50 Turbo, Loved By Racers!...\n",
      "   üîó [10/10] Post ID: 1pgpnyd\n",
      "   üìÖ [10/10] Sunday, December 07, 2025 | üïê 07:33:10 PM  | ‚è∞ 4 days ago\n",
      "   üñºÔ∏è [10/10] Testing 1 candidate URLs...\n",
      "   ‚ùå [10/10] No working URLs found\n",
      "   üéâ [10/10] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  VISUALS ‚Üí ../data/visuals/\n",
      "üéµ AUDIO ‚Üí ../data/audio/ (keyword_filter_period_postnumber_audio.m4a)\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 10 posts + media ‚Üí ../data/\n",
      "üìÅ Videos(mute): ../data/videos/\n",
      "üìÅ Visuals: ../data/visuals/\n",
      "üéµ Audio: ../data/audio/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "    \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "        \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "    \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "    \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "    \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "    \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "    \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "    \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "    \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "    \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# üî• FIXED: PERFECT AUDIO NAMING FORMAT - SAVES TO ../data/audio/\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• EXACT REQUIRED NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visuals_folder, videos_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí CORRECT FOLDERS BY TYPE\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "        \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # üî• ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                else:  # images, gifs\n",
    "                    target_folder = visuals_folder\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "            \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # üî• ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:  # images, gifs\n",
    "                    target_folder = visuals_folder\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + PERFECT AUDIO + CORRECT FOLDERS\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    # üî• NEW FOLDER STRUCTURE PER SPECS\n",
    "    VIDEOS_FOLDER = \"../data/videos\"           # Videos (mute)\n",
    "    VISUALS_FOLDER = \"../data/visuals\"         # Images, GIFs, Videos+Audio  \n",
    "    AUDIO_FOLDER = \"../data/audio\"             # Audio files\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  IMAGES/GIFs ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VIDEOS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "    \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "    \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "    \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + PERFECT AUDIO + CORRECT FOLDERS!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "    \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• PERFECT NAMING COLUMN\n",
    "        }\n",
    "    \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "    \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "    \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD TO CORRECT FOLDERS\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                    \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS + CORRECT FOLDERS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER, VIDEOS_FOLDER  # üî• PASS BOTH FOLDERS\n",
    "                        )\n",
    "                    \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                        \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING TO ../data/audio/\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER  # üî• FIXED: ../data/audio/\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "    \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + media ‚Üí ../data/\")\n",
    "    print(f\"üìÅ Videos(mute): ../data/videos/\")\n",
    "    print(f\"üìÅ Visuals: ../data/visuals/\")\n",
    "    print(f\"üéµ Audio: ../data/audio/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed846626",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ü™ú STEPS EXTRACT+DOWNLOAD\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf6ed1",
   "metadata": {},
   "source": [
    "\n",
    "| Type                     | Directory              | Description                          |\n",
    "|--------------------------|----------------------|--------------------------------------|\n",
    "| **Videos (mute)**            | `../data/videos/`       | Video files with no audio            |\n",
    "| **Audio files**              | `../data/audio/`        | Audio-only files                     |\n",
    "| **Images, GIFs, Videos** | `../data/visuals/`   | Visual files including images, GIFs, and videos with audio |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb8fdd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üóÉÔ∏è 1 - FIX EXTRACTION OF GIF CAROUSEL\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: purple;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f72cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO + CAROUSEL_GIF!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: All time ‚Üí API t=all\n",
      "\n",
      "üî• Scraping 10 'music theory' TOP posts...\n",
      "‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES/GIFs‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\n",
      "‚úÖ NEW: CAROUSEL_IMAGE, CAROUSEL_GIF detection!\n",
      "   ‚è∞ Time filter: All time\n",
      "üì° Fetching EXACTLY 10 top posts (Period: All time)...\n",
      "üîç Fetching UP TO 10 top posts for 'music theory'...\n",
      "   ‚è∞ Time filter: All time\n",
      "   üì° API: q=music%20theory&sort=top&t=all...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 10/10 top posts loaded!\n",
      "‚úÖ Saved 10 REAL posts ‚Üí ../data/reddit/music_theory_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/music_theory_top_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  IMAGES/GIFs ‚Üí ../data/visuals/\n",
      "üéµ AUDIO ‚Üí ../data/audio/\n",
      "====================================================================================================\n",
      "üîç [ 1/10] StarWarsTheory creates a Darth Vader fan film, hires a compo...\n",
      "   üîó [ 1/10] Post ID: ag8ovy\n",
      "   üìÖ [ 1/10] Tuesday, January 15, 2019 | üïê 02:51:53 PM  | ‚è∞ 6 years 11 months 2 days ago\n",
      "   üñºÔ∏è [ 1/10] Testing 1 candidate URLs...\n",
      "   ‚ùå [ 1/10] No working URLs found\n",
      "   üéâ [ 1/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/10] TIL that Thomas Young proved that light is a wave, described...\n",
      "   üîó [ 2/10] Post ID: ic4jgp\n",
      "   üìÖ [ 2/10] Tuesday, August 18, 2020 | üïê 06:41:18 PM  | ‚è∞ 5 years 3 months 3 weeks 2 days ago\n",
      "   ‚ûñ [ 2/10] No visuals\n",
      "   üéâ [ 2/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/10] The Problem with Immortality...\n",
      "   üîó [ 3/10] Post ID: dn1lx7\n",
      "   üìÖ [ 3/10] Friday, October 25, 2019 | üïê 08:29:22 PM  | ‚è∞ 6 years 1 month 2 weeks 6 days ago\n",
      "   üìù [ 3/10] 1989 chars\n",
      "   ‚ûñ [ 3/10] No visuals\n",
      "   üéâ [ 3/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/10] This community is known to help others, let‚Äôs help Star Wars...\n",
      "   üîó [ 4/10] Post ID: aggl8q\n",
      "   üìÖ [ 4/10] Wednesday, January 16, 2019 | üïê 03:44:00 AM  | ‚è∞ 6 years 11 months 2 days ago\n",
      "   üñºÔ∏è [ 4/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].jpg music_theory_top_all_time_4_img.jpg (101.7KB)\n",
      "   ‚úÖ [ 4/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 4/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/10] \"I will be playing this in front of the whole class\" lol oka...\n",
      "   üîó [ 5/10] Post ID: 7sqtnh\n",
      "   üìÖ [ 5/10] Wednesday, January 24, 2018 | üïê 11:01:51 PM  | ‚è∞ 7 years 10 months 4 weeks 1 day ago\n",
      "   üìù [ 5/10] 1952 chars\n",
      "   ‚ûñ [ 5/10] No visuals\n",
      "   üéâ [ 5/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/10] MEME DAY: RESURGENCE ‚Äî The EU Upload Filter Threat Is Back...\n",
      "   üîó [ 6/10] Post ID: 9epjue\n",
      "   üìÖ [ 6/10] Monday, September 10, 2018 | üïê 08:08:21 PM  | ‚è∞ 7 years 3 months 1 week 3 days ago\n",
      "   üìù [ 6/10] 1439 chars\n",
      "   ‚ûñ [ 6/10] No visuals\n",
      "   üéâ [ 6/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/10] AITA For not punishing my daughter for mocking her cousin?...\n",
      "   üîó [ 7/10] Post ID: k2seky\n",
      "   üìÖ [ 7/10] Saturday, November 28, 2020 | üïê 07:07:09 PM  | ‚è∞ 5 years 1 week 5 days ago\n",
      "   üìù [ 7/10] 1992 chars\n",
      "   ‚ûñ [ 7/10] No visuals\n",
      "   üéâ [ 7/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/10] TIFU by running with bone conduction headphones, now I have ...\n",
      "   üîó [ 8/10] Post ID: gjfsqf\n",
      "   üìÖ [ 8/10] Thursday, May 14, 2020 | üïê 07:06:29 AM  | ‚è∞ 5 years 7 months 4 weeks 1 day ago\n",
      "   üìù [ 8/10] 1981 chars\n",
      "   ‚ûñ [ 8/10] No visuals\n",
      "   üéâ [ 8/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/10] [Discussion] ACTION to SAVE NET NEUTRALITY and the WORLD's L...\n",
      "   üîó [ 9/10] Post ID: 7iv6pi\n",
      "   üìÖ [ 9/10] Sunday, December 10, 2017 | üïê 06:21:48 PM  | ‚è∞ 8 years 1 week 4 days ago\n",
      "   üìù [ 9/10] 1980 chars\n",
      "   ‚ûñ [ 9/10] No visuals\n",
      "   üéâ [ 9/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/10] Megathread: Mitch McConnell to Step Down in November as the ...\n",
      "   üîó [10/10] Post ID: 1b2clkw\n",
      "   üìÖ [10/10] Wednesday, February 28, 2024 | üïê 06:59:11 PM  | ‚è∞ 1 year 9 months 3 weeks 1 day ago\n",
      "   üìù [10/10] 1057 chars\n",
      "   ‚ûñ [10/10] No visuals\n",
      "   üéâ [10/10] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/music_theory_top_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  VISUALS ‚Üí ../data/visuals/ (incl. CAROUSEL_IMAGE, CAROUSEL_GIF)\n",
      "üéµ AUDIO ‚Üí ../data/audio/ (keyword_filter_period_postnumber_audio.m4a)\n",
      "‚úÖ NEW visual_types: CAROUSEL_IMAGE, CAROUSEL_GIF, CAROUSEL_MIXED!\n",
      "\n",
      "‚úÖ DONE! 10 posts + media ‚Üí ../data/\n",
      "üìÅ Videos(mute): ../data/videos/\n",
      "üìÅ Visuals(GIFs/Images): ../data/visuals/\n",
      "üéµ Audio: ../data/audio/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # for yt-dlp audio\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"Convert display text to Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "    \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "        \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "    \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "    \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "    \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "    \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "    \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "    \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "    \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "    \n",
    "        # IMAGES/GIFS (GIFs prioritized)\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# AUDIO NAMING FORMAT - SAVES TO ../data/audio/\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visuals_folder, videos_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí CORRECT FOLDERS BY TYPE\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number - PRIORITIZE GIFs!\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "        \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                else:  # images, gifs ‚Üí visuals/\n",
    "                    target_folder = visuals_folder\n",
    "                    if 'gif' in content_type:\n",
    "                        file_prefix = 'gif'\n",
    "                    else:\n",
    "                        file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "            \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO/GIF: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # üî• ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:  # images, gifs\n",
    "                    target_folder = visuals_folder\n",
    "                    if 'gif' in content_type:\n",
    "                        file_prefix = 'gif'\n",
    "                    else:\n",
    "                        file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_{file_prefix}{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUDIO + CORRECT FOLDERS + CAROUSEL_IMAGE/GIF\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    # FOLDER STRUCTURE\n",
    "    VIDEOS_FOLDER = \"../data/videos\"           # Videos (mute)\n",
    "    VISUALS_FOLDER = \"../data/visuals\"         # Images, GIFs, Videos+Audio  \n",
    "    AUDIO_FOLDER = \"../data/audio\"             # Audio files\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  IMAGES/GIFs ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VIDEOS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    # CAROUSEL_IMAGE, CAROUSEL_GIF DETECTION\n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        \n",
    "        # VIDEO DETECTION\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        \n",
    "        # CAROUSEL DETECTION WITH GIF/IMAGE CLASSIFICATION\n",
    "        if '\\n' in visual_str:\n",
    "            lines = visual_str.splitlines()\n",
    "            gif_count = sum(1 for line in lines if '.gif' in line.lower())\n",
    "            total_count = len(lines)\n",
    "            \n",
    "            if gif_count > 0:\n",
    "                if gif_count == total_count:\n",
    "                    return 'CAROUSEL_GIF', total_count\n",
    "                else:\n",
    "                    return 'CAROUSEL_MIXED', total_count\n",
    "            else:\n",
    "                return 'CAROUSEL_IMAGE', total_count\n",
    "        \n",
    "        # SINGLE GIF/IMAGE\n",
    "        if '.gif' in visual_str or 'gif' in visual_str:\n",
    "            return 'GIF', 1\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png']):\n",
    "            return 'IMAGE', 1\n",
    "        \n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # ENHANCED: Comprehensive visual extraction with carousel + video + GIF support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "    \n",
    "            # 3. CAROUSEL - ENHANCED GIF/IMAGE EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # Try ALL possible formats for this media_id (GIFs prioritized)\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 4. SINGLE IMAGE/GIF\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "    \n",
    "            # 5. PREVIEW IMAGES/GIFs\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "    \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + AUDIO + CORRECT FOLDERS!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "    \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A' \n",
    "        }\n",
    "    \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "    \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "    \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                \n",
    "                    # DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                \n",
    "                    # ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD TO CORRECT FOLDERS\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                    \n",
    "                        # TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS + CORRECT FOLDERS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if len(all_candidate_urls) > 1 else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER, VIDEOS_FOLDER\n",
    "                        )\n",
    "                    \n",
    "                        # ONLY WORKING LINKS go to post_visual! NEW CAROUSEL TYPES!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                        \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                \n",
    "                    # üî• AUDIO DOWNLOAD with EXACT NAMING TO ../data/audio/\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "    \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  VISUALS ‚Üí {VISUALS_FOLDER}/ (incl. CAROUSEL_IMAGE, CAROUSEL_GIF)\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ NEW visual_types: CAROUSEL_IMAGE, CAROUSEL_GIF, CAROUSEL_MIXED!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# INTERACTIVE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO + CAROUSEL_GIF!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES/GIFs‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\")\n",
    "    print(f\"‚úÖ NEW: CAROUSEL_IMAGE, CAROUSEL_GIF detection!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + media ‚Üí ../data/\")\n",
    "    print(f\"üìÅ Videos(mute): ../data/videos/\")\n",
    "    print(f\"üìÅ Visuals(GIFs/Images): ../data/visuals/\")\n",
    "    print(f\"üéµ Audio: ../data/audio/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e570d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üóÉÔ∏è 2 - MERGE MUTE VIDEOS with AUDIOS\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: purple;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97d9200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for video/audio pairs...\n",
      "üìä Found 4 video files in C:\\Users\\sboub\\Documents\\GitHub\\reddit-scraper\\data\\videos\n",
      "\n",
      "üîÑ [1/4] bestchairfordevelopers_top_pastyear_7\n",
      "   üìÅ Video:  bestchairfordevelopers_top_pastyear_7_vid.mp4 (from videos)\n",
      "   üìÅ Audio:  bestchairfordevelopers_top_pastyear_7_audio.m4a\n",
      "   üìÅ Output: bestchairfordevelopers_top_pastyear_7_video.mp4 (to visuals)\n",
      "   ‚úÖ Already exists - SKIPPING\n",
      "\n",
      "üîÑ [2/4] oban_star_racers_relevance_all_time_2\n",
      "   üìÅ Video:  oban_star_racers_relevance_all_time_2_vid.mp4 (from videos)\n",
      "   üìÅ Audio:  oban_star_racers_relevance_all_time_2_audio.m4a\n",
      "   üìÅ Output: oban_star_racers_relevance_all_time_2_video.mp4 (to visuals)\n",
      "   ‚úÖ Already exists - SKIPPING\n",
      "\n",
      "üîÑ [3/4] oban_star_racers_relevance_all_time_4\n",
      "   üìÅ Video:  oban_star_racers_relevance_all_time_4_vid.mp4 (from videos)\n",
      "   üìÅ Audio:  oban_star_racers_relevance_all_time_4_audio.m4a\n",
      "   üìÅ Output: oban_star_racers_relevance_all_time_4_video.mp4 (to visuals)\n",
      "   ‚úÖ Already exists - SKIPPING\n",
      "\n",
      "üîÑ [4/4] ocarinaoftimezeldavideo_relevance_alltime_5\n",
      "   üìÅ Video:  ocarinaoftimezeldavideo_relevance_alltime_5_vid.mp4 (from videos)\n",
      "   üìÅ Audio:  ocarinaoftimezeldavideo_relevance_alltime_5_audio.m4a\n",
      "   üìÅ Output: ocarinaoftimezeldavideo_relevance_alltime_5_video.mp4 (to visuals)\n",
      "   ‚úÖ Already exists - SKIPPING\n",
      "\n",
      "üéâ SUMMARY:\n",
      "   ‚úÖ Success: 4\n",
      "   ‚ùå Failed:  0\n",
      "   üìÅ Total:   4\n",
      "   üìÇ Source:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-scraper\\data\\videos/(*_vid*.mp4)\n",
      "   üìÇ Audio:   C:\\Users\\sboub\\Documents\\GitHub\\reddit-scraper\\data\\audio/(*_audio.m4a)\n",
      "   üìÇ Output:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-scraper\\data\\visuals/(*_video.mp4)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "def auto_video_merge_all():\n",
    "    \"\"\"üöÄ Automatically merge ALL video+audio pairs - outputs *_video.mp4 to ../data/visuals/\"\"\"\n",
    "    \n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-scraper\")\n",
    "    \n",
    "    #  DIRECTORIES\n",
    "    videos_dir = base_dir / \"data\" / \"videos\"      # Videos (mute) - SOURCE *_vid*.mp4\n",
    "    audio_dir = base_dir / \"data\" / \"audio\"        # Audio files\n",
    "    visuals_dir = base_dir / \"data\" / \"visuals\"    # Videos+Audio destination\n",
    "    \n",
    "    print(\"üîç Scanning for video/audio pairs...\")\n",
    "    \n",
    "    # SEARCH VIDEOS IN ../data/videos/ (mute videos)\n",
    "    video_files = list(videos_dir.rglob(\"*_vid*.mp4\")) + list(videos_dir.rglob(\"*_vid*.webm\"))\n",
    "    print(f\"üìä Found {len(video_files)} video files in {videos_dir}\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        # Extract keyword_filter_period_postnumber from *_vid.mp4 OR *_vid_01.mp4\n",
    "        video_name = video_path.stem  # removes .mp4\n",
    "        \n",
    "        # FLEXIBLE PATTERN: keyword_filter_period_postnumber_vid OR keyword_filter_period_postnumber_vid_01\n",
    "        match = re.match(r'(.+?)_vid(?:_\\d+)?$', video_name)\n",
    "        if not match:\n",
    "            print(f\"‚ö†Ô∏è  Skipping non-matching video: {video_name}\")\n",
    "            continue\n",
    "            \n",
    "        keyword_filter_period_postnumber = match.group(1)\n",
    "        \n",
    "        # Construct matching audio path: same name + _audio.m4a\n",
    "        audio_filename = f\"{keyword_filter_period_postnumber}_audio.m4a\"\n",
    "        audio_path = audio_dir / audio_filename\n",
    "        \n",
    "        # OUTPUT *_video.mp4 TO ../data/visuals/ (Videos+Audio)\n",
    "        output_filename = f\"{keyword_filter_period_postnumber}_video.mp4\"\n",
    "        output_path = visuals_dir / output_filename\n",
    "        \n",
    "        print(f\"\\nüîÑ [{success_count + fail_count + 1}/{len(video_files)}] {keyword_filter_period_postnumber}\")\n",
    "        print(f\"   üìÅ Video:  {video_path.name} (from {videos_dir.name})\")\n",
    "        print(f\"   üìÅ Audio:  {audio_filename}\")\n",
    "        print(f\"   üìÅ Output: {output_filename} (to {visuals_dir.name})\")\n",
    "        \n",
    "        # Skip if already merged\n",
    "        if output_path.exists():\n",
    "            print(\"   ‚úÖ Already exists - SKIPPING\")\n",
    "            success_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not audio_path.exists():\n",
    "            print(\"   ‚ùå Audio missing - SKIPPING\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Merge command\n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "            \"-y\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-i\", str(audio_path),\n",
    "            \"-map\", \"0:v:0\",\n",
    "            \"-map\", \"1:a:0\",\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-shortest\",\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                size_mb = output_path.stat().st_size / 1e6\n",
    "                print(f\"   ‚úÖ SUCCESS! ({size_mb:.1f} MB)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ‚ùå FAILED: {result.stderr[:200]}\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   ‚è∞ TIMEOUT - Killed\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}\")\n",
    "    print(f\"   ‚ùå Failed:  {fail_count}\")\n",
    "    print(f\"   üìÅ Total:   {len(video_files)}\")\n",
    "    print(f\"   üìÇ Source:  {videos_dir}/(*_vid*.mp4)\")\n",
    "    print(f\"   üìÇ Audio:   {audio_dir}/(*_audio.m4a)\")\n",
    "    print(f\"   üìÇ Output:  {visuals_dir}/(*_video.mp4)\")  # Videos+Audio\n",
    "\n",
    "\n",
    "# Auto-run\n",
    "if __name__ == \"__main__\":\n",
    "    auto_video_merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e96337",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üß™ TESTING\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: red;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f409633",
   "metadata": {},
   "source": [
    "## TEST 01 tried to improve the video extraction and download but it's still not perfect :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO + CAROUSEL/GIF + AUTO-MERGE!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "Using period: All time (API t=all)\n",
      "\n",
      "Scraping 10 ocarina of time zelda video RELEVANCE posts...\n",
      "VIDEOS: ..\\data\\videos\n",
      "IMAGES/GIFs: ..\\data\\visuals\n",
      "AUDIO: ..\\data\\audio\n",
      "NEW: CAROUSEL/IMAGE, CAROUSEL/GIF detection!\n",
      "Time filter: All time\n",
      "Fetching EXACTLY 10 relevance posts. Period: All time...\n",
      "Fetching UP TO 10 relevance posts for search='ocarina of time zelda video'...\n",
      "Time filter: All time\n",
      "API: q=ocarina%20of%20time%20zelda%20video&sort=relevance&t=all...\n",
      "API returned 100 posts available\n",
      "SUCCESS: 10/10 relevance posts loaded!\n",
      "Saved 10 REAL posts: ..\\data\\reddit\\ocarinaoftimezeldavideomain.csv\n",
      "PROCESSING 10 posts -> ..\\data\\reddit\\ocarinaoftimezeldavideorelevancealltime.csv\n",
      "VIDEOS(mute): ..\\data\\videos\n",
      "IMAGES/GIFs: ..\\data\\visuals\n",
      "AUDIO: ..\\data\\audio\n",
      "100%\n",
      "[  1/10] The Legend of Zelda: Breath of the Wild is voted the best vi...\n",
      "[  1/10] Post ID: rszz5i\n",
      "[  1/10] Friday, December 31, 2021 07:22 PM UTC (3 years ago)\n",
      "[  1/10] No visuals\n",
      "[  1/10] COMPLETE\n",
      "\n",
      "[  2/10] [Oot] Zelda Ocarina of Time Lego. How it is build- Video in ...\n",
      "[  2/10] Post ID: 1i0h7hv\n",
      "[  2/10] Monday, January 13, 2025 05:01 PM UTC (11 months ago)\n",
      "[  2/10] 402 chars\n",
      "[  2/10] Testing 320 candidate URLs...\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/xjouucwlasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/xjouucwlasce1.mp4\n",
      "Broken URL: https://i.redd.it/xjouucwlasce1.webm\n",
      "Broken URL: https://i.redd.it/xjouucwlasce1.gif\n",
      "Broken URL: https://i.redd.it/xjouucwlasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img15.jpg (299.3KB)\n",
      "Broken URL: https://i.redd.it/xjouucwlasce1.jpeg\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/pe1z5izlasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/pe1z5izlasce1.mp4\n",
      "Broken URL: https://i.redd.it/pe1z5izlasce1.webm\n",
      "Broken URL: https://i.redd.it/pe1z5izlasce1.gif\n",
      "Broken URL: https://i.redd.it/pe1z5izlasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img31.jpg (827.7KB)\n",
      "Broken URL: https://i.redd.it/pe1z5izlasce1.jpeg\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/DASH_720\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/DASH_480\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/nco8i76masce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/nco8i76masce1.mp4\n",
      "Broken URL: https://i.redd.it/nco8i76masce1.webm\n",
      "Broken URL: https://i.redd.it/nco8i76masce1.gif\n",
      "Broken URL: https://i.redd.it/nco8i76masce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img47.jpg (694.2KB)\n",
      "Broken URL: https://i.redd.it/nco8i76masce1.jpeg\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/wit0mccmasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/wit0mccmasce1.mp4\n",
      "Broken URL: https://i.redd.it/wit0mccmasce1.webm\n",
      "Broken URL: https://i.redd.it/wit0mccmasce1.gif\n",
      "Broken URL: https://i.redd.it/wit0mccmasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img63.jpg (1025.0KB)\n",
      "Broken URL: https://i.redd.it/wit0mccmasce1.jpeg\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/0zokvgjmasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/0zokvgjmasce1.mp4\n",
      "Broken URL: https://i.redd.it/0zokvgjmasce1.webm\n",
      "Broken URL: https://i.redd.it/0zokvgjmasce1.gif\n",
      "Broken URL: https://i.redd.it/0zokvgjmasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img79.jpg (898.4KB)\n",
      "Broken URL: https://i.redd.it/0zokvgjmasce1.jpeg\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/ppgucyrmasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/ppgucyrmasce1.mp4\n",
      "Broken URL: https://i.redd.it/ppgucyrmasce1.webm\n",
      "Broken URL: https://i.redd.it/ppgucyrmasce1.gif\n",
      "Broken URL: https://i.redd.it/ppgucyrmasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img95.jpg (1017.4KB)\n",
      "Broken URL: https://i.redd.it/ppgucyrmasce1.jpeg\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/zztcnmzmasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/zztcnmzmasce1.mp4\n",
      "Broken URL: https://i.redd.it/zztcnmzmasce1.webm\n",
      "Broken URL: https://i.redd.it/zztcnmzmasce1.gif\n",
      "Broken URL: https://i.redd.it/zztcnmzmasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img111.jpg (757.3KB)\n",
      "Broken URL: https://i.redd.it/zztcnmzmasce1.jpeg\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/cnleb25nasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/cnleb25nasce1.mp4\n",
      "Broken URL: https://i.redd.it/cnleb25nasce1.webm\n",
      "Broken URL: https://i.redd.it/cnleb25nasce1.gif\n",
      "Broken URL: https://i.redd.it/cnleb25nasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img127.jpg (687.7KB)\n",
      "Broken URL: https://i.redd.it/cnleb25nasce1.jpeg\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/wdaj7abnasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/wdaj7abnasce1.mp4\n",
      "Broken URL: https://i.redd.it/wdaj7abnasce1.webm\n",
      "Broken URL: https://i.redd.it/wdaj7abnasce1.gif\n",
      "Broken URL: https://i.redd.it/wdaj7abnasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img143.jpg (1204.2KB)\n",
      "Broken URL: https://i.redd.it/wdaj7abnasce1.jpeg\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/2ptot2lnasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/2ptot2lnasce1.mp4\n",
      "Broken URL: https://i.redd.it/2ptot2lnasce1.webm\n",
      "Broken URL: https://i.redd.it/2ptot2lnasce1.gif\n",
      "Broken URL: https://i.redd.it/2ptot2lnasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img159.jpg (777.2KB)\n",
      "Broken URL: https://i.redd.it/2ptot2lnasce1.jpeg\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/9ksbj5snasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/9ksbj5snasce1.mp4\n",
      "Broken URL: https://i.redd.it/9ksbj5snasce1.webm\n",
      "Broken URL: https://i.redd.it/9ksbj5snasce1.gif\n",
      "Broken URL: https://i.redd.it/9ksbj5snasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img175.jpg (1012.7KB)\n",
      "Broken URL: https://i.redd.it/9ksbj5snasce1.jpeg\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/3gio3aznasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/3gio3aznasce1.mp4\n",
      "Broken URL: https://i.redd.it/3gio3aznasce1.webm\n",
      "Broken URL: https://i.redd.it/3gio3aznasce1.gif\n",
      "Broken URL: https://i.redd.it/3gio3aznasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img191.jpg (1067.3KB)\n",
      "Broken URL: https://i.redd.it/3gio3aznasce1.jpeg\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/mymkrg5oasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/mymkrg5oasce1.mp4\n",
      "Broken URL: https://i.redd.it/mymkrg5oasce1.webm\n",
      "Broken URL: https://i.redd.it/mymkrg5oasce1.gif\n",
      "Broken URL: https://i.redd.it/mymkrg5oasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img207.jpg (1223.7KB)\n",
      "Broken URL: https://i.redd.it/mymkrg5oasce1.jpeg\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/qbjctbfoasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/qbjctbfoasce1.mp4\n",
      "Broken URL: https://i.redd.it/qbjctbfoasce1.webm\n",
      "Broken URL: https://i.redd.it/qbjctbfoasce1.gif\n",
      "Broken URL: https://i.redd.it/qbjctbfoasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img223.jpg (1009.4KB)\n",
      "Broken URL: https://i.redd.it/qbjctbfoasce1.jpeg\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/d7srvznoasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/d7srvznoasce1.mp4\n",
      "Broken URL: https://i.redd.it/d7srvznoasce1.webm\n",
      "Broken URL: https://i.redd.it/d7srvznoasce1.gif\n",
      "Broken URL: https://i.redd.it/d7srvznoasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img239.jpg (828.5KB)\n",
      "Broken URL: https://i.redd.it/d7srvznoasce1.jpeg\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/y67v1ruoasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/y67v1ruoasce1.mp4\n",
      "Broken URL: https://i.redd.it/y67v1ruoasce1.webm\n",
      "Broken URL: https://i.redd.it/y67v1ruoasce1.gif\n",
      "Broken URL: https://i.redd.it/y67v1ruoasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img255.jpg (1156.9KB)\n",
      "Broken URL: https://i.redd.it/y67v1ruoasce1.jpeg\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/tazdq55pasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/tazdq55pasce1.mp4\n",
      "Broken URL: https://i.redd.it/tazdq55pasce1.webm\n",
      "Broken URL: https://i.redd.it/tazdq55pasce1.gif\n",
      "Broken URL: https://i.redd.it/tazdq55pasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img271.jpg (887.5KB)\n",
      "Broken URL: https://i.redd.it/tazdq55pasce1.jpeg\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/gbpqtjcpasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/gbpqtjcpasce1.mp4\n",
      "Broken URL: https://i.redd.it/gbpqtjcpasce1.webm\n",
      "Broken URL: https://i.redd.it/gbpqtjcpasce1.gif\n",
      "Broken URL: https://i.redd.it/gbpqtjcpasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img287.jpg (1598.6KB)\n",
      "Broken URL: https://i.redd.it/gbpqtjcpasce1.jpeg\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/cc9b31lpasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/cc9b31lpasce1.mp4\n",
      "Broken URL: https://i.redd.it/cc9b31lpasce1.webm\n",
      "Broken URL: https://i.redd.it/cc9b31lpasce1.gif\n",
      "Broken URL: https://i.redd.it/cc9b31lpasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img303.jpg (1379.9KB)\n",
      "Broken URL: https://i.redd.it/cc9b31lpasce1.jpeg\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/DASH_1080\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/DASH_720\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/DASH_480\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/audio.m4a\n",
      "Broken URL: https://v.redd.it/x1ojqutpasce1/DASH_audio\n",
      "Broken URL: https://i.redd.it/x1ojqutpasce1.mp4\n",
      "Broken URL: https://i.redd.it/x1ojqutpasce1.webm\n",
      "Broken URL: https://i.redd.it/x1ojqutpasce1.gif\n",
      "Broken URL: https://i.redd.it/x1ojqutpasce1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime2img319.jpg (901.8KB)\n",
      "Broken URL: https://i.redd.it/x1ojqutpasce1.jpeg\n",
      "[  2/10] IMAGE 1 - 20 WORKING URLs!\n",
      "  20 files saved!\n",
      "[  2/10] COMPLETE\n",
      "\n",
      "[  3/10] I am an Ocarina Specialist with over 20 years of professiona...\n",
      "[  3/10] Post ID: 12dw2iy\n",
      "[  3/10] Thursday, April 06, 2023 10:00 PM UTC (2 years ago)\n",
      "[  3/10] 1725 chars\n",
      "[  3/10] No visuals\n",
      "[  3/10] COMPLETE\n",
      "\n",
      "[  4/10] Sequelitis - ZELDA: A Link to the Past vs. Ocarina of Time...\n",
      "[  4/10] Post ID: 29lazt\n",
      "[  4/10] Tuesday, July 01, 2014 10:13 PM UTC (11 years ago)\n",
      "[  4/10] Testing 1 candidate URLs...\n",
      "[  4/10] No working URLs found\n",
      "[  4/10] COMPLETE\n",
      "\n",
      "[  5/10] [OoT] My dogs name is Zelda. Whenever I play Zelda‚Äôs Lullaby...\n",
      "[  5/10] Post ID: df8bsf\n",
      "[  5/10] Wednesday, October 09, 2019 01:28 AM UTC (6 years ago)\n",
      "[  5/10] Testing 1 candidate URLs...\n",
      "VIDEO.mp4: ocarinaoftimezeldavideorelevancealltime5vid.mp4 (28602.5KB)\n",
      "Success! Done with this post\n",
      "[  5/10] VIDEO 1 - 1 WORKING URLs!\n",
      "  1 files saved!\n",
      "[  5/10] Extracting audio...\n",
      "Running yt-dlp: ocarinaoftimezeldavideorelevancealltime5audio.m4a\n",
      "Audio saved: ocarinaoftimezeldavideorelevancealltime5audio.m4a (774.4KB)\n",
      "[  5/10] Audio: ocarinaoftimezeldavideorelevancealltime5audio.m4a\n",
      "[  5/10] COMPLETE\n",
      "\n",
      "[  6/10] The Legend of Zelda: Ocarina of Time enters the Video Games ...\n",
      "[  6/10] Post ID: uj2mg3\n",
      "[  6/10] Thursday, May 05, 2022 07:32 PM UTC (3 years ago)\n",
      "[  6/10] No visuals\n",
      "[  6/10] COMPLETE\n",
      "\n",
      "[  7/10] Games that have been rated a perfect 10/10 by the majority o...\n",
      "[  7/10] Post ID: 1ntcweb\n",
      "[  7/10] Monday, September 29, 2025 10:48 AM UTC (2 months ago)\n",
      "[  7/10] 1712 chars\n",
      "[  7/10] Testing 1 candidate URLs...\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime7img.jpg (9876.1KB)\n",
      "Success! Done with this post\n",
      "[  7/10] IMAGE 1 - 1 WORKING URLs!\n",
      "  1 files saved!\n",
      "[  7/10] COMPLETE\n",
      "\n",
      "[  8/10] Zelda: Ocarina of Time‚Äôs PC port now supports 60fps, save st...\n",
      "[  8/10] Post ID: up3e19\n",
      "[  8/10] Saturday, May 14, 2022 12:42 AM UTC (3 years ago)\n",
      "[  8/10] No visuals\n",
      "[  8/10] COMPLETE\n",
      "\n",
      "[  9/10] Zelda Ocarina of Time Moc...\n",
      "[  9/10] Post ID: 1hnkael\n",
      "[  9/10] Friday, December 27, 2024 06:41 PM UTC (11 months ago)\n",
      "[  9/10] 178 chars\n",
      "[  9/10] Testing 304 candidate URLs...\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/gqqjtgr2hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/gqqjtgr2hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/gqqjtgr2hf9e1.webm\n",
      "Broken URL: https://i.redd.it/gqqjtgr2hf9e1.gif\n",
      "Broken URL: https://i.redd.it/gqqjtgr2hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img15.jpg (873.1KB)\n",
      "Broken URL: https://i.redd.it/gqqjtgr2hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/va5m1sy2hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/va5m1sy2hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/va5m1sy2hf9e1.webm\n",
      "Broken URL: https://i.redd.it/va5m1sy2hf9e1.gif\n",
      "Broken URL: https://i.redd.it/va5m1sy2hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img31.jpg (928.5KB)\n",
      "Broken URL: https://i.redd.it/va5m1sy2hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/4k7c8w53hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/4k7c8w53hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/4k7c8w53hf9e1.webm\n",
      "Broken URL: https://i.redd.it/4k7c8w53hf9e1.gif\n",
      "Broken URL: https://i.redd.it/4k7c8w53hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img47.jpg (1162.3KB)\n",
      "Broken URL: https://i.redd.it/4k7c8w53hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/e0p0kzd3hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/e0p0kzd3hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/e0p0kzd3hf9e1.webm\n",
      "Broken URL: https://i.redd.it/e0p0kzd3hf9e1.gif\n",
      "Broken URL: https://i.redd.it/e0p0kzd3hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img63.jpg (785.8KB)\n",
      "Broken URL: https://i.redd.it/e0p0kzd3hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/7ifsmgk3hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/7ifsmgk3hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/7ifsmgk3hf9e1.webm\n",
      "Broken URL: https://i.redd.it/7ifsmgk3hf9e1.gif\n",
      "Broken URL: https://i.redd.it/7ifsmgk3hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img79.jpg (808.9KB)\n",
      "Broken URL: https://i.redd.it/7ifsmgk3hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/0ll3hgr3hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/0ll3hgr3hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/0ll3hgr3hf9e1.webm\n",
      "Broken URL: https://i.redd.it/0ll3hgr3hf9e1.gif\n",
      "Broken URL: https://i.redd.it/0ll3hgr3hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img95.jpg (874.4KB)\n",
      "Broken URL: https://i.redd.it/0ll3hgr3hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/tp4114y3hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/tp4114y3hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/tp4114y3hf9e1.webm\n",
      "Broken URL: https://i.redd.it/tp4114y3hf9e1.gif\n",
      "Broken URL: https://i.redd.it/tp4114y3hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img111.jpg (1046.1KB)\n",
      "Broken URL: https://i.redd.it/tp4114y3hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/knkvbg54hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/knkvbg54hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/knkvbg54hf9e1.webm\n",
      "Broken URL: https://i.redd.it/knkvbg54hf9e1.gif\n",
      "Broken URL: https://i.redd.it/knkvbg54hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img127.jpg (768.3KB)\n",
      "Broken URL: https://i.redd.it/knkvbg54hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/43h53qb4hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/43h53qb4hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/43h53qb4hf9e1.webm\n",
      "Broken URL: https://i.redd.it/43h53qb4hf9e1.gif\n",
      "Broken URL: https://i.redd.it/43h53qb4hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img143.jpg (1140.7KB)\n",
      "Broken URL: https://i.redd.it/43h53qb4hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/3zc4spi4hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/3zc4spi4hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/3zc4spi4hf9e1.webm\n",
      "Broken URL: https://i.redd.it/3zc4spi4hf9e1.gif\n",
      "Broken URL: https://i.redd.it/3zc4spi4hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img159.jpg (1689.6KB)\n",
      "Broken URL: https://i.redd.it/3zc4spi4hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/f3ff93r4hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/f3ff93r4hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/f3ff93r4hf9e1.webm\n",
      "Broken URL: https://i.redd.it/f3ff93r4hf9e1.gif\n",
      "Broken URL: https://i.redd.it/f3ff93r4hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img175.jpg (920.3KB)\n",
      "Broken URL: https://i.redd.it/f3ff93r4hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/kh883tx4hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/kh883tx4hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/kh883tx4hf9e1.webm\n",
      "Broken URL: https://i.redd.it/kh883tx4hf9e1.gif\n",
      "Broken URL: https://i.redd.it/kh883tx4hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img191.jpg (1519.2KB)\n",
      "Broken URL: https://i.redd.it/kh883tx4hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/0xnkua65hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/0xnkua65hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/0xnkua65hf9e1.webm\n",
      "Broken URL: https://i.redd.it/0xnkua65hf9e1.gif\n",
      "Broken URL: https://i.redd.it/0xnkua65hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img207.jpg (1370.8KB)\n",
      "Broken URL: https://i.redd.it/0xnkua65hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/ift3pae5hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/ift3pae5hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/ift3pae5hf9e1.webm\n",
      "Broken URL: https://i.redd.it/ift3pae5hf9e1.gif\n",
      "Broken URL: https://i.redd.it/ift3pae5hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img223.jpg (1379.9KB)\n",
      "Broken URL: https://i.redd.it/ift3pae5hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/mrabrnm5hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/mrabrnm5hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/mrabrnm5hf9e1.webm\n",
      "Broken URL: https://i.redd.it/mrabrnm5hf9e1.gif\n",
      "Broken URL: https://i.redd.it/mrabrnm5hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img239.jpg (1437.3KB)\n",
      "Broken URL: https://i.redd.it/mrabrnm5hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/1ogbmou5hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/1ogbmou5hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/1ogbmou5hf9e1.webm\n",
      "Broken URL: https://i.redd.it/1ogbmou5hf9e1.gif\n",
      "Broken URL: https://i.redd.it/1ogbmou5hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img255.jpg (1569.7KB)\n",
      "Broken URL: https://i.redd.it/1ogbmou5hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/xq802r36hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/xq802r36hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/xq802r36hf9e1.webm\n",
      "Broken URL: https://i.redd.it/xq802r36hf9e1.gif\n",
      "Broken URL: https://i.redd.it/xq802r36hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img271.jpg (1256.6KB)\n",
      "Broken URL: https://i.redd.it/xq802r36hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/tltxk5b6hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/tltxk5b6hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/tltxk5b6hf9e1.webm\n",
      "Broken URL: https://i.redd.it/tltxk5b6hf9e1.gif\n",
      "Broken URL: https://i.redd.it/tltxk5b6hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img287.jpg (1092.4KB)\n",
      "Broken URL: https://i.redd.it/tltxk5b6hf9e1.jpeg\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/DASH_1080\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/DASH_720\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/DASH_480\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/audio.m4a\n",
      "Broken URL: https://v.redd.it/i6mcryi6hf9e1/DASH_audio\n",
      "Broken URL: https://i.redd.it/i6mcryi6hf9e1.mp4\n",
      "Broken URL: https://i.redd.it/i6mcryi6hf9e1.webm\n",
      "Broken URL: https://i.redd.it/i6mcryi6hf9e1.gif\n",
      "Broken URL: https://i.redd.it/i6mcryi6hf9e1.png\n",
      "IMAGE.jpg: ocarinaoftimezeldavideorelevancealltime9img303.jpg (648.7KB)\n",
      "Broken URL: https://i.redd.it/i6mcryi6hf9e1.jpeg\n",
      "[  9/10] IMAGE 1 - 19 WORKING URLs!\n",
      "  19 files saved!\n",
      "[  9/10] COMPLETE\n",
      "\n",
      "[ 10/10] What is your favourite video game of all time?...\n",
      "[ 10/10] Post ID: tokrrl\n",
      "[ 10/10] Saturday, March 26, 2022 10:16 AM UTC (3 years ago)\n",
      "[ 10/10] 272 chars\n",
      "[ 10/10] No visuals\n",
      "[ 10/10] COMPLETE\n",
      "\n",
      "SAVED 10/10 posts: ..\\data\\reddit\\ocarinaoftimezeldavideorelevancealltime.csv\n",
      "VIDEOS(mute): ..\\data\\videos\n",
      "VISUALS: ..\\data\\visuals (incl. CAROUSEL/IMAGE, CAROUSEL/GIF)\n",
      "\n",
      "=== AUTO-MERGE VIDEOS + AUDIO ===\n",
      "Scanning for video+audio pairs...\n",
      "Skipping non-matching video: oban_star_racers_relevance_all_time_2_vid\n",
      "Skipping non-matching video: oban_star_racers_relevance_all_time_4_vid\n",
      "Skipping non-matching video: ocarinaoftimezeldavideorelevancealltime5vid\n",
      "MERGE COMPLETE: 0 success, 0 failed / 3 videos\n",
      "==================================\n",
      "\n",
      "DONE! 10 posts + media -> ..\\data\n",
      "Videos(mute): ..\\data\\videos\n",
      "Visuals/GIFs/Images: ..\\data\\visuals\n",
      "Audio: ..\\data\\audio\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def createsamplemaindata(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keywordclean = keyword.replace(\" \", \"\")\n",
    "    sampledata = {\n",
    "        'posttitle': [f\"{keyword.title()} post {i+1}\" for i in range(limit)],\n",
    "        'postlink': [f\"https://www.reddit.com/r/{keyword}i/{i+1}/title/{i+1}\" for i in range(limit)],\n",
    "        'postid': [f\"{i+1}\" for i in range(limit)]\n",
    "    }\n",
    "    INPUTFILE = f\"..\\\\data\\\\reddit\\\\{keywordclean}main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUTFILE), exist_ok=True)\n",
    "    pd.DataFrame(sampledata).to_csv(INPUTFILE, index=False)\n",
    "    print(f\"Created sample MAIN data: {limit} posts -> {INPUTFILE}\")\n",
    "    return INPUTFILE\n",
    "\n",
    "def getperiodparam(periodfilter):\n",
    "    \"\"\"Convert display text to Reddit API 't' parameter\"\"\"\n",
    "    periodmap = {\n",
    "        'All time': 'all', 'Past year': 'year', 'Past month': 'month',\n",
    "        'Past week': 'week', 'Today': 'day', 'Past hour': 'hour'\n",
    "    }\n",
    "    return periodmap.get(periodfilter, 'month')\n",
    "\n",
    "def fetchredditpostssearch(searchkeyword, filterhot, limit=50, periodfilter=None):\n",
    "    \"\"\"Fetch Reddit posts via API\"\"\"\n",
    "    print(f\"Fetching UP TO {limit} {filterhot} posts for search='{searchkeyword}'...\")\n",
    "    if periodfilter:\n",
    "        print(f\"Time filter: {periodfilter}\")\n",
    "    encodedkeyword = urllib.parse.quote(searchkeyword)\n",
    "    periodparam = getperiodparam(periodfilter) if periodfilter else 'month'\n",
    "    searchurl = f\"https://www.reddit.com/search.json?q={encodedkeyword}&sort={filterhot}&limit=100&t={periodparam}&type=link\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    try:\n",
    "        print(f\"API: q={encodedkeyword}&sort={filterhot}&t={periodparam}...\")\n",
    "        response = requests.get(searchurl, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"API returned {available} posts available\")\n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                postdata = post['data']\n",
    "                posts.append({\n",
    "                    'posttitle': postdata.get('title', 'NA'),\n",
    "                    'postlink': f\"https://www.reddit.com{postdata.get('permalink', '')}\",\n",
    "                    'postid': postdata.get('id', 'NA'),\n",
    "                    'numvotes': postdata.get('score', 0),\n",
    "                    'numcomments': postdata.get('num_comments', 0),\n",
    "                    'filter': filterhot,\n",
    "                    'periodfilter': periodfilter or 'NA'\n",
    "                })\n",
    "        actualposts = len(posts)\n",
    "        print(f\"SUCCESS: {actualposts}/{limit} {filterhot} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def getviewableimageurl(url):\n",
    "    \"\"\"ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    urllower = url.lower()\n",
    "    if 'i.redd.it' in urllower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    match = re.search(r'preview\\.redd\\.it/[a-z0-9]+', urllower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/[a-z0-9]+', urllower)\n",
    "    if not match:\n",
    "        match = re.search(r'[a-z0-9]{13}\\.', urllower)\n",
    "    if match:\n",
    "        mediaid = match.group(1)\n",
    "        return f\"https://i.redd.it/{mediaid}.png\"\n",
    "    return url\n",
    "\n",
    "def formatpostdate(createdutc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable datetime\"\"\"\n",
    "    if not createdutc or createdutc == 'NA':\n",
    "        return 'NA', 'NA'\n",
    "    try:\n",
    "        timestamp = float(createdutc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        postdate = dt.strftime('%A, %B %d, %Y')\n",
    "        posttime = dt.strftime('%I:%M %p UTC')\n",
    "        return postdate, posttime\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculatetimeago(postdatestr, posttimestr):\n",
    "    \"\"\"Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if postdatestr == 'NA' or posttimestr == 'NA':\n",
    "        return 'NA'\n",
    "    try:\n",
    "        datetimestr = f\"{postdatestr} {posttimestr.replace(' UTC', '')}\"\n",
    "        postdt = datetime.strptime(datetimestr, '%A, %B %d, %Y %I:%M %p')\n",
    "        now = datetime.now()\n",
    "        delta = now - postdt\n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year{'s' if years != 1 else ''}\")\n",
    "        elif months > 0:\n",
    "            parts.append(f\"{months} month{'s' if months != 1 else ''}\")\n",
    "        elif weeks > 0:\n",
    "            parts.append(f\"{weeks} week{'s' if weeks != 1 else ''}\")\n",
    "        elif days > 0:\n",
    "            parts.append(f\"{days} day{'s' if days != 1 else ''}\")\n",
    "        elif hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour{'s' if hours != 1 else ''}\")\n",
    "        elif not parts:\n",
    "            return \"just now\"\n",
    "        timeago = \" \".join(parts) + \" ago\"\n",
    "        return timeago\n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def getenhancedmediacandidates(mediaid):\n",
    "    \"\"\"Generate ALL possible media URLs for a mediaid (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_720\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_480\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_audio\",\n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{mediaid}.mp4\",\n",
    "        f\"https://i.redd.it/{mediaid}.webm\",\n",
    "        # IMAGES/GIFs\n",
    "        f\"https://i.redd.it/{mediaid}.gif\",\n",
    "        f\"https://i.redd.it/{mediaid}.png\",\n",
    "        f\"https://i.redd.it/{mediaid}.jpg\",\n",
    "        f\"https://i.redd.it/{mediaid}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def testurlworking(url, headersbrowser, timeout=10):\n",
    "    \"\"\"Returns workingurl, contenttype, fileext or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headersbrowser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            contenttype = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0)) or 0\n",
    "            if size > 1000 and any(mediatype in contenttype for mediatype in ['video/', 'image/', 'audio/']):\n",
    "                # IMAGES/GIFs (GIFs prioritized)\n",
    "                if 'video/' in contenttype:\n",
    "                    fileext = '.mp4' if 'mp4' in contenttype else '.webm'\n",
    "                elif 'image/' in contenttype:\n",
    "                    if 'gif' in contenttype:\n",
    "                        fileext = '.gif'\n",
    "                    elif 'png' in contenttype:\n",
    "                        fileext = '.png'\n",
    "                    elif 'jpeg' in contenttype:\n",
    "                        fileext = '.jpg'\n",
    "                    else:\n",
    "                        fileext = '.jpg'\n",
    "                else:\n",
    "                    fileext = '.bin'\n",
    "                return url, contenttype, fileext\n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headersbrowser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            contenttype = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            if size > 1000 and any(mediatype in contenttype for mediatype in ['video/', 'image/', 'audio/']):\n",
    "                if 'video/' in contenttype:\n",
    "                    fileext = '.mp4' if 'mp4' in contenttype else '.webm'\n",
    "                elif 'image/' in contenttype:\n",
    "                    if 'gif' in contenttype:\n",
    "                        fileext = '.gif'\n",
    "                    elif 'png' in contenttype:\n",
    "                        fileext = '.png'\n",
    "                    elif 'jpeg' in contenttype:\n",
    "                        fileext = '.jpg'\n",
    "                    else:\n",
    "                        fileext = '.jpg'\n",
    "                else:\n",
    "                    fileext = '.bin'\n",
    "                return url, contenttype, fileext\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def downloadredditaudioonly(posturl, keywordclean, filterparam, periodstr, postnumber, audiofolder):\n",
    "    \"\"\"PERFECT AUDIO ONLY with EXACT naming: keyword+filter+period+postnumber+audio.m4a\"\"\"\n",
    "    postid = re.search(r'comments/([a-zA-Z0-9]+)', posturl)\n",
    "    if not postid:\n",
    "        return None\n",
    "    postid = postid.group(1)\n",
    "    audiofilename = f\"{keywordclean}{filterparam}{periodstr}{postnumber}audio.m4a\"\n",
    "    audiopath = os.path.join(audiofolder, audiofilename)\n",
    "    if os.path.exists(audiopath):\n",
    "        print(f\"Audio exists: {audiofilename}\")\n",
    "        return audiopath\n",
    "    cmd = [\n",
    "        'yt-dlp', '--extract-audio',  # Audio only (no video)\n",
    "        '--audio-format', 'm4a',  # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',  # Title, uploader info\n",
    "        '-o', audiopath,  # EXACT filename required\n",
    "        posturl\n",
    "    ]\n",
    "    try:\n",
    "        print(f\"Running yt-dlp: {audiofilename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audiopath):\n",
    "            filesize = os.path.getsize(audiopath) / 1024\n",
    "            print(f\"Audio saved: {audiofilename} ({filesize:.1f}KB)\")\n",
    "            return audiopath\n",
    "        else:\n",
    "            print(f\"yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"yt-dlp error: {str(e)[:50]}\")\n",
    "    return None\n",
    "\n",
    "def downloadvisualauto(postnumber, visualtype, visualurls, basefilename, visualsfolder, videosfolder):\n",
    "    \"\"\"DOWNLOADS visuals ONLY WORKING LINKS + PROPER EXTENSIONS + CORRECT FOLDERS BY TYPE\"\"\"\n",
    "    downloadedfiles = []\n",
    "    headersbrowser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    workingurls = []\n",
    "    \n",
    "    if visualtype == 'CAROUSEL':\n",
    "        for seqidx, url in enumerate(visualurls, 1):\n",
    "            seqstr = f\"{seqidx:02d}\"\n",
    "            result = testurlworking(url, headersbrowser)\n",
    "            if result:\n",
    "                workingurl, contenttype, fileext = result\n",
    "                workingurls.append(workingurl)\n",
    "                if 'video/' in contenttype:\n",
    "                    targetfolder = videosfolder\n",
    "                    fileprefix = 'vid'\n",
    "                else:  # images, gifs, visuals\n",
    "                    targetfolder = visualsfolder\n",
    "                    fileprefix = 'gif' if 'gif' in contenttype else 'img'\n",
    "                filename = f\"{basefilename}{fileprefix}{seqstr}{fileext}\"\n",
    "                filepath = os.path.join(targetfolder, filename)\n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"SKIP: {filename}\")\n",
    "                    downloadedfiles.append(filename)\n",
    "                    continue\n",
    "                # ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                try:\n",
    "                    resp = requests.get(workingurl, headers=headersbrowser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        mediatype = contenttype.split('/')[0].upper()\n",
    "                        print(f\"{mediatype}{fileext}: {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloadedfiles.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"Broken URL: {url[:60]}\")\n",
    "    else:  # SINGLE IMAGE/VIDEO/GIF\n",
    "        for url in visualurls:\n",
    "            result = testurlworking(url, headersbrowser)\n",
    "            if result:\n",
    "                workingurl, contenttype, fileext = result\n",
    "                workingurls.append(workingurl)\n",
    "                if 'video/' in contenttype:\n",
    "                    targetfolder = videosfolder\n",
    "                    fileprefix = 'vid'\n",
    "                    filename = f\"{basefilename}vid{fileext}\"\n",
    "                else:  # images, gifs\n",
    "                    targetfolder = visualsfolder\n",
    "                    fileprefix = 'gif' if 'gif' in contenttype else 'img'\n",
    "                    filename = f\"{basefilename}{fileprefix}{fileext}\"\n",
    "                filepath = os.path.join(targetfolder, filename)\n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"SKIP: {filename}\")\n",
    "                    downloadedfiles.append(filename)\n",
    "                    break\n",
    "                # ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                try:\n",
    "                    resp = requests.get(workingurl, headers=headersbrowser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        mediatype = contenttype.split('/')[0].upper()\n",
    "                        print(f\"{mediatype}{fileext}: {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloadedfiles.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        print(\"Success! Done with this post\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Download error: {str(e)[:30]}\")\n",
    "                    break\n",
    "    return downloadedfiles, workingurls\n",
    "\n",
    "def extractpostid(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    url = str(url).strip()\n",
    "    match = re.search(r'comments/([a-zA-Z0-9]+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def getvisualtypecount(visual):\n",
    "    if visual in ['NA', 'MEDIAERROR', 'ERROR']:\n",
    "        return 'NONE', 0\n",
    "    visualstr = str(visual).lower()\n",
    "    # CAROUSEL/IMAGE, CAROUSEL/GIF DETECTION\n",
    "    if any(x in visualstr for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "        return 'VIDEO', 1\n",
    "    if '\\n' in visualstr:\n",
    "        lines = visualstr.splitlines()\n",
    "        gifcount = sum(1 for line in lines if '.gif' in line.lower())\n",
    "        totalcount = len(lines)\n",
    "        if gifcount == totalcount:\n",
    "            return 'CAROUSEL/GIF', totalcount\n",
    "        elif gifcount > 0:\n",
    "            return 'CAROUSEL/MIXED', totalcount\n",
    "        else:\n",
    "            return 'CAROUSEL/IMAGE', totalcount\n",
    "    # SINGLE GIF/IMAGE\n",
    "    if '.gif' in visualstr or 'gif' in visualstr:\n",
    "        return 'GIF', 1\n",
    "    if 'i.redd.it' in visualstr or any(ext in visualstr for ext in ['.jpg', '.png']):\n",
    "        return 'IMAGE', 1\n",
    "    return 'OTHER', 1\n",
    "\n",
    "def calculatetextlength(description):\n",
    "    if not description or description in ['NA', 'ERROR', 'INVALIDLINK']:\n",
    "        return 0\n",
    "    text = re.sub(r'https?://\\S+', '', str(description))\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return len(text)\n",
    "\n",
    "def extractvisualurls(postinfo):\n",
    "    \"\"\"ENHANCED: Comprehensive visual extraction with carousel/video/GIF support\"\"\"\n",
    "    visualurls = []\n",
    "    try:\n",
    "        # 1. REDDIT VIDEO (highest priority)\n",
    "        if postinfo.get('is_video') and postinfo.get('media', {}).get('reddit_video'):\n",
    "            fallbackurl = postinfo['media']['reddit_video'].get('fallback_url')\n",
    "            if fallbackurl:\n",
    "                visualurls.append(fallbackurl)\n",
    "                return visualurls\n",
    "        # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "        if any(domain in postinfo.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "            visualurls.append(postinfo['url'])\n",
    "            return visualurls\n",
    "        # 3. CAROUSEL - ENHANCED GIF/IMAGE EXTRACTION\n",
    "        gallerydata = postinfo.get('gallery_data')\n",
    "        if gallerydata and gallerydata.get('items'):\n",
    "            for item in gallerydata['items']:\n",
    "                if isinstance(item, dict) and 'media_id' in item:\n",
    "                    mediaid = item['media_id']\n",
    "                    mediacandidates = getenhancedmediacandidates(mediaid)\n",
    "                    for candidateurl in mediacandidates:\n",
    "                        visualurls.append(candidateurl)\n",
    "            if visualurls:\n",
    "                return visualurls\n",
    "        # 4. SINGLE IMAGE/GIF\n",
    "        posturl = postinfo.get('url', '')\n",
    "        viewableurl = getviewableimageurl(posturl)\n",
    "        if viewableurl and 'i.redd.it' in viewableurl:\n",
    "            return [viewableurl]\n",
    "        # 5. PREVIEW IMAGES/GIFs\n",
    "        if postinfo.get('preview', {}).get('images'):\n",
    "            for img in postinfo['preview']['images']:\n",
    "                sourceurl = img.get('source', {}).get('url', '')\n",
    "                if sourceurl:\n",
    "                    viewableurl = getviewableimageurl(sourceurl)\n",
    "                    if 'i.redd.it' in viewableurl:\n",
    "                        return [viewableurl]\n",
    "        # 6. THUMBNAIL FALLBACK\n",
    "        if postinfo.get('thumbnail') and 'i.redd.it' in postinfo['thumbnail']:\n",
    "            return [postinfo['thumbnail']]\n",
    "    except:\n",
    "        pass\n",
    "    return visualurls\n",
    "\n",
    "def extractpostdetailscomplete(keyword, filterhot, limit=50, periodfilter=None):\n",
    "    \"\"\"MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUDIO + CORRECT FOLDERS + CAROUSEL/IMAGE/GIF\"\"\"\n",
    "    keywordclean = keyword.replace(\" \", \"\")\n",
    "    periodstr = periodfilter.replace(\" \", \"\").lower() if periodfilter else \"alltime\"\n",
    "    INPUTFILE = f\"..\\\\data\\\\reddit\\\\{keywordclean}main.csv\"\n",
    "    OUTPUTFILE = f\"..\\\\data\\\\reddit\\\\{keywordclean}{filterhot}{periodstr}.csv\"\n",
    "    VIDEOSFOLDER = \"..\\\\data\\\\videos\"  # Videos (mute)\n",
    "    VISUALSFOLDER = \"..\\\\data\\\\visuals\"  # Images, GIFs, Videos+Audio\n",
    "    AUDIOFOLDER = \"..\\\\data\\\\audio\"  # Audio files\n",
    "    \n",
    "    print(f\"Fetching EXACTLY {limit} {filterhot} posts. Period: {periodfilter or 'All time'}...\")\n",
    "    df = fetchredditpostssearch(keyword, filterhot, limit, periodfilter)\n",
    "    if df is None or df.empty:\n",
    "        print(\"Search failed. Using sample data...\")\n",
    "        createsamplemaindata(keywordclean, limit)\n",
    "        df = pd.read_csv(INPUTFILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUTFILE), exist_ok=True)\n",
    "        df.to_csv(INPUTFILE, index=False)\n",
    "        print(f\"Saved {len(df)} REAL posts: {INPUTFILE}\")\n",
    "    \n",
    "    totalposts = len(df)\n",
    "    print(f\"PROCESSING {totalposts} posts -> {OUTPUTFILE}\")\n",
    "    print(f\"VIDEOS(mute): {VIDEOSFOLDER}\")\n",
    "    print(f\"IMAGES/GIFs: {VISUALSFOLDER}\")\n",
    "    print(f\"AUDIO: {AUDIOFOLDER}\")\n",
    "    print(\"100%\")\n",
    "    \n",
    "    os.makedirs(VIDEOSFOLDER, exist_ok=True)\n",
    "    os.makedirs(VISUALSFOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIOFOLDER, exist_ok=True)\n",
    "    \n",
    "    newdata = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"[{idx+1:3d}/{totalposts}]\"\n",
    "        posttitle = str(row['posttitle'])[:60]\n",
    "        postnumber = idx + 1\n",
    "        print(f\"{progress} {posttitle}...\")\n",
    "        \n",
    "        postdata = {\n",
    "            'posttitle': row.get('posttitle', 'NA'),\n",
    "            'postlink': row.get('postlink', 'NA'),\n",
    "            'postid': 'NA',\n",
    "            'numvotes': row.get('numvotes', 'NA'),\n",
    "            'numcomments': row.get('numcomments', 'NA'),\n",
    "            'filter': filterhot,\n",
    "            'periodfilter': periodfilter or 'NA',\n",
    "            'postdate': 'NA',\n",
    "            'posttime': 'NA',\n",
    "            'timeago': 'NA',\n",
    "            'textlength': 0,\n",
    "            'postdescription': 'NA',\n",
    "            'postvisual': 'NA',\n",
    "            'visualtype': 'NONE',\n",
    "            'visualcount': 0,\n",
    "            'downloadedfiles': 'NA',\n",
    "            'audiofile': 'NA'\n",
    "        }\n",
    "        \n",
    "        postid = extractpostid(row['postlink'])\n",
    "        if not postid:\n",
    "            print(f\"{progress} Invalid link - SKIPPED\")\n",
    "            newdata.append(postdata)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"{progress} Post ID: {postid}\")\n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{postid}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    postinfo = data[0]['data']['children'][0]['data']\n",
    "                    postdata.update({\n",
    "                        'postid': postid,\n",
    "                        'numvotes': str(postinfo.get('score', 'NA')),\n",
    "                    })\n",
    "                    # FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + AUDIO + CORRECT FOLDERS\n",
    "                    createdutc = postinfo.get('created_utc')\n",
    "                    postdate, posttime = formatpostdate(createdutc)\n",
    "                    postdata['postdate'] = postdate\n",
    "                    postdata['posttime'] = posttime\n",
    "                    postdatatimeago = calculatetimeago(postdate, posttime)\n",
    "                    print(f\"{progress} {postdate} {posttime[:12]} ({postdatatimeago})\")\n",
    "                    \n",
    "                    selftext = postinfo.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        postdata['postdescription'] = selftext\n",
    "                        postdata['textlength'] = calculatetextlength(selftext)\n",
    "                        print(f\"{progress} {postdata['textlength']} chars\")\n",
    "                    \n",
    "                    # ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD TO CORRECT FOLDERS\n",
    "                    allcandidateurls = extractvisualurls(postinfo)\n",
    "                    basefilename = f\"{keywordclean}{filterhot}{periodstr}{postnumber}\"\n",
    "                    if allcandidateurls:\n",
    "                        print(f\"{progress} Testing {len(allcandidateurls)} candidate URLs...\")\n",
    "                        downloadedfiles, workingurls = downloadvisualauto(\n",
    "                            postnumber, 'CAROUSEL' if len(allcandidateurls) > 1 else 'IMAGE',\n",
    "                            allcandidateurls, basefilename, VISUALSFOLDER, VIDEOSFOLDER\n",
    "                        )\n",
    "                        # TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS + CORRECT FOLDERS\n",
    "                        if workingurls:\n",
    "                            postdata['postvisual'] = \"; \".join(workingurls)\n",
    "                            vtype, vcount = getvisualtypecount(postdata['postvisual'])\n",
    "                            postdata.update({\n",
    "                                'visualtype': vtype,\n",
    "                                'visualcount': vcount,\n",
    "                                'downloadedfiles': \"; \".join(downloadedfiles) if downloadedfiles else 'ERROR'\n",
    "                            })\n",
    "                            print(f\"{progress} {vtype} {vcount} - {len(workingurls)} WORKING URLs!\")\n",
    "                            print(f\"  {len(downloadedfiles)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"{progress} No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"{progress} No visuals\")\n",
    "                    \n",
    "                    # ONLY WORKING LINKS go to postvisual! NEW CAROUSEL TYPES!\n",
    "                    if postdata['visualtype'] in ['VIDEO'] and postid:\n",
    "                        print(f\"{progress} Extracting audio...\")\n",
    "                        audiopath = downloadredditaudioonly(\n",
    "                            postdata['postlink'], keywordclean, filterhot, periodstr, postnumber, AUDIOFOLDER\n",
    "                        )\n",
    "                        if audiopath:\n",
    "                            audiofilename = os.path.basename(audiopath)\n",
    "                            postdata['audiofile'] = audiofilename\n",
    "                            print(f\"{progress} Audio: {audiofilename}\")\n",
    "                        else:\n",
    "                            print(f\"{progress} No audio extracted\")\n",
    "                    print(f\"{progress} COMPLETE\")\n",
    "                else:\n",
    "                    print(f\"{progress} No post data\")\n",
    "            else:\n",
    "                print(f\"{progress} HTTP {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{progress} Error: {str(e)[:40]}\")\n",
    "        \n",
    "        newdata.append(postdata)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print(\"\")  # Empty line\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUTFILE), exist_ok=True)\n",
    "    newdf = pd.DataFrame(newdata, columns=[\n",
    "        'posttitle', 'postlink', 'postid', 'numvotes', 'numcomments', 'filter', 'periodfilter',\n",
    "        'postdate', 'posttime', 'timeago', 'textlength', 'postdescription', 'postvisual',\n",
    "        'visualtype', 'visualcount', 'downloadedfiles', 'audiofile'\n",
    "    ])\n",
    "    newdf.to_csv(OUTPUTFILE, index=False)\n",
    "    print(f\"SAVED {len(newdf)}/{limit} posts: {OUTPUTFILE}\")\n",
    "    print(f\"VIDEOS(mute): {VIDEOSFOLDER}\")\n",
    "    print(f\"VISUALS: {VISUALSFOLDER} (incl. CAROUSEL/IMAGE, CAROUSEL/GIF)\")\n",
    "    \n",
    "    # NEW: Auto-merge videos + audio after all downloads\n",
    "    autovideomergeall()\n",
    "    return newdf\n",
    "\n",
    "def autovideomergeall():\n",
    "    \"\"\"Automatically merge ALL video+audio pairs - outputs video.mp4 to ..data\\visuals\"\"\"\n",
    "    basedir = Path(\"..\")  # Adjust to your project root\n",
    "    videosdir = basedir / \"data\" / \"videos\"  # SOURCE: vid*.mp4\n",
    "    audiodir = basedir / \"data\" / \"audio\"    # Audio files\n",
    "    visualsdir = basedir / \"data\" / \"visuals\" # DESTINATION: video.mp4\n",
    "    visualsdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n=== AUTO-MERGE VIDEOS + AUDIO ===\")\n",
    "    print(\"Scanning for video+audio pairs...\")\n",
    "    videofiles = list(videosdir.glob(\"*.mp4\"))\n",
    "    successcount, failcount = 0, 0\n",
    "    \n",
    "    for videopath in videofiles:\n",
    "        videoname = videopath.stem\n",
    "        match = re.match(r'.*?(\\w+filter\\w+period\\d+)', videoname)  # Flexible: keywordfilterperiodpostnum*\n",
    "        if not match:\n",
    "            print(f\"Skipping non-matching video: {videoname}\")\n",
    "            continue\n",
    "        basepattern = match.group(1)\n",
    "        audiopath = audiodir / f\"{basepattern}audio.m4a\"\n",
    "        outputpath = visualsdir / f\"{basepattern}video.mp4\"\n",
    "        \n",
    "        if outputpath.exists():\n",
    "            print(f\"Already exists - SKIPPING: {outputpath.name}\")\n",
    "            successcount += 1\n",
    "            continue\n",
    "        if not audiopath.exists():\n",
    "            print(f\"Audio missing - SKIPPING: {audiopath.name}\")\n",
    "            failcount += 1\n",
    "            continue\n",
    "        \n",
    "        # FFmpeg merge command\n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\", \"-y\",  # -y overwrite\n",
    "            \"-i\", str(videopath), \"-i\", str(audiopath),\n",
    "            \"-map\", \"0:v:0\", \"-map\", \"1:a:0\",  # video from 0, audio from 1\n",
    "            \"-c:v\", \"copy\", \"-c:a\", \"aac\", \"-shortest\",  # copy video, re-encode audio, match duration\n",
    "            str(outputpath)\n",
    "        ]\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "            if result.returncode == 0 and outputpath.exists():\n",
    "                print(f\"SUCCESS: {videopath.name} + {audiopath.name} ‚Üí {outputpath.name}\")\n",
    "                successcount += 1\n",
    "            else:\n",
    "                print(f\"FFmpeg failed: {result.stderr[:100]}\")\n",
    "                failcount += 1\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"FFmpeg timeout\")\n",
    "            failcount += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)[:50]}\")\n",
    "            failcount += 1\n",
    "    \n",
    "    print(f\"MERGE COMPLETE: {successcount} success, {failcount} failed / {len(videofiles)} videos\")\n",
    "    print(\"==================================\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO + CAROUSEL/GIF + AUTO-MERGE!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword (default: music): \").strip() or \"music\"\n",
    "    print(\"Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter (1): \").strip() or \"1\"\n",
    "    filtermap = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filterhot = filtermap.get(choice, 'hot')\n",
    "    \n",
    "    periodfilter = None\n",
    "    if filterhot in ['relevance', 'top', 'comments']:\n",
    "        print(\"PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        periodchoice = input(\"Choose period (2=Past year): \").strip() or \"2\"\n",
    "        periodmap = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', '4': 'Past week',\n",
    "            '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        periodfilter = periodmap.get(periodchoice, 'Past year')\n",
    "        print(f\"Using period: {periodfilter} (API t={getperiodparam(periodfilter)})\")\n",
    "    \n",
    "    limitinput = input(\"How many posts? (1-100, default=20): \").strip()\n",
    "    limit = int(limitinput) if limitinput.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nScraping {limit} {keyword} {filterhot.upper()} posts...\")\n",
    "    print(f\"VIDEOS: ..\\\\data\\\\videos\")\n",
    "    print(f\"IMAGES/GIFs: ..\\\\data\\\\visuals\") \n",
    "    print(f\"AUDIO: ..\\\\data\\\\audio\")\n",
    "    print(\"NEW: CAROUSEL/IMAGE, CAROUSEL/GIF detection!\")\n",
    "    if periodfilter:\n",
    "        print(f\"Time filter: {periodfilter}\")\n",
    "    \n",
    "    result = extractpostdetailscomplete(keyword, filterhot, limit, periodfilter)\n",
    "    periodfilename = periodfilter.replace(\" \", \"\").lower() if periodfilter else \"alltime\"\n",
    "    print(f\"DONE! {len(result)} posts + media -> ..\\\\data\")\n",
    "    print(\"Videos(mute): ..\\\\data\\\\videos\")\n",
    "    print(\"Visuals/GIFs/Images: ..\\\\data\\\\visuals\")\n",
    "    print(\"Audio: ..\\\\data\\\\audio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41601c3",
   "metadata": {},
   "source": [
    "## TEST 02 fixed the names of downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb579b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDDIT EXTRACTOR + EXACT NAMING + AUTO-MERGE!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "PERIOD FILTER:\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "Using period: Past year\n",
      "\n",
      "EXACT NAMING:\n",
      "- GIF: keyword_filter_period_postnumber_gif.gif\n",
      "- MUTE VID: keyword_filter_period_postnumber_vid.mp4\n",
      "- AUDIO: keyword_filter_period_postnumber_audio.m4a\n",
      "- VIDEO+AUDIO: keyword_filter_period_postnumber_video.mp4\n",
      "- IMG: keyword_filter_period_postnumber_img.jpg/png\n",
      "Fetching EXACTLY 10 top posts. Period: Past year...\n",
      "Fetching UP TO 10 top posts for search='best chair for developers'...\n",
      "Time filter: Past year\n",
      "API: q=best%20chair%20for%20developers&sort=top&t=year...\n",
      "API returned 100 posts available\n",
      "SUCCESS: 10/10 top posts loaded!\n",
      "Saved 10 REAL posts: ..\\data\\reddit\\bestchairfordevelopersmain.csv\n",
      "PROCESSING 10 posts -> ..\\data\\reddit\\bestchairfordevelopers_top_pastyear.csv\n",
      "VIDEOS(mute): ..\\data\\videos (keyword_filter_period_postnumber_vid.mp4)\n",
      "VISUALS: ..\\data\\visuals (keyword_filter_period_postnumber_[gif|img].*)\n",
      "AUDIO: ..\\data\\audio (keyword_filter_period_postnumber_audio.m4a)\n",
      "100%\n",
      "[  1/10] Sad but true...\n",
      "[  1/10] Post ID: 1juefwv\n",
      "[  1/10] Tuesday, April 08, 2025 04:08 PM UTC (8 months ago)\n",
      "[  1/10] Testing 1 candidate URLs...\n",
      "Download error: 'str' object has no attribute \n",
      "[  1/10] IMAGE 1 - 1 WORKING URLs!\n",
      "  0 files saved!\n",
      "[  1/10] COMPLETE\n",
      "\n",
      "[  2/10] The slop I have been eating every day for the last two years...\n",
      "[  2/10] Post ID: 1otncn2\n",
      "[  2/10] Monday, November 10, 2025 08:28 PM UTC (1 month ago)\n",
      "[  2/10] 1964 chars\n",
      "[  2/10] Testing 1 candidate URLs...\n",
      "Download error: 'str' object has no attribute \n",
      "[  2/10] IMAGE 1 - 1 WORKING URLs!\n",
      "  0 files saved!\n",
      "[  2/10] COMPLETE\n",
      "\n",
      "[  3/10] \"He could literally r*pe a child on Fox News, look to the ca...\n",
      "[  3/10] Post ID: 1mhu6zu\n",
      "[  3/10] Tuesday, August 05, 2025 02:07 AM UTC (4 months ago)\n",
      "[  3/10] 1802 chars\n",
      "[  3/10] No visuals\n",
      "[  3/10] COMPLETE\n",
      "\n",
      "[  4/10] Lost my baby girl of 8 years yesterday...\n",
      "[  4/10] Post ID: 1iu4nj1\n",
      "[  4/10] Thursday, February 20, 2025 06:54 PM UTC (9 months ago)\n",
      "[  4/10] 1999 chars\n",
      "[  4/10] Testing 96 candidate URLs...\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/DASH_1080\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/DASH_720\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/DASH_480\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/audio.m4a\n",
      "Broken URL: https://v.redd.it/045jp5gj1cke1/DASH_audio\n",
      "Broken URL: https://i.redd.it/045jp5gj1cke1.mp4\n",
      "Broken URL: https://i.redd.it/045jp5gj1cke1.webm\n",
      "Broken URL: https://i.redd.it/045jp5gj1cke1.gif\n",
      "Broken URL: https://i.redd.it/045jp5gj1cke1.png\n",
      "IMAGE.jpg: bestchairfordevelopers_top_pastyear_4_img.jpg (113.2KB)\n",
      "Broken URL: https://i.redd.it/045jp5gj1cke1.jpeg\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/DASH_1080\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/DASH_720\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/DASH_480\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/audio.m4a\n",
      "Broken URL: https://v.redd.it/n8s2f6gj1cke1/DASH_audio\n",
      "Broken URL: https://i.redd.it/n8s2f6gj1cke1.mp4\n",
      "Broken URL: https://i.redd.it/n8s2f6gj1cke1.webm\n",
      "Broken URL: https://i.redd.it/n8s2f6gj1cke1.gif\n",
      "Broken URL: https://i.redd.it/n8s2f6gj1cke1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_4_img.jpg\n",
      "Broken URL: https://i.redd.it/n8s2f6gj1cke1.jpeg\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/DASH_1080\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/DASH_720\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/DASH_480\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/audio.m4a\n",
      "Broken URL: https://v.redd.it/forl46gj1cke1/DASH_audio\n",
      "Broken URL: https://i.redd.it/forl46gj1cke1.mp4\n",
      "Broken URL: https://i.redd.it/forl46gj1cke1.webm\n",
      "Broken URL: https://i.redd.it/forl46gj1cke1.gif\n",
      "Broken URL: https://i.redd.it/forl46gj1cke1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_4_img.jpg\n",
      "Broken URL: https://i.redd.it/forl46gj1cke1.jpeg\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/DASH_1080\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/DASH_720\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/DASH_480\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/audio.m4a\n",
      "Broken URL: https://v.redd.it/9zokg6gj1cke1/DASH_audio\n",
      "Broken URL: https://i.redd.it/9zokg6gj1cke1.mp4\n",
      "Broken URL: https://i.redd.it/9zokg6gj1cke1.webm\n",
      "Broken URL: https://i.redd.it/9zokg6gj1cke1.gif\n",
      "Broken URL: https://i.redd.it/9zokg6gj1cke1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_4_img.jpg\n",
      "Broken URL: https://i.redd.it/9zokg6gj1cke1.jpeg\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/DASH_1080\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/DASH_720\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/DASH_480\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/audio.m4a\n",
      "Broken URL: https://v.redd.it/bb8666gj1cke1/DASH_audio\n",
      "Broken URL: https://i.redd.it/bb8666gj1cke1.mp4\n",
      "Broken URL: https://i.redd.it/bb8666gj1cke1.webm\n",
      "Broken URL: https://i.redd.it/bb8666gj1cke1.gif\n",
      "Broken URL: https://i.redd.it/bb8666gj1cke1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_4_img.jpg\n",
      "Broken URL: https://i.redd.it/bb8666gj1cke1.jpeg\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/DASH_1080\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/DASH_720\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/DASH_480\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/audio.m4a\n",
      "Broken URL: https://v.redd.it/yfwah6gj1cke1/DASH_audio\n",
      "Broken URL: https://i.redd.it/yfwah6gj1cke1.mp4\n",
      "Broken URL: https://i.redd.it/yfwah6gj1cke1.webm\n",
      "Broken URL: https://i.redd.it/yfwah6gj1cke1.gif\n",
      "Broken URL: https://i.redd.it/yfwah6gj1cke1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_4_img.jpg\n",
      "Broken URL: https://i.redd.it/yfwah6gj1cke1.jpeg\n",
      "[  4/10] IMAGE 1 - 6 WORKING URLs!\n",
      "  6 files saved!\n",
      "[  4/10] COMPLETE\n",
      "\n",
      "[  5/10] 40M living in a subsidised government apartment...\n",
      "[  5/10] Post ID: 1oykg8l\n",
      "[  5/10] Sunday, November 16, 2025 01:25 PM UTC (3 weeks ago)\n",
      "[  5/10] 1583 chars\n",
      "[  5/10] Testing 96 candidate URLs...\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/DASH_1080\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/DASH_720\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/DASH_480\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/audio.m4a\n",
      "Broken URL: https://v.redd.it/kxux5ap24m1g1/DASH_audio\n",
      "Broken URL: https://i.redd.it/kxux5ap24m1g1.mp4\n",
      "Broken URL: https://i.redd.it/kxux5ap24m1g1.webm\n",
      "Broken URL: https://i.redd.it/kxux5ap24m1g1.gif\n",
      "Broken URL: https://i.redd.it/kxux5ap24m1g1.png\n",
      "IMAGE.jpg: bestchairfordevelopers_top_pastyear_5_img.jpg (1690.6KB)\n",
      "Broken URL: https://i.redd.it/kxux5ap24m1g1.jpeg\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/DASH_1080\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/DASH_720\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/DASH_480\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/audio.m4a\n",
      "Broken URL: https://v.redd.it/yd2pxjp24m1g1/DASH_audio\n",
      "Broken URL: https://i.redd.it/yd2pxjp24m1g1.mp4\n",
      "Broken URL: https://i.redd.it/yd2pxjp24m1g1.webm\n",
      "Broken URL: https://i.redd.it/yd2pxjp24m1g1.gif\n",
      "Broken URL: https://i.redd.it/yd2pxjp24m1g1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_5_img.jpg\n",
      "Broken URL: https://i.redd.it/yd2pxjp24m1g1.jpeg\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/DASH_1080\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/DASH_720\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/DASH_480\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/audio.m4a\n",
      "Broken URL: https://v.redd.it/p778zkp24m1g1/DASH_audio\n",
      "Broken URL: https://i.redd.it/p778zkp24m1g1.mp4\n",
      "Broken URL: https://i.redd.it/p778zkp24m1g1.webm\n",
      "Broken URL: https://i.redd.it/p778zkp24m1g1.gif\n",
      "Broken URL: https://i.redd.it/p778zkp24m1g1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_5_img.jpg\n",
      "Broken URL: https://i.redd.it/p778zkp24m1g1.jpeg\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/DASH_1080\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/DASH_720\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/DASH_480\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/audio.m4a\n",
      "Broken URL: https://v.redd.it/kr2oo5r24m1g1/DASH_audio\n",
      "Broken URL: https://i.redd.it/kr2oo5r24m1g1.mp4\n",
      "Broken URL: https://i.redd.it/kr2oo5r24m1g1.webm\n",
      "Broken URL: https://i.redd.it/kr2oo5r24m1g1.gif\n",
      "Broken URL: https://i.redd.it/kr2oo5r24m1g1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_5_img.jpg\n",
      "Broken URL: https://i.redd.it/kr2oo5r24m1g1.jpeg\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/DASH_1080\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/DASH_720\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/DASH_480\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/audio.m4a\n",
      "Broken URL: https://v.redd.it/r2zvecs24m1g1/DASH_audio\n",
      "Broken URL: https://i.redd.it/r2zvecs24m1g1.mp4\n",
      "Broken URL: https://i.redd.it/r2zvecs24m1g1.webm\n",
      "Broken URL: https://i.redd.it/r2zvecs24m1g1.gif\n",
      "Broken URL: https://i.redd.it/r2zvecs24m1g1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_5_img.jpg\n",
      "Broken URL: https://i.redd.it/r2zvecs24m1g1.jpeg\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/DASH_1080\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/DASH_720\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/DASH_480\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/audio.m4a\n",
      "Broken URL: https://v.redd.it/i9ju8ep24m1g1/DASH_audio\n",
      "Broken URL: https://i.redd.it/i9ju8ep24m1g1.mp4\n",
      "Broken URL: https://i.redd.it/i9ju8ep24m1g1.webm\n",
      "Broken URL: https://i.redd.it/i9ju8ep24m1g1.gif\n",
      "Broken URL: https://i.redd.it/i9ju8ep24m1g1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_5_img.jpg\n",
      "Broken URL: https://i.redd.it/i9ju8ep24m1g1.jpeg\n",
      "[  5/10] IMAGE 1 - 6 WORKING URLs!\n",
      "  6 files saved!\n",
      "[  5/10] COMPLETE\n",
      "\n",
      "[  6/10] Now do you understand why????\"...\n",
      "[  6/10] Post ID: 1jmi4sk\n",
      "[  6/10] Saturday, March 29, 2025 09:53 AM UTC (8 months ago)\n",
      "[  6/10] Testing 1 candidate URLs...\n",
      "Download error: 'str' object has no attribute \n",
      "[  6/10] IMAGE 1 - 1 WORKING URLs!\n",
      "  0 files saved!\n",
      "[  6/10] COMPLETE\n",
      "\n",
      "[  7/10] Genie Wiley learning how to talk in 1970. She spent the firs...\n",
      "[  7/10] Post ID: 1jl5bsz\n",
      "[  7/10] Thursday, March 27, 2025 03:34 PM UTC (8 months ago)\n",
      "[  7/10] Testing 1 candidate URLs...\n",
      "Download error: 'str' object has no attribute \n",
      "[  7/10] VIDEO 1 - 1 WORKING URLs!\n",
      "  0 files saved!\n",
      "[  7/10] Extracting audio...\n",
      "Running yt-dlp: bestchairfordevelopers_top_pastyear_7_audio.m4a\n",
      "Audio saved: bestchairfordevelopers_top_pastyear_7_audio.m4a (2143.5KB)\n",
      "[  7/10] Audio: bestchairfordevelopers_top_pastyear_7_audio.m4a\n",
      "[  7/10] COMPLETE\n",
      "\n",
      "[  8/10] Full Circle or Karma? : Trump Edition...\n",
      "[  8/10] Post ID: 1orql91\n",
      "[  8/10] Saturday, November 08, 2025 03:20 PM UTC (1 month ago)\n",
      "[  8/10] Testing 48 candidate URLs...\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/DASH_1080\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/DASH_720\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/DASH_480\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/audio.m4a\n",
      "Broken URL: https://v.redd.it/7uhg2ho4l10g1/DASH_audio\n",
      "Broken URL: https://i.redd.it/7uhg2ho4l10g1.mp4\n",
      "Broken URL: https://i.redd.it/7uhg2ho4l10g1.webm\n",
      "Broken URL: https://i.redd.it/7uhg2ho4l10g1.gif\n",
      "Broken URL: https://i.redd.it/7uhg2ho4l10g1.png\n",
      "IMAGE.jpg: bestchairfordevelopers_top_pastyear_8_img.jpg (466.4KB)\n",
      "Broken URL: https://i.redd.it/7uhg2ho4l10g1.jpeg\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/DASH_1080\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/DASH_720\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/DASH_480\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/audio.m4a\n",
      "Broken URL: https://v.redd.it/8eub3eo5l10g1/DASH_audio\n",
      "Broken URL: https://i.redd.it/8eub3eo5l10g1.mp4\n",
      "Broken URL: https://i.redd.it/8eub3eo5l10g1.webm\n",
      "Broken URL: https://i.redd.it/8eub3eo5l10g1.gif\n",
      "Broken URL: https://i.redd.it/8eub3eo5l10g1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_8_img.jpg\n",
      "Broken URL: https://i.redd.it/8eub3eo5l10g1.jpeg\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/DASH_1080.mp4\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/DASH_720.mp4\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/DASH_480.mp4\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/DASH_360.mp4\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/DASH_1080\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/DASH_720\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/DASH_480\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/DASH_audio.mp4\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/audio.m4a\n",
      "Broken URL: https://v.redd.it/azcja4n6l10g1/DASH_audio\n",
      "Broken URL: https://i.redd.it/azcja4n6l10g1.mp4\n",
      "Broken URL: https://i.redd.it/azcja4n6l10g1.webm\n",
      "Broken URL: https://i.redd.it/azcja4n6l10g1.gif\n",
      "Broken URL: https://i.redd.it/azcja4n6l10g1.png\n",
      "SKIP: bestchairfordevelopers_top_pastyear_8_img.jpg\n",
      "Broken URL: https://i.redd.it/azcja4n6l10g1.jpeg\n",
      "[  8/10] IMAGE 1 - 3 WORKING URLs!\n",
      "  3 files saved!\n",
      "[  8/10] COMPLETE\n",
      "\n",
      "[  9/10] Recruiter just copied and pasted the job description and did...\n",
      "[  9/10] Post ID: 1ij1zk8\n",
      "[  9/10] Thursday, February 06, 2025 02:01 PM UTC (10 months ago)\n",
      "[  9/10] Testing 1 candidate URLs...\n",
      "Download error: 'str' object has no attribute \n",
      "[  9/10] IMAGE 1 - 1 WORKING URLs!\n",
      "  0 files saved!\n",
      "[  9/10] COMPLETE\n",
      "\n",
      "[ 10/10] r/Conservative imploding after Donald \"No Wars\" Trump strike...\n",
      "[ 10/10] Post ID: 1lhenmr\n",
      "[ 10/10] Sunday, June 22, 2025 05:48 AM UTC (5 months ago)\n",
      "[ 10/10] 1530 chars\n",
      "[ 10/10] No visuals\n",
      "[ 10/10] COMPLETE\n",
      "\n",
      "SAVED 10/10 posts: ..\\data\\reddit\\bestchairfordevelopers_top_pastyear.csv\n",
      "\n",
      "=== AUTO-MERGE VIDEOS + AUDIO ===\n",
      "Scanning for video+audio pairs...\n",
      "SUCCESS: bestchairfordevelopers_top_pastyear_7_vid.mp4 + bestchairfordevelopers_top_pastyear_7_audio.m4a ‚Üí bestchairfordevelopers_top_pastyear_7_video.mp4\n",
      "Skipping non-matching video: oban_star_racers_relevance_all_time_2_vid\n",
      "Skipping non-matching video: oban_star_racers_relevance_all_time_4_vid\n",
      "Skipping non-matching video: ocarinaoftimezeldavideorelevancealltime5vid\n",
      "Skipping non-matching video: ocarinaoftimezeldavideo_relevance_alltime_5_vid\n",
      "MERGE COMPLETE: 1 success, 0 failed / 5 videos\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def createsamplemaindata(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keywordclean = keyword.replace(\" \", \"\")\n",
    "    sampledata = {\n",
    "        'posttitle': [f\"{keyword.title()} post {i+1}\" for i in range(limit)],\n",
    "        'postlink': [f\"https://www.reddit.com/r/{keyword}i/{i+1}/title/{i+1}\" for i in range(limit)],\n",
    "        'postid': [f\"{i+1}\" for i in range(limit)]\n",
    "    }\n",
    "    INPUTFILE = f\"..\\\\data\\\\reddit\\\\{keywordclean}main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUTFILE), exist_ok=True)\n",
    "    pd.DataFrame(sampledata).to_csv(INPUTFILE, index=False)\n",
    "    print(f\"Created sample MAIN data: {limit} posts -> {INPUTFILE}\")\n",
    "    return INPUTFILE\n",
    "\n",
    "def getperiodparam(periodfilter):\n",
    "    \"\"\"Convert display text to Reddit API 't' parameter\"\"\"\n",
    "    periodmap = {\n",
    "        'All time': 'all', 'Past year': 'year', 'Past month': 'month',\n",
    "        'Past week': 'week', 'Today': 'day', 'Past hour': 'hour'\n",
    "    }\n",
    "    return periodmap.get(periodfilter, 'month')\n",
    "\n",
    "def fetchredditpostssearch(searchkeyword, filterhot, limit=50, periodfilter=None):\n",
    "    \"\"\"Fetch Reddit posts via API\"\"\"\n",
    "    print(f\"Fetching UP TO {limit} {filterhot} posts for search='{searchkeyword}'...\")\n",
    "    if periodfilter:\n",
    "        print(f\"Time filter: {periodfilter}\")\n",
    "    encodedkeyword = urllib.parse.quote(searchkeyword)\n",
    "    periodparam = getperiodparam(periodfilter) if periodfilter else 'month'\n",
    "    searchurl = f\"https://www.reddit.com/search.json?q={encodedkeyword}&sort={filterhot}&limit=100&t={periodparam}&type=link\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    try:\n",
    "        print(f\"API: q={encodedkeyword}&sort={filterhot}&t={periodparam}...\")\n",
    "        response = requests.get(searchurl, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"API returned {available} posts available\")\n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                postdata = post['data']\n",
    "                posts.append({\n",
    "                    'posttitle': postdata.get('title', 'NA'),\n",
    "                    'postlink': f\"https://www.reddit.com{postdata.get('permalink', '')}\",\n",
    "                    'postid': postdata.get('id', 'NA'),\n",
    "                    'numvotes': postdata.get('score', 0),\n",
    "                    'numcomments': postdata.get('num_comments', 0),\n",
    "                    'filter': filterhot,\n",
    "                    'periodfilter': periodfilter or 'NA'\n",
    "                })\n",
    "        actualposts = len(posts)\n",
    "        print(f\"SUCCESS: {actualposts}/{limit} {filterhot} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def getviewableimageurl(url):\n",
    "    \"\"\"ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    urllower = url.lower()\n",
    "    if 'i.redd.it' in urllower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    match = re.search(r'preview\\.redd\\.it/[a-z0-9]+', urllower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/[a-z0-9]+', urllower)\n",
    "    if not match:\n",
    "        match = re.search(r'[a-z0-9]{13}\\.', urllower)\n",
    "    if match:\n",
    "        mediaid = match.group(1)\n",
    "        return f\"https://i.redd.it/{mediaid}.png\"\n",
    "    return url\n",
    "\n",
    "def formatpostdate(createdutc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable datetime\"\"\"\n",
    "    if not createdutc or createdutc == 'NA':\n",
    "        return 'NA', 'NA'\n",
    "    try:\n",
    "        timestamp = float(createdutc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        postdate = dt.strftime('%A, %B %d, %Y')\n",
    "        posttime = dt.strftime('%I:%M %p UTC')\n",
    "        return postdate, posttime\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculatetimeago(postdatestr, posttimestr):\n",
    "    \"\"\"Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if postdatestr == 'NA' or posttimestr == 'NA':\n",
    "        return 'NA'\n",
    "    try:\n",
    "        datetimestr = f\"{postdatestr} {posttimestr.replace(' UTC', '')}\"\n",
    "        postdt = datetime.strptime(datetimestr, '%A, %B %d, %Y %I:%M %p')\n",
    "        now = datetime.now()\n",
    "        delta = now - postdt\n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year{'s' if years != 1 else ''}\")\n",
    "        elif months > 0:\n",
    "            parts.append(f\"{months} month{'s' if months != 1 else ''}\")\n",
    "        elif weeks > 0:\n",
    "            parts.append(f\"{weeks} week{'s' if weeks != 1 else ''}\")\n",
    "        elif days > 0:\n",
    "            parts.append(f\"{days} day{'s' if days != 1 else ''}\")\n",
    "        elif hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour{'s' if hours != 1 else ''}\")\n",
    "        elif not parts:\n",
    "            return \"just now\"\n",
    "        timeago = \" \".join(parts) + \" ago\"\n",
    "        return timeago\n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def getenhancedmediacandidates(mediaid):\n",
    "    \"\"\"Generate ALL possible media URLs for a mediaid (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_720\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_480\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{mediaid}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{mediaid}/DASH_audio\",\n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{mediaid}.mp4\",\n",
    "        f\"https://i.redd.it/{mediaid}.webm\",\n",
    "        # IMAGES/GIFs\n",
    "        f\"https://i.redd.it/{mediaid}.gif\",\n",
    "        f\"https://i.redd.it/{mediaid}.png\",\n",
    "        f\"https://i.redd.it/{mediaid}.jpg\",\n",
    "        f\"https://i.redd.it/{mediaid}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def testurlworking(url, headersbrowser, timeout=10):\n",
    "    \"\"\"Returns workingurl, contenttype, fileext or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headersbrowser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            contenttype = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0)) or 0\n",
    "            if size > 1000 and any(mediatype in contenttype for mediatype in ['video/', 'image/', 'audio/']):\n",
    "                if 'video/' in contenttype:\n",
    "                    fileext = '.mp4' if 'mp4' in contenttype else '.webm'\n",
    "                elif 'image/' in contenttype:\n",
    "                    if 'gif' in contenttype:\n",
    "                        fileext = '.gif'\n",
    "                    elif 'png' in contenttype:\n",
    "                        fileext = '.png'\n",
    "                    elif 'jpeg' in contenttype:\n",
    "                        fileext = '.jpg'\n",
    "                    else:\n",
    "                        fileext = '.jpg'\n",
    "                else:\n",
    "                    fileext = '.bin'\n",
    "                return url, contenttype, fileext\n",
    "        resp = requests.get(url, headers=headersbrowser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            contenttype = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            if size > 1000 and any(mediatype in contenttype for mediatype in ['video/', 'image/', 'audio/']):\n",
    "                if 'video/' in contenttype:\n",
    "                    fileext = '.mp4' if 'mp4' in contenttype else '.webm'\n",
    "                elif 'image/' in contenttype:\n",
    "                    if 'gif' in contenttype:\n",
    "                        fileext = '.gif'\n",
    "                    elif 'png' in contenttype:\n",
    "                        fileext = '.png'\n",
    "                    elif 'jpeg' in contenttype:\n",
    "                        fileext = '.jpg'\n",
    "                    else:\n",
    "                        fileext = '.jpg'\n",
    "                else:\n",
    "                    fileext = '.bin'\n",
    "                return url, contenttype, fileext\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def downloadredditaudioonly(posturl, keywordclean, filterparam, periodstr, postnumber, audiofolder):\n",
    "    \"\"\"EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    postid = re.search(r'comments/([a-zA-Z0-9]+)', posturl)\n",
    "    if not postid:\n",
    "        return None\n",
    "    postid = postid.group(1)\n",
    "    audiofilename = f\"{keywordclean}_{filterparam}_{periodstr}_{postnumber}_audio.m4a\"\n",
    "    audiopath = os.path.join(audiofolder, audiofilename)\n",
    "    if os.path.exists(audiopath):\n",
    "        print(f\"Audio exists: {audiofilename}\")\n",
    "        return audiopath\n",
    "    cmd = [\n",
    "        'yt-dlp', '--extract-audio',\n",
    "        '--audio-format', 'm4a',\n",
    "        '--audio-quality', '0',\n",
    "        '--embed-metadata',\n",
    "        '-o', audiopath,\n",
    "        posturl\n",
    "    ]\n",
    "    try:\n",
    "        print(f\"Running yt-dlp: {audiofilename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audiopath):\n",
    "            filesize = os.path.getsize(audiopath) / 1024\n",
    "            print(f\"Audio saved: {audiofilename} ({filesize:.1f}KB)\")\n",
    "            return audiopath\n",
    "        else:\n",
    "            print(f\"yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"yt-dlp error: {str(e)[:50]}\")\n",
    "    return None\n",
    "\n",
    "def downloadvisualauto(postnumber, visualtype, visualurls, keywordclean, filterparam, periodstr, visualsfolder, videosfolder):\n",
    "    \"\"\"EXACT naming: keyword_filter_period_postnumber_[gif|vid|img].[ext]\"\"\"\n",
    "    downloadedfiles = []\n",
    "    headersbrowser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    workingurls = []\n",
    "    basefilename = f\"{keywordclean}_{filterparam}_{periodstr}_{postnumber}\"\n",
    "    \n",
    "    if visualtype == 'CAROUSEL':\n",
    "        for seqidx, url in enumerate(visualurls, 1):\n",
    "            result = testurlworking(url, headersbrowser)\n",
    "            if result:\n",
    "                workingurl, contenttype, fileext = result\n",
    "                workingurls.append(workingurl)\n",
    "                if 'video/' in contenttype:\n",
    "                    targetfolder = videosfolder\n",
    "                    filename = f\"{basefilename}_vid{fileext}\"\n",
    "                elif 'gif' in contenttype:\n",
    "                    targetfolder = visualsfolder\n",
    "                    filename = f\"{basefilename}_gif{fileext}\"\n",
    "                else:  # image\n",
    "                    targetfolder = visualsfolder\n",
    "                    filename = f\"{basefilename}_img{fileext}\"\n",
    "                filepath = os.path.join(targetfolder, filename)\n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"SKIP: {filename}\")\n",
    "                    downloadedfiles.append(filename)\n",
    "                    continue\n",
    "                try:\n",
    "                    resp = requests.get(workingurl, headers=headersbrowser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        mediatype = contenttype.split('/')[0].upper()\n",
    "                        print(f\"{mediatype}{fileext}: {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloadedfiles.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"Broken URL: {url[:60]}\")\n",
    "    else:  # SINGLE\n",
    "        for url in visualurls:\n",
    "            result = testurlworking(url, headersbrowser)\n",
    "            if result:\n",
    "                workingurl, contenttype, fileext = result\n",
    "                workingurls.append(workingurl)\n",
    "                if 'video/' in contenttype:\n",
    "                    targetfolder = videosfolder\n",
    "                    filename = f\"{basefilename}_vid{fileext}\"\n",
    "                elif 'gif' in contenttype:\n",
    "                    targetfolder = visualsfolder\n",
    "                    filename = f\"{basefilename}_gif{fileext}\"\n",
    "                else:  # image\n",
    "                    targetfolder = visualsfolder\n",
    "                    filename = f\"{basefilename}_img{fileext}\"\n",
    "                filepath = os.path.join(targetfolder, filename)\n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"SKIP: {filename}\")\n",
    "                    downloadedfiles.append(filename)\n",
    "                    break\n",
    "                try:\n",
    "                    resp = requests.get(workingurl, headers=headersbrowser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        mediatype = contenttype.split('/')[0].UPPER()\n",
    "                        print(f\"{mediatype}{fileext}: {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloadedfiles.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        print(\"Success! Done with this post\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Download error: {str(e)[:30]}\")\n",
    "                    break\n",
    "    return downloadedfiles, workingurls\n",
    "\n",
    "def extractpostid(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    url = str(url).strip()\n",
    "    match = re.search(r'comments/([a-zA-Z0-9]+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def getvisualtypecount(visual):\n",
    "    if visual in ['NA', 'MEDIAERROR', 'ERROR']:\n",
    "        return 'NONE', 0\n",
    "    visualstr = str(visual).lower()\n",
    "    if any(x in visualstr for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "        return 'VIDEO', 1\n",
    "    if '\\n' in visualstr:\n",
    "        lines = visualstr.splitlines()\n",
    "        gifcount = sum(1 for line in lines if '.gif' in line.lower())\n",
    "        totalcount = len(lines)\n",
    "        if gifcount == totalcount:\n",
    "            return 'CAROUSEL/GIF', totalcount\n",
    "        elif gifcount > 0:\n",
    "            return 'CAROUSEL/MIXED', totalcount\n",
    "        else:\n",
    "            return 'CAROUSEL/IMAGE', totalcount\n",
    "    if '.gif' in visualstr or 'gif' in visualstr:\n",
    "        return 'GIF', 1\n",
    "    if 'i.redd.it' in visualstr or any(ext in visualstr for ext in ['.jpg', '.png']):\n",
    "        return 'IMAGE', 1\n",
    "    return 'OTHER', 1\n",
    "\n",
    "def calculatetextlength(description):\n",
    "    if not description or description in ['NA', 'ERROR', 'INVALIDLINK']:\n",
    "        return 0\n",
    "    text = re.sub(r'https?://\\S+', '', str(description))\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return len(text)\n",
    "\n",
    "def extractvisualurls(postinfo):\n",
    "    \"\"\"ENHANCED: Comprehensive visual extraction with carousel/video/GIF support\"\"\"\n",
    "    visualurls = []\n",
    "    try:\n",
    "        if postinfo.get('is_video') and postinfo.get('media', {}).get('reddit_video'):\n",
    "            fallbackurl = postinfo['media']['reddit_video'].get('fallback_url')\n",
    "            if fallbackurl:\n",
    "                visualurls.append(fallbackurl)\n",
    "                return visualurls\n",
    "        if any(domain in postinfo.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "            visualurls.append(postinfo['url'])\n",
    "            return visualurls\n",
    "        gallerydata = postinfo.get('gallery_data')\n",
    "        if gallerydata and gallerydata.get('items'):\n",
    "            for item in gallerydata['items']:\n",
    "                if isinstance(item, dict) and 'media_id' in item:\n",
    "                    mediaid = item['media_id']\n",
    "                    mediacandidates = getenhancedmediacandidates(mediaid)\n",
    "                    for candidateurl in mediacandidates:\n",
    "                        visualurls.append(candidateurl)\n",
    "            if visualurls:\n",
    "                return visualurls\n",
    "        posturl = postinfo.get('url', '')\n",
    "        viewableurl = getviewableimageurl(posturl)\n",
    "        if viewableurl and 'i.redd.it' in viewableurl:\n",
    "            return [viewableurl]\n",
    "        if postinfo.get('preview', {}).get('images'):\n",
    "            for img in postinfo['preview']['images']:\n",
    "                sourceurl = img.get('source', {}).get('url', '')\n",
    "                if sourceurl:\n",
    "                    viewableurl = getviewableimageurl(sourceurl)\n",
    "                    if 'i.redd.it' in viewableurl:\n",
    "                        return [viewableurl]\n",
    "        if postinfo.get('thumbnail') and 'i.redd.it' in postinfo['thumbnail']:\n",
    "            return [postinfo['thumbnail']]\n",
    "    except:\n",
    "        pass\n",
    "    return visualurls\n",
    "\n",
    "def extractpostdetailscomplete(keyword, filterhot, limit=50, periodfilter=None):\n",
    "    \"\"\"MAIN FUNCTION with EXACT naming convention\"\"\"\n",
    "    keywordclean = keyword.replace(\" \", \"\")\n",
    "    periodstr = periodfilter.replace(\" \", \"\").lower() if periodfilter else \"alltime\"\n",
    "    INPUTFILE = f\"..\\\\data\\\\reddit\\\\{keywordclean}main.csv\"\n",
    "    OUTPUTFILE = f\"..\\\\data\\\\reddit\\\\{keywordclean}_{filterhot}_{periodstr}.csv\"\n",
    "    VIDEOSFOLDER = \"..\\\\data\\\\videos\"  \n",
    "    VISUALSFOLDER = \"..\\\\data\\\\visuals\" \n",
    "    AUDIOFOLDER = \"..\\\\data\\\\audio\"  \n",
    "    \n",
    "    print(f\"Fetching EXACTLY {limit} {filterhot} posts. Period: {periodfilter or 'All time'}...\")\n",
    "    df = fetchredditpostssearch(keyword, filterhot, limit, periodfilter)\n",
    "    if df is None or df.empty:\n",
    "        print(\"Search failed. Using sample data...\")\n",
    "        createsamplemaindata(keywordclean, limit)\n",
    "        df = pd.read_csv(INPUTFILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUTFILE), exist_ok=True)\n",
    "        df.to_csv(INPUTFILE, index=False)\n",
    "        print(f\"Saved {len(df)} REAL posts: {INPUTFILE}\")\n",
    "    \n",
    "    totalposts = len(df)\n",
    "    print(f\"PROCESSING {totalposts} posts -> {OUTPUTFILE}\")\n",
    "    print(f\"VIDEOS(mute): {VIDEOSFOLDER} (keyword_filter_period_postnumber_vid.mp4)\")\n",
    "    print(f\"VISUALS: {VISUALSFOLDER} (keyword_filter_period_postnumber_[gif|img].*)\")\n",
    "    print(f\"AUDIO: {AUDIOFOLDER} (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(\"100%\")\n",
    "    \n",
    "    os.makedirs(VIDEOSFOLDER, exist_ok=True)\n",
    "    os.makedirs(VISUALSFOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIOFOLDER, exist_ok=True)\n",
    "    \n",
    "    newdata = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"[{idx+1:3d}/{totalposts}]\"\n",
    "        posttitle = str(row['posttitle'])[:60]\n",
    "        postnumber = idx + 1\n",
    "        print(f\"{progress} {posttitle}...\")\n",
    "        \n",
    "        postdata = {\n",
    "            'posttitle': row.get('posttitle', 'NA'),\n",
    "            'postlink': row.get('postlink', 'NA'),\n",
    "            'postid': 'NA',\n",
    "            'numvotes': row.get('numvotes', 'NA'),\n",
    "            'numcomments': row.get('numcomments', 'NA'),\n",
    "            'filter': filterhot,\n",
    "            'periodfilter': periodfilter or 'NA',\n",
    "            'postdate': 'NA',\n",
    "            'posttime': 'NA',\n",
    "            'timeago': 'NA',\n",
    "            'textlength': 0,\n",
    "            'postdescription': 'NA',\n",
    "            'postvisual': 'NA',\n",
    "            'visualtype': 'NONE',\n",
    "            'visualcount': 0,\n",
    "            'downloadedfiles': 'NA',\n",
    "            'audiofile': 'NA'\n",
    "        }\n",
    "        \n",
    "        postid = extractpostid(row['postlink'])\n",
    "        if not postid:\n",
    "            print(f\"{progress} Invalid link - SKIPPED\")\n",
    "            newdata.append(postdata)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"{progress} Post ID: {postid}\")\n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{postid}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    postinfo = data[0]['data']['children'][0]['data']\n",
    "                    postdata.update({\n",
    "                        'postid': postid,\n",
    "                        'numvotes': str(postinfo.get('score', 'NA')),\n",
    "                    })\n",
    "                    createdutc = postinfo.get('created_utc')\n",
    "                    postdate, posttime = formatpostdate(createdutc)\n",
    "                    postdata['postdate'] = postdate\n",
    "                    postdata['posttime'] = posttime\n",
    "                    postdatatimeago = calculatetimeago(postdate, posttime)\n",
    "                    print(f\"{progress} {postdate} {posttime[:12]} ({postdatatimeago})\")\n",
    "                    \n",
    "                    selftext = postinfo.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        postdata['postdescription'] = selftext\n",
    "                        postdata['textlength'] = calculatetextlength(selftext)\n",
    "                        print(f\"{progress} {postdata['textlength']} chars\")\n",
    "                    \n",
    "                    allcandidateurls = extractvisualurls(postinfo)\n",
    "                    if allcandidateurls:\n",
    "                        print(f\"{progress} Testing {len(allcandidateurls)} candidate URLs...\")\n",
    "                        downloadedfiles, workingurls = downloadvisualauto(\n",
    "                            postnumber, 'CAROUSEL' if len(allcandidateurls) > 1 else 'IMAGE',\n",
    "                            allcandidateurls, keywordclean, filterhot, periodstr, VISUALSFOLDER, VIDEOSFOLDER\n",
    "                        )\n",
    "                        if workingurls:\n",
    "                            postdata['postvisual'] = \"; \".join(workingurls)\n",
    "                            vtype, vcount = getvisualtypecount(postdata['postvisual'])\n",
    "                            postdata.update({\n",
    "                                'visualtype': vtype,\n",
    "                                'visualcount': vcount,\n",
    "                                'downloadedfiles': \"; \".join(downloadedfiles) if downloadedfiles else 'ERROR'\n",
    "                            })\n",
    "                            print(f\"{progress} {vtype} {vcount} - {len(workingurls)} WORKING URLs!\")\n",
    "                            print(f\"  {len(downloadedfiles)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"{progress} No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"{progress} No visuals\")\n",
    "                    \n",
    "                    if postdata['visualtype'] in ['VIDEO'] and postid:\n",
    "                        print(f\"{progress} Extracting audio...\")\n",
    "                        audiopath = downloadredditaudioonly(\n",
    "                            postdata['postlink'], keywordclean, filterhot, periodstr, postnumber, AUDIOFOLDER\n",
    "                        )\n",
    "                        if audiopath:\n",
    "                            audiofilename = os.path.basename(audiopath)\n",
    "                            postdata['audiofile'] = audiofilename\n",
    "                            print(f\"{progress} Audio: {audiofilename}\")\n",
    "                        else:\n",
    "                            print(f\"{progress} No audio extracted\")\n",
    "                    print(f\"{progress} COMPLETE\")\n",
    "                else:\n",
    "                    print(f\"{progress} No post data\")\n",
    "            else:\n",
    "                print(f\"{progress} HTTP {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{progress} Error: {str(e)[:40]}\")\n",
    "        \n",
    "        newdata.append(postdata)\n",
    "        time.sleep(2.5)\n",
    "        print(\"\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUTFILE), exist_ok=True)\n",
    "    newdf = pd.DataFrame(newdata, columns=[\n",
    "        'posttitle', 'postlink', 'postid', 'numvotes', 'numcomments', 'filter', 'periodfilter',\n",
    "        'postdate', 'posttime', 'timeago', 'textlength', 'postdescription', 'postvisual',\n",
    "        'visualtype', 'visualcount', 'downloadedfiles', 'audiofile'\n",
    "    ])\n",
    "    newdf.to_csv(OUTPUTFILE, index=False)\n",
    "    print(f\"SAVED {len(newdf)}/{limit} posts: {OUTPUTFILE}\")\n",
    "    \n",
    "    # Auto-merge videos + audio\n",
    "    autovideomergeall(keywordclean, filterhot, periodstr)\n",
    "    return newdf\n",
    "\n",
    "def autovideomergeall(keywordclean, filterhot, periodstr):\n",
    "    \"\"\"Merge: keyword_filter_period_postnumber_vid.mp4 + _audio.m4a ‚Üí _video.mp4\"\"\"\n",
    "    basedir = Path(\"..\")\n",
    "    videosdir = basedir / \"data\" / \"videos\"\n",
    "    audiodir = basedir / \"data\" / \"audio\"\n",
    "    visualsdir = basedir / \"data\" / \"visuals\"\n",
    "    visualsdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n=== AUTO-MERGE VIDEOS + AUDIO ===\")\n",
    "    print(\"Scanning for video+audio pairs...\")\n",
    "    videofiles = list(videosdir.glob(\"*.mp4\"))\n",
    "    successcount, failcount = 0, 0\n",
    "    \n",
    "    for videopath in videofiles:\n",
    "        videoname = videopath.stem\n",
    "        # Match: keyword_filter_period_postnumber_vid\n",
    "        match = re.search(rf'{keywordclean}_{filterhot}_{periodstr}_(\\d+)_vid', videoname)\n",
    "        if not match:\n",
    "            print(f\"Skipping non-matching video: {videoname}\")\n",
    "            continue\n",
    "        postnumber = match.group(1)\n",
    "        audiopath = audiodir / f\"{keywordclean}_{filterhot}_{periodstr}_{postnumber}_audio.m4a\"\n",
    "        outputpath = visualsdir / f\"{keywordclean}_{filterhot}_{periodstr}_{postnumber}_video.mp4\"\n",
    "        \n",
    "        if outputpath.exists():\n",
    "            print(f\"Already exists - SKIPPING: {outputpath.name}\")\n",
    "            successcount += 1\n",
    "            continue\n",
    "        if not audiopath.exists():\n",
    "            print(f\"Audio missing - SKIPPING: {audiopath.name}\")\n",
    "            failcount += 1\n",
    "            continue\n",
    "        \n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\", \"-y\",\n",
    "            \"-i\", str(videopath), \"-i\", str(audiopath),\n",
    "            \"-map\", \"0:v:0\", \"-map\", \"1:a:0\",\n",
    "            \"-c:v\", \"copy\", \"-c:a\", \"aac\", \"-shortest\",\n",
    "            str(outputpath)\n",
    "        ]\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "            if result.returncode == 0 and outputpath.exists():\n",
    "                print(f\"SUCCESS: {videopath.name} + {audiopath.name} ‚Üí {outputpath.name}\")\n",
    "                successcount += 1\n",
    "            else:\n",
    "                print(f\"FFmpeg failed: {result.stderr[:100]}\")\n",
    "                failcount += 1\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"FFmpeg timeout\")\n",
    "            failcount += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)[:50]}\")\n",
    "            failcount += 1\n",
    "    \n",
    "    print(f\"MERGE COMPLETE: {successcount} success, {failcount} failed / {len(videofiles)} videos\")\n",
    "    print(\"==================================\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"REDDIT EXTRACTOR + EXACT NAMING + AUTO-MERGE!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword (default: music): \").strip() or \"music\"\n",
    "    print(\"Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter (1): \").strip() or \"1\"\n",
    "    filtermap = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filterhot = filtermap.get(choice, 'hot')\n",
    "    \n",
    "    periodfilter = None\n",
    "    if filterhot in ['relevance', 'top', 'comments']:\n",
    "        print(\"PERIOD FILTER:\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        periodchoice = input(\"Choose period (2): \").strip() or \"2\"\n",
    "        periodmap = {'1': 'All time', '2': 'Past year', '3': 'Past month', '4': 'Past week', '5': 'Today', '6': 'Past hour'}\n",
    "        periodfilter = periodmap.get(periodchoice, 'Past year')\n",
    "        print(f\"Using period: {periodfilter}\")\n",
    "    \n",
    "    limitinput = input(\"How many posts? (1-100, default=20): \").strip()\n",
    "    limit = int(limitinput) if limitinput.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nEXACT NAMING:\")\n",
    "    print(\"- GIF: keyword_filter_period_postnumber_gif.gif\")\n",
    "    print(\"- MUTE VID: keyword_filter_period_postnumber_vid.mp4\") \n",
    "    print(\"- AUDIO: keyword_filter_period_postnumber_audio.m4a\")\n",
    "    print(\"- VIDEO+AUDIO: keyword_filter_period_postnumber_video.mp4\")\n",
    "    print(\"- IMG: keyword_filter_period_postnumber_img.jpg/png\")\n",
    "    \n",
    "    result = extractpostdetailscomplete(keyword, filterhot, limit, periodfilter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
