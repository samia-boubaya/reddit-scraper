{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d29526d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# README\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5a37d",
   "metadata": {},
   "source": [
    "# ‚≠ê **Reddit Scraper for Posts** \n",
    "\n",
    "A Python-based scraper to extract high-engagement Reddit posts related to **music**. Automatically collects post data, including title, description, votes, comments, visuals (images/videos), and metadata. Ideal for content analysis, research, or curating trending music posts. Supports filtering by **Hot, Top, New, Comments, and Relevance**, with time-based searches like **past hour**, **week**, **month**, or **all-time**.  \n",
    "\n",
    "## Features\n",
    "- Extract post metadata: title, link, ID, votes, comments, post date & time.\n",
    "- Download visuals (image, video, carousel) with automatic file naming.\n",
    "- Filter posts by engagement, relevance, and time period.\n",
    "- Focused on **music-related content**, including theory, instruments, production, memes, and historical highlights.\n",
    "\n",
    "Perfect for researchers, content creators, or music enthusiasts looking to track trends and high-performing posts on Reddit.\n",
    "\n",
    "\n",
    "## üéµ **Reddit/Music Threads Guidelines**\n",
    "\n",
    "**Conditions:**  \n",
    "- Description: 100‚Äì300 characters  \n",
    "- Posts must be HOT & relevant to music  \n",
    "- Downloadable visuals (images/videos)  \n",
    "- Only include posts with ‚â•100 shares  \n",
    "- High engagement / top-performing posts  \n",
    "\n",
    "**Essentials:**  \n",
    "- Keyword: `music`  \n",
    "- Focus: short text or image, simplicity, relevance  \n",
    "\n",
    "**Relevance Categories:**  \n",
    "- Music theory, instruments, production, teaching  \n",
    "- Historical highlights, facts, memes, communities  \n",
    "- Target: anyone learning, playing, or producing music  \n",
    "\n",
    "**Threads Post Rules:**  \n",
    "- Video under 15 seconds  \n",
    "- If both image & video exist, use only video  \n",
    "\n",
    "\n",
    "\n",
    "## ‚≠ê **Reddit Post Data Fields**\n",
    "\n",
    "The main data fields to extract from the Reddit Post :\n",
    "| Field Name         | Python Data Type | Description |\n",
    "|--------------------|-----------------|-------------|\n",
    "| <span style=\"color:green\">**post_title**</span>     | `str`            | Title of the post. |\n",
    "| **post_link**      | `str`            | Direct URL to the post. |\n",
    "| **post_id**        | `str`            | Unique identifier for the post. |\n",
    "| <span style=\"color:red\">**num_votes**</span>      | `int`            | Total number of upvotes the post received. |\n",
    "| <span style=\"color:red\">**num_comments**</span>   | `int`            | Total number of comments on the post. |\n",
    "| **text_length**    | `int`            | Character count of the post‚Äôs text description. |\n",
    "| <span style=\"color:green\">**post_description**</span> | `str`          | Full text description or caption of the post. |\n",
    "| **post_date**      | `date`           | Calendar date when the post was published. |\n",
    "| **post_time**      | `str`            | Time (with timezone) when the post was published. |\n",
    "| <span style=\"color:green\">**post_visual**</span>    | `list[str]`      | Direct URLs to visual content (images or videos). |\n",
    "| **visual_type**    | `str`            | Type of visual content: `\"IMAGE\"`, `\"VIDEO\"`,`\"CAROUSEL\"`, or `\"NONE\"`. |\n",
    "| **visual_count**   | `int`            | Number of visual items in the post. |\n",
    "| <span style=\"color:blue\">**filter**</span>         | `str`            | Reddit search filter used. Must be one of: `\"Relevance\"`, `\"Hot\"`, `\"Top\"`, `\"New\"`, `\"Comment count\"`. |\n",
    "| <span style=\"color:blue\">**keyword**</span>        | `str`            | Search keyword or query. |\n",
    "| <span style=\"color:blue\">**limit**</span>          | `int`            | Number of posts requested. |\n",
    "| <span style=\"color:blue\">**period**</span>         | `str`            | Time filter used when searching posts. Must be one of: `\"All time\"`, `\"Past year\"`, `\"Past month\"`, `\"Past week\"`, `\"Today\"`, `\"Past hour\"`. |\n",
    "| <span style=\"color:red\">**time_ago**</span>       | `str`            | Relative time since the post was published (e.g., `\"3 hours ago\"`). |\n",
    "\n",
    "\n",
    "\n",
    "## ‚≠ê **POST LINK - EXPLORING**\n",
    "\n",
    "| Link             | Keyword       | Filter       |\n",
    "|------------------------|-----------------------|-----------------------|\n",
    "| https://www.reddit.com/search/?q=music             |  ```music```  | ```Relevance``` by default |\n",
    "| https://www.reddit.com/search/?q=***keyword***             |  ```keyword```  | ```Relevance``` by default |\n",
    "| https://www.reddit.com/search/?q=music&type=posts&sort=hot             |  ```music``` |  ```HOT```  |\n",
    "| https://www.reddit.com/search/?q=music&type=posts&sort=top             |  ```music``` |  ```Top```  |\n",
    "| https://www.reddit.com/search/?q=***keyword***&type=posts&sort=***filter***             |  ```keyword``` |  ```filter```  |\n",
    "\n",
    "\n",
    "## ‚≠ê **FILTER**\n",
    "\n",
    "| Filter       | What it does                                             | Use it when                                      |\n",
    "|--------------|----------------------------------------------------------|-------------------------------------------------|\n",
    "| ```Relevance```    | Shows posts most related to your search terms           | You want posts that best match your search query |\n",
    "| ```Hot```          | Shows currently trending posts based on upvotes, recency, and engagement | You want popular and active posts right now    |\n",
    "| ```Top```          | Shows posts with the highest score for a given time period | You want the most upvoted or \"best\" content for a topic |\n",
    "| ```New```          | Shows posts in chronological order, newest first       | You want fresh content or to track recent activity |\n",
    "| ```Comments Count```     | Sorts posts by number of comments (descending)          | You want posts with lots of community discussion |\n",
    "\n",
    "## ‚≠ê **PERIOD**\n",
    "\n",
    "| Filter       | What it does                                             | Use it when                                      |\n",
    "|--------------|----------------------------------------------------------|-------------------------------------------------|\n",
    "| ```All time```    | Shows posts most related to your search terms           | You want posts that best match your search query |\n",
    "| ```Past year```          | Shows currently trending posts based on upvotes, recency, and engagement | You want popular and active posts right now    |\n",
    "| ```Past month```          | Shows posts with the highest score for a given time period | You want the most upvoted or \"best\" content for a topic |\n",
    "| ```Past week```          | Shows posts in chronological order, newest first       | You want fresh content or to track recent activity |\n",
    "| ```Past hour```     | Sorts posts by number of comments (descending)          | You want posts with lots of community discussion |\n",
    "\n",
    "\n",
    "## ‚≠ê **DOWNLOAD AUTOMATICALLY**\n",
    "\n",
    "| Visual Type | File Naming Formula | Example File Names |\n",
    "|-------------|-------------------|------------------|\n",
    "| ```CAROUSEL```    | **keyword_filter_period_**`<post_number>_<type>_<sequence>` | **keyword_filter_period_**`1_img_01`<br>**keyword_filter_period_**`1_img_02`<br>**keyword_filter_period_**`1_img_03` |\n",
    "| ```IMAGE```       | **keyword_filter_period_**`<post_number>_<type>` | **keyword_filter_period_**`2_img`<br>**keyword_filter_period_**`3_img`<br>**keyword_filter_period_**`4_img` |\n",
    "| ```VIDEO```       | **keyword_filter_period_**`<post_number>_<type>` | **keyword_filter_period_**`5_vid`<br>**keyword_filter_period_**`6_vid`<br>**keyword_filter_period_**`7_vid` |\n",
    "\n",
    "\n",
    "## ‚≠ê **HOW TO USE - STEPS**\n",
    "\n",
    "| Step | Instruction |\n",
    "|------|-------------|\n",
    "| 01  | Run the code |\n",
    "| 02  | Input what you want to search (example: `\"music\"`) |\n",
    "| 03  | Input the Reddit search filter: `\"Relevance\"`, `\"Hot\"`, `\"Top\"`, `\"New\"`, `\"Comment count\"` |\n",
    "| 04  | Input the time filter: `\"All time\"`, `\"Past year\"`, `\"Past month\"`, `\"Past week\"`, `\"Today\"`, `\"Past hour\"` |\n",
    "| 05  | Input the limit of how many posts you want to extract (example: `\"17\"`) |\n",
    "| 06  | Use the CSV file for data analysis and access the post visual content files in `../data/visuals/` |\n",
    "\n",
    "\n",
    "## ‚≠ê **SEARCH FILTER 5 CHOICES**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **01**   | `\"Hot\"` |\n",
    "| **02**   | `\"Top\"` |\n",
    "| **03**   | `\"New\"` |\n",
    "| **04**   | `\"Comments Count\"` |\n",
    "| **05**   | `\"Relevance\"` |\n",
    "\n",
    "\n",
    "## ‚≠ê **PERIOD FILTER 6 CHOICES**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **01**   | `\"All time\"` |\n",
    "| **02**   | `\"Past year\"` |\n",
    "| **03**   | `\"Past month\"` |\n",
    "| **04**   | `\"Past week\"` |\n",
    "| **05**   | `\"Today\"` |\n",
    "| **06**   | `\"Past hour\"` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309219d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ‚öôÔ∏è INSTALL BEFORE RUNNING CODE\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0eb49",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1Ô∏è‚É£ INSTALL using **GIT BASH**\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests beautifulsoup4 lxml\n",
    "%pip install playwright pandas beautifulsoup4 lxml\n",
    "\n",
    "#pip install selenium beautifulsoup4 requests webdriver-manager\n",
    "%pip install selenium beautifulsoup4 requests webdriver-manager\n",
    "\n",
    "# install selenium\n",
    "%pip install selenium pandas\n",
    "\n",
    "# install chromium\n",
    "%playwright install chromium\n",
    "\n",
    "# install fake-useragent\n",
    "%pip install fake-useragent\n",
    "\n",
    "# install requests & beautifulsoup\n",
    "%pip install requests beautifulsoup4 fake-useragent pandas\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628cc80",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2Ô∏è‚É£ INSTALL using **NOTEBOOK**\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5370ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core packages\n",
    "%pip install requests beautifulsoup4 lxml pandas fake-useragent\n",
    "\n",
    "# Install Selenium and WebDriver manager\n",
    "%pip install selenium webdriver-manager\n",
    "\n",
    "# Install Playwright and Chromium browser\n",
    "%pip install playwright\n",
    "%playwright install chromium\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c4d3f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üß∞ V4 - STABLE SCRIPT\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70407d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "         \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "     \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "         \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "         \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "     \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "     \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "     \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "     \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "     \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "     \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "     \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "     \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "     \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• AUDIO NAMING FORMAT\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "         \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "             \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "             \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUDIO NAMING\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "     \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "     \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "     \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "         \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + AUDIO NAMING\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "     \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'\n",
    "        }\n",
    "     \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "     \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "     \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                 \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                 \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                 \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                 \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                 \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                 \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                     \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                     \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                         \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                 \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ example oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                 \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "     \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538dbf35",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üß∞ V5 - STABLE SCRIPT  \n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: hotpink;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecf481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "üî• Scraping 10 'oban star racers' HOT posts...\n",
      "‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\n",
      "üì° Fetching EXACTLY 10 hot posts (Period: All time)...\n",
      "üîç Fetching UP TO 10 hot posts for 'oban star racers'...\n",
      "   üì° API: q=oban%20star%20racers&sort=hot&t=month...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 10/10 hot posts loaded!\n",
      "‚úÖ Saved 10 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  IMAGES/GIFs ‚Üí ../data/visuals/\n",
      "üéµ AUDIO ‚Üí ../data/audio/\n",
      "====================================================================================================\n",
      "üîç [ 1/10] NEW OFICIAL OBAN STAR RACERS COMIC...\n",
      "   üîó [ 1/10] Post ID: 1pk06te\n",
      "   üìÖ [ 1/10] Thursday, December 11, 2025 | üïê 04:23:24 PM  | ‚è∞ 6 hours ago\n",
      "   üìù [ 1/10] 160 chars\n",
      "   üñºÔ∏è [ 1/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].png oban_star_racers_hot_all_time_1_img.png (597.3KB)\n",
      "   ‚úÖ [ 1/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 1/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/10] Respect Ondai (Oban Star Racers)...\n",
      "   üîó [ 2/10] Post ID: 1pjk4g8\n",
      "   üìÖ [ 2/10] Thursday, December 11, 2025 | üïê 01:59:47 AM  | ‚è∞ 21 hours ago\n",
      "   üìù [ 2/10] 1630 chars\n",
      "   ‚ûñ [ 2/10] No visuals\n",
      "   üéâ [ 2/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/10] Oban Star-Racers extended opening intro (2005)....\n",
      "   üîó [ 3/10] Post ID: 1pf2m6v\n",
      "   üìÖ [ 3/10] Friday, December 05, 2025 | üïê 07:24:56 PM  | ‚è∞ 6 days ago\n",
      "   üìù [ 3/10] 104 chars\n",
      "   üñºÔ∏è [ 3/10] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_hot_all_time_3_vid.mp4 (10595.4KB)\n",
      "   ‚úÖ [ 3/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 3/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_hot_all_time_3_audio.m4a\n",
      "   ‚úÖ Audio saved: oban_star_racers_hot_all_time_3_audio.m4a (1652.4KB)\n",
      "   ‚úÖ [ 3/10] Audio: oban_star_racers_hot_all_time_3_audio.m4a\n",
      "   üéâ [ 3/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/10] Oban Star Racers Chance to Shine ‚ù§Ô∏è‚Äçüî• Alwas Traps ü™§ Please S...\n",
      "   üîó [ 4/10] Post ID: 1pdpc6z\n",
      "   üìÖ [ 4/10] Thursday, December 04, 2025 | üïê 04:24:36 AM  | ‚è∞ 1 week ago\n",
      "   üìù [ 4/10] 1089 chars\n",
      "   üñºÔ∏è [ 4/10] Testing 112 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_13.gif (481.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_29.gif (760.8KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_45.gif (728.4KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_61.gif (2640.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_77.gif (454.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_4_img_95.jpg (110.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_4_img_111.jpg (143.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.jpeg...\n",
      "   ‚úÖ [ 4/10] CAROUSEL (7) - 7 WORKING URLs!\n",
      "   üíæ 7 files saved!\n",
      "   üéâ [ 4/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/10] 20 Years Ago Today on December 2, 2005, the official trailer...\n",
      "   üîó [ 5/10] Post ID: 1pcl5km\n",
      "   üìÖ [ 5/10] Tuesday, December 02, 2025 | üïê 10:30:11 PM  | ‚è∞ 1 week 2 days ago\n",
      "   üìù [ 5/10] 9 chars\n",
      "   üñºÔ∏è [ 5/10] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_hot_all_time_5_vid.mp4 (13218.3KB)\n",
      "   ‚úÖ [ 5/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 5/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_hot_all_time_5_audio.m4a\n",
      "   ‚úÖ Audio saved: oban_star_racers_hot_all_time_5_audio.m4a (2758.7KB)\n",
      "   ‚úÖ [ 5/10] Audio: oban_star_racers_hot_all_time_5_audio.m4a\n",
      "   üéâ [ 5/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/10] Kurakasis: Star Wars game annoucment incoming that ends with...\n",
      "   üîó [ 6/10] Post ID: 1pjvv8j\n",
      "   üìÖ [ 6/10] Thursday, December 11, 2025 | üïê 01:05:00 PM  | ‚è∞ 9 hours ago\n",
      "   üìù [ 6/10] 98 chars\n",
      "   ‚ûñ [ 6/10] No visuals\n",
      "   üéâ [ 6/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/10] There‚Äôs another Star Wars game in development ending with ‚ÄòR...\n",
      "   üîó [ 7/10] Post ID: 1pjva8a\n",
      "   üìÖ [ 7/10] Thursday, December 11, 2025 | üïê 12:31:45 PM  | ‚è∞ 10 hours ago\n",
      "   ‚ûñ [ 7/10] No visuals\n",
      "   üéâ [ 7/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/10] Fimbul Mamba Cafe Racer with nose art - Star Atlas, by Gary ...\n",
      "   üîó [ 8/10] Post ID: 1pjrlmt\n",
      "   üìÖ [ 8/10] Thursday, December 11, 2025 | üïê 08:30:58 AM  | ‚è∞ 14 hours ago\n",
      "   üñºÔ∏è [ 8/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_8_img.jpg (1296.5KB)\n",
      "   ‚úÖ [ 8/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 8/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/10] Kurakasis: There‚Äôs another Star Wars game announcement comin...\n",
      "   üîó [ 9/10] Post ID: 1pjxg5d\n",
      "   üìÖ [ 9/10] Thursday, December 11, 2025 | üïê 02:25:13 PM  | ‚è∞ 8 hours ago\n",
      "   ‚ûñ [ 9/10] No visuals\n",
      "   üéâ [ 9/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/10] Star Citizen The Cup - Origin M50 Turbo, Loved By Racers!...\n",
      "   üîó [10/10] Post ID: 1pgpnyd\n",
      "   üìÖ [10/10] Sunday, December 07, 2025 | üïê 07:33:10 PM  | ‚è∞ 4 days ago\n",
      "   üñºÔ∏è [10/10] Testing 1 candidate URLs...\n",
      "   ‚ùå [10/10] No working URLs found\n",
      "   üéâ [10/10] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  VISUALS ‚Üí ../data/visuals/\n",
      "üéµ AUDIO ‚Üí ../data/audio/ (keyword_filter_period_postnumber_audio.m4a)\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 10 posts + media ‚Üí ../data/\n",
      "üìÅ Videos(mute): ../data/videos/\n",
      "üìÅ Visuals: ../data/visuals/\n",
      "üéµ Audio: ../data/audio/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "    \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "        \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "    \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "    \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "    \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "    \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "    \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "    \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "    \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "    \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# üî• FIXED: PERFECT AUDIO NAMING FORMAT - SAVES TO ../data/audio/\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• EXACT REQUIRED NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visuals_folder, videos_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí CORRECT FOLDERS BY TYPE\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "        \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # üî• ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                else:  # images, gifs\n",
    "                    target_folder = visuals_folder\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "            \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # üî• ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:  # images, gifs\n",
    "                    target_folder = visuals_folder\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + PERFECT AUDIO + CORRECT FOLDERS\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    # üî• NEW FOLDER STRUCTURE PER SPECS\n",
    "    VIDEOS_FOLDER = \"../data/videos\"           # Videos (mute)\n",
    "    VISUALS_FOLDER = \"../data/visuals\"         # Images, GIFs, Videos+Audio  \n",
    "    AUDIO_FOLDER = \"../data/audio\"             # Audio files\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  IMAGES/GIFs ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VIDEOS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "    \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "    \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "    \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + PERFECT AUDIO + CORRECT FOLDERS!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "    \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• PERFECT NAMING COLUMN\n",
    "        }\n",
    "    \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "    \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "    \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD TO CORRECT FOLDERS\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                    \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS + CORRECT FOLDERS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER, VIDEOS_FOLDER  # üî• PASS BOTH FOLDERS\n",
    "                        )\n",
    "                    \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                        \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING TO ../data/audio/\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER  # üî• FIXED: ../data/audio/\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "    \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + media ‚Üí ../data/\")\n",
    "    print(f\"üìÅ Videos(mute): ../data/videos/\")\n",
    "    print(f\"üìÅ Visuals: ../data/visuals/\")\n",
    "    print(f\"üéµ Audio: ../data/audio/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed846626",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ü™ú STEPS EXTRACT+DOWNLOAD\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf6ed1",
   "metadata": {},
   "source": [
    "\n",
    "| Type                     | Directory              | Description                          |\n",
    "|--------------------------|----------------------|--------------------------------------|\n",
    "| **Videos (mute)**            | `../data/videos/`       | Video files with no audio            |\n",
    "| **Audio files**              | `../data/audio/`        | Audio-only files                     |\n",
    "| **Images, GIFs, Videos** | `../data/visuals/`   | Visual files including images, GIFs, and videos with audio |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb8fdd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üóÉÔ∏è 1 - FIX EXTRACTION OF GIF CAROUSEL\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: purple;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f72cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO + CAROUSEL_GIF!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "‚è∞ PERIOD FILTER (HTML dropdown match):\n",
      "1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\n",
      "   ‚úÖ Using period: All time ‚Üí API t=all\n",
      "\n",
      "üî• Scraping 10 'oban star racers' RELEVANCE posts...\n",
      "‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES/GIFs‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\n",
      "‚úÖ NEW: CAROUSEL_IMAGE, CAROUSEL_GIF detection!\n",
      "   ‚è∞ Time filter: All time\n",
      "üì° Fetching EXACTLY 10 relevance posts (Period: All time)...\n",
      "üîç Fetching UP TO 10 relevance posts for 'oban star racers'...\n",
      "   ‚è∞ Time filter: All time\n",
      "   üì° API: q=oban%20star%20racers&sort=relevance&t=all...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 10/10 relevance posts loaded!\n",
      "‚úÖ Saved 10 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/oban_star_racers_relevance_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  IMAGES/GIFs ‚Üí ../data/visuals/\n",
      "üéµ AUDIO ‚Üí ../data/audio/\n",
      "====================================================================================================\n",
      "üîç [ 1/10] Oban Star Racers Chance to Shine Update...\n",
      "   üîó [ 1/10] Post ID: 1oiumb2\n",
      "   üìÖ [ 1/10] Wednesday, October 29, 2025 | üïê 04:51:36 AM  | ‚è∞ 1 month 1 week 1 day ago\n",
      "   üìù [ 1/10] 1994 chars\n",
      "   üñºÔ∏è [ 1/10] Testing 224 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gvmo4fsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/gvmo4fsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/gvmo4fsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_13.gif (4317.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/gvmo4fsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/gvmo4fsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/gvmo4fsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/w36ufdsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/w36ufdsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/w36ufdsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_29.gif (3032.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/w36ufdsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/w36ufdsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/w36ufdsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/i20f0fsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/i20f0fsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/i20f0fsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_45.gif (3598.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/i20f0fsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/i20f0fsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/i20f0fsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6gbgxdsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/6gbgxdsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/6gbgxdsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_61.gif (2463.8KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/6gbgxdsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/6gbgxdsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/6gbgxdsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/xq6q0wsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/xq6q0wsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/xq6q0wsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_77.gif (2985.0KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/xq6q0wsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/xq6q0wsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/xq6q0wsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ainmhesz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ainmhesz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ainmhesz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_93.gif (4135.6KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/ainmhesz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ainmhesz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ainmhesz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/brszhfsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/brszhfsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/brszhfsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_109.gif (2221.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/brszhfsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/brszhfsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/brszhfsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mwddsfsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mwddsfsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mwddsfsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_125.gif (3505.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/mwddsfsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mwddsfsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mwddsfsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/5s91cfsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/5s91cfsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/5s91cfsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_141.gif (2178.3KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/5s91cfsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/5s91cfsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/5s91cfsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zve3wesz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/zve3wesz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/zve3wesz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_157.gif (3002.3KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/zve3wesz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/zve3wesz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/zve3wesz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/hyp3kesz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/hyp3kesz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/hyp3kesz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_173.gif (2539.0KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/hyp3kesz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/hyp3kesz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/hyp3kesz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/eofkogsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/eofkogsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/eofkogsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_189.gif (4285.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/eofkogsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/eofkogsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/eofkogsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3szp3rsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/3szp3rsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/3szp3rsz3zxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_1_gif_205.gif (3419.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/3szp3rsz3zxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/3szp3rsz3zxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/3szp3rsz3zxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/fx21frsz3zxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/fx21frsz3zxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/fx21frsz3zxf1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/fx21frsz3zxf1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/fx21frsz3zxf1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_1_img_223.jpg (747.8KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/fx21frsz3zxf1.jpeg...\n",
      "   ‚úÖ [ 1/10] CAROUSEL_MIXED (14) - 14 WORKING URLs!\n",
      "   üíæ 14 files saved!\n",
      "   üéâ [ 1/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/10] 20 Years Ago Today on December 2, 2005, the official trailer...\n",
      "   üîó [ 2/10] Post ID: 1pcl5km\n",
      "   üìÖ [ 2/10] Tuesday, December 02, 2025 | üïê 10:30:11 PM  | ‚è∞ 1 week 2 days ago\n",
      "   üìù [ 2/10] 9 chars\n",
      "   üñºÔ∏è [ 2/10] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_relevance_all_time_2_vid.mp4 (13218.3KB)\n",
      "   ‚úÖ [ 2/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 2/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_relevance_all_time_2_audio.m4a\n",
      "   ‚úÖ Audio saved: oban_star_racers_relevance_all_time_2_audio.m4a (2758.7KB)\n",
      "   ‚úÖ [ 2/10] Audio: oban_star_racers_relevance_all_time_2_audio.m4a\n",
      "   üéâ [ 2/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/10] NEW OFICIAL OBAN STAR RACERS COMIC...\n",
      "   üîó [ 3/10] Post ID: 1pk06te\n",
      "   üìÖ [ 3/10] Thursday, December 11, 2025 | üïê 04:23:24 PM  | ‚è∞ 7 hours ago\n",
      "   üìù [ 3/10] 160 chars\n",
      "   üñºÔ∏è [ 3/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].png oban_star_racers_relevance_all_time_3_img.png (597.3KB)\n",
      "   ‚úÖ [ 3/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 3/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/10] Oban Star-Racers extended opening intro (2005)....\n",
      "   üîó [ 4/10] Post ID: 1pf2m6v\n",
      "   üìÖ [ 4/10] Friday, December 05, 2025 | üïê 07:24:56 PM  | ‚è∞ 6 days ago\n",
      "   üìù [ 4/10] 104 chars\n",
      "   üñºÔ∏è [ 4/10] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_relevance_all_time_4_vid.mp4 (10595.4KB)\n",
      "   ‚úÖ [ 4/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 4/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_relevance_all_time_4_audio.m4a\n",
      "   ‚úÖ Audio saved: oban_star_racers_relevance_all_time_4_audio.m4a (1652.4KB)\n",
      "   ‚úÖ [ 4/10] Audio: oban_star_racers_relevance_all_time_4_audio.m4a\n",
      "   üéâ [ 4/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/10] Oban Star Racers Chance to Shine ‚ù§Ô∏è‚Äçüî• Alwas Traps ü™§ Please S...\n",
      "   üîó [ 5/10] Post ID: 1pdpc6z\n",
      "   üìÖ [ 5/10] Thursday, December 04, 2025 | üïê 04:24:36 AM  | ‚è∞ 1 week ago\n",
      "   üìù [ 5/10] 1089 chars\n",
      "   üñºÔ∏è [ 5/10] Testing 112 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_5_gif_13.gif (481.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_5_gif_29.gif (760.8KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_5_gif_45.gif (728.4KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_5_gif_61.gif (2640.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_5_gif_77.gif (454.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_5_img_95.jpg (110.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_5_img_111.jpg (143.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.jpeg...\n",
      "   ‚úÖ [ 5/10] CAROUSEL_MIXED (7) - 7 WORKING URLs!\n",
      "   üíæ 7 files saved!\n",
      "   üéâ [ 5/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/10] Oban Star Racers Chance to Shine Sneak Peak of Alwas...\n",
      "   üîó [ 6/10] Post ID: 1oivknc\n",
      "   üìÖ [ 6/10] Wednesday, October 29, 2025 | üïê 05:43:51 AM  | ‚è∞ 1 month 1 week 1 day ago\n",
      "   üìù [ 6/10] 225 chars\n",
      "   üñºÔ∏è [ 6/10] Testing 32 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/sm6h97pbdzxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/sm6h97pbdzxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/sm6h97pbdzxf1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_6_gif_13.gif (1994.3KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/sm6h97pbdzxf1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/sm6h97pbdzxf1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/sm6h97pbdzxf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/3fmll9pbdzxf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/3fmll9pbdzxf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/3fmll9pbdzxf1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/3fmll9pbdzxf1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/3fmll9pbdzxf1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_6_img_31.jpg (1137.2KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/3fmll9pbdzxf1.jpeg...\n",
      "   ‚úÖ [ 6/10] CAROUSEL_MIXED (2) - 2 WORKING URLs!\n",
      "   üíæ 2 files saved!\n",
      "   üéâ [ 6/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/10] Oban Star Racers Chance to Shine ‚ù§Ô∏è‚Äçüî• Alwas Gate Trap ü™§...\n",
      "   üîó [ 7/10] Post ID: 1owxhaw\n",
      "   üìÖ [ 7/10] Friday, November 14, 2025 | üïê 03:09:52 PM  | ‚è∞ 3 weeks 6 days ago\n",
      "   üìù [ 7/10] 439 chars\n",
      "   üñºÔ∏è [ 7/10] Testing 32 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/kiti4xcwc81g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/kiti4xcwc81g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/kiti4xcwc81g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_relevance_all_time_7_gif_13.gif (2518.8KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/kiti4xcwc81g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/kiti4xcwc81g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/kiti4xcwc81g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7g4kvvcwc81g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/7g4kvvcwc81g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/7g4kvvcwc81g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/7g4kvvcwc81g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/7g4kvvcwc81g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_7_img_31.jpg (71.6KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/7g4kvvcwc81g1.jpeg...\n",
      "   ‚úÖ [ 7/10] CAROUSEL_MIXED (2) - 2 WORKING URLs!\n",
      "   üíæ 2 files saved!\n",
      "   üéâ [ 7/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/10] Oban Star Racers. Reupload \"Arrow 4\" after some modernizatio...\n",
      "   üîó [ 8/10] Post ID: 1ntqm06\n",
      "   üìÖ [ 8/10] Monday, September 29, 2025 | üïê 08:55:14 PM  | ‚è∞ 2 months 1 week 3 days ago\n",
      "   üìù [ 8/10] 28 chars\n",
      "   üñºÔ∏è [ 8/10] Testing 96 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/gre8q1i8h5sf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/gre8q1i8h5sf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/gre8q1i8h5sf1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/gre8q1i8h5sf1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/gre8q1i8h5sf1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_8_img_15.jpg (373.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/gre8q1i8h5sf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/zcaw8rmbh5sf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/zcaw8rmbh5sf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/zcaw8rmbh5sf1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/zcaw8rmbh5sf1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/zcaw8rmbh5sf1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_8_img_31.jpg (339.7KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/zcaw8rmbh5sf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/6z8xqwjch5sf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/6z8xqwjch5sf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/6z8xqwjch5sf1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/6z8xqwjch5sf1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/6z8xqwjch5sf1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_8_img_47.jpg (304.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/6z8xqwjch5sf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/51zgf14eh5sf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/51zgf14eh5sf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/51zgf14eh5sf1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/51zgf14eh5sf1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/51zgf14eh5sf1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_8_img_63.jpg (417.7KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/51zgf14eh5sf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/8vfsjojkh5sf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/8vfsjojkh5sf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/8vfsjojkh5sf1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/8vfsjojkh5sf1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/8vfsjojkh5sf1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_8_img_79.jpg (444.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/8vfsjojkh5sf1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/7516o6puh5sf1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/7516o6puh5sf1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/7516o6puh5sf1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/7516o6puh5sf1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/7516o6puh5sf1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_relevance_all_time_8_img_95.jpg (329.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/7516o6puh5sf1.jpeg...\n",
      "   ‚úÖ [ 8/10] CAROUSEL_IMAGE (6) - 6 WORKING URLs!\n",
      "   üíæ 6 files saved!\n",
      "   üéâ [ 8/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/10] Oban Star Racers Lost Media &amp; Games Found! ‚ù§Ô∏è‚Äçüî•...\n",
      "   üîó [ 9/10] Post ID: 1oivqpb\n",
      "   üìÖ [ 9/10] Wednesday, October 29, 2025 | üïê 05:53:37 AM  | ‚è∞ 1 month 1 week 1 day ago\n",
      "   üìù [ 9/10] 318 chars\n",
      "   ‚ûñ [ 9/10] No visuals\n",
      "   üéâ [ 9/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/10] \"Around 15 years ago a team developed [≈åban Star-Racers\"] fo...\n",
      "   üîó [10/10] Post ID: 1pary80\n",
      "   üìÖ [10/10] Sunday, November 30, 2025 | üïê 09:04:44 PM  | ‚è∞ 1 week 4 days ago\n",
      "   üìù [10/10] 1 chars\n",
      "   üñºÔ∏è [10/10] Testing 1 candidate URLs...\n",
      "   ‚ùå [10/10] No working URLs found\n",
      "   üéâ [10/10] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/oban_star_racers_relevance_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  VISUALS ‚Üí ../data/visuals/ (incl. CAROUSEL_IMAGE, CAROUSEL_GIF)\n",
      "üéµ AUDIO ‚Üí ../data/audio/ (keyword_filter_period_postnumber_audio.m4a)\n",
      "‚úÖ NEW visual_types: CAROUSEL_IMAGE, CAROUSEL_GIF, CAROUSEL_MIXED!\n",
      "\n",
      "‚úÖ DONE! 10 posts + media ‚Üí ../data/\n",
      "üìÅ Videos(mute): ../data/videos/\n",
      "üìÅ Visuals(GIFs/Images): ../data/visuals/\n",
      "üéµ Audio: ../data/audio/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # for yt-dlp audio\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"Convert display text to Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "    \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "        \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "    \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "    \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "    \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "    \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "    \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "    \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "    \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "    \n",
    "        # IMAGES/GIFS (GIFs prioritized)\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# AUDIO NAMING FORMAT - SAVES TO ../data/audio/\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visuals_folder, videos_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí CORRECT FOLDERS BY TYPE\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number - PRIORITIZE GIFs!\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "        \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                else:  # images, gifs ‚Üí visuals/\n",
    "                    target_folder = visuals_folder\n",
    "                    if 'gif' in content_type:\n",
    "                        file_prefix = 'gif'\n",
    "                    else:\n",
    "                        file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "            \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO/GIF: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # üî• ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:  # images, gifs\n",
    "                    target_folder = visuals_folder\n",
    "                    if 'gif' in content_type:\n",
    "                        file_prefix = 'gif'\n",
    "                    else:\n",
    "                        file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_{file_prefix}{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUDIO + CORRECT FOLDERS + CAROUSEL_IMAGE/GIF\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    # FOLDER STRUCTURE\n",
    "    VIDEOS_FOLDER = \"../data/videos\"           # Videos (mute)\n",
    "    VISUALS_FOLDER = \"../data/visuals\"         # Images, GIFs, Videos+Audio  \n",
    "    AUDIO_FOLDER = \"../data/audio\"             # Audio files\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  IMAGES/GIFs ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VIDEOS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    # CAROUSEL_IMAGE, CAROUSEL_GIF DETECTION\n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        \n",
    "        # VIDEO DETECTION\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        \n",
    "        # CAROUSEL DETECTION WITH GIF/IMAGE CLASSIFICATION\n",
    "        if '\\n' in visual_str:\n",
    "            lines = visual_str.splitlines()\n",
    "            gif_count = sum(1 for line in lines if '.gif' in line.lower())\n",
    "            total_count = len(lines)\n",
    "            \n",
    "            if gif_count > 0:\n",
    "                if gif_count == total_count:\n",
    "                    return 'CAROUSEL_GIF', total_count\n",
    "                else:\n",
    "                    return 'CAROUSEL_MIXED', total_count\n",
    "            else:\n",
    "                return 'CAROUSEL_IMAGE', total_count\n",
    "        \n",
    "        # SINGLE GIF/IMAGE\n",
    "        if '.gif' in visual_str or 'gif' in visual_str:\n",
    "            return 'GIF', 1\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png']):\n",
    "            return 'IMAGE', 1\n",
    "        \n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # ENHANCED: Comprehensive visual extraction with carousel + video + GIF support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "    \n",
    "            # 3. CAROUSEL - ENHANCED GIF/IMAGE EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # Try ALL possible formats for this media_id (GIFs prioritized)\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 4. SINGLE IMAGE/GIF\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "    \n",
    "            # 5. PREVIEW IMAGES/GIFs\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "    \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + AUDIO + CORRECT FOLDERS!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "    \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A' \n",
    "        }\n",
    "    \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "    \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "    \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                \n",
    "                    # DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                \n",
    "                    # ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD TO CORRECT FOLDERS\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                    \n",
    "                        # TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS + CORRECT FOLDERS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if len(all_candidate_urls) > 1 else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER, VIDEOS_FOLDER\n",
    "                        )\n",
    "                    \n",
    "                        # ONLY WORKING LINKS go to post_visual! NEW CAROUSEL TYPES!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                        \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                \n",
    "                    # üî• AUDIO DOWNLOAD with EXACT NAMING TO ../data/audio/\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "    \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  VISUALS ‚Üí {VISUALS_FOLDER}/ (incl. CAROUSEL_IMAGE, CAROUSEL_GIF)\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ NEW visual_types: CAROUSEL_IMAGE, CAROUSEL_GIF, CAROUSEL_MIXED!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# INTERACTIVE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO + CAROUSEL_GIF!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES/GIFs‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\")\n",
    "    print(f\"‚úÖ NEW: CAROUSEL_IMAGE, CAROUSEL_GIF detection!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + media ‚Üí ../data/\")\n",
    "    print(f\"üìÅ Videos(mute): ../data/videos/\")\n",
    "    print(f\"üìÅ Visuals(GIFs/Images): ../data/visuals/\")\n",
    "    print(f\"üéµ Audio: ../data/audio/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e570d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üóÉÔ∏è 2 - MERGE MUTE VIDEOS with AUDIOS\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: purple;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d9200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for video/audio pairs...\n",
      "üìä Found 2 video files in C:\\Users\\sboub\\Documents\\GitHub\\reddit-scraper\\data\\videos\n",
      "\n",
      "üîÑ [1/2] oban_star_racers_relevance_all_time_2\n",
      "   üìÅ Video:  oban_star_racers_relevance_all_time_2_vid.mp4 (from videos)\n",
      "   üìÅ Audio:  oban_star_racers_relevance_all_time_2_audio.m4a\n",
      "   üìÅ Output: oban_star_racers_relevance_all_time_2_video.mp4 (to visuals)\n",
      "   ‚úÖ SUCCESS! (16.4 MB)\n",
      "\n",
      "üîÑ [2/2] oban_star_racers_relevance_all_time_4\n",
      "   üìÅ Video:  oban_star_racers_relevance_all_time_4_vid.mp4 (from videos)\n",
      "   üìÅ Audio:  oban_star_racers_relevance_all_time_4_audio.m4a\n",
      "   üìÅ Output: oban_star_racers_relevance_all_time_4_video.mp4 (to visuals)\n",
      "   ‚úÖ SUCCESS! (12.6 MB)\n",
      "\n",
      "üéâ SUMMARY:\n",
      "   ‚úÖ Success: 2\n",
      "   ‚ùå Failed:  0\n",
      "   üìÅ Total:   2\n",
      "   üìÇ Source:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-scraper\\data\\videos/(*_vid*.mp4)\n",
      "   üìÇ Audio:   C:\\Users\\sboub\\Documents\\GitHub\\reddit-scraper\\data\\audio/(*_audio.m4a)\n",
      "   üìÇ Output:  C:\\Users\\sboub\\Documents\\GitHub\\reddit-scraper\\data\\visuals/(*_video.mp4)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "def auto_video_merge_all():\n",
    "    \"\"\"üöÄ Automatically merge ALL video+audio pairs - outputs *_video.mp4 to ../data/visuals/\"\"\"\n",
    "    \n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-scraper\")\n",
    "    \n",
    "    #  DIRECTORIES\n",
    "    videos_dir = base_dir / \"data\" / \"videos\"      # Videos (mute) - SOURCE *_vid*.mp4\n",
    "    audio_dir = base_dir / \"data\" / \"audio\"        # Audio files\n",
    "    visuals_dir = base_dir / \"data\" / \"visuals\"    # Videos+Audio destination\n",
    "    \n",
    "    print(\"üîç Scanning for video/audio pairs...\")\n",
    "    \n",
    "    # SEARCH VIDEOS IN ../data/videos/ (mute videos)\n",
    "    video_files = list(videos_dir.rglob(\"*_vid*.mp4\")) + list(videos_dir.rglob(\"*_vid*.webm\"))\n",
    "    print(f\"üìä Found {len(video_files)} video files in {videos_dir}\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        # Extract keyword_filter_period_postnumber from *_vid.mp4 OR *_vid_01.mp4\n",
    "        video_name = video_path.stem  # removes .mp4\n",
    "        \n",
    "        # FLEXIBLE PATTERN: keyword_filter_period_postnumber_vid OR keyword_filter_period_postnumber_vid_01\n",
    "        match = re.match(r'(.+?)_vid(?:_\\d+)?$', video_name)\n",
    "        if not match:\n",
    "            print(f\"‚ö†Ô∏è  Skipping non-matching video: {video_name}\")\n",
    "            continue\n",
    "            \n",
    "        keyword_filter_period_postnumber = match.group(1)\n",
    "        \n",
    "        # Construct matching audio path: same name + _audio.m4a\n",
    "        audio_filename = f\"{keyword_filter_period_postnumber}_audio.m4a\"\n",
    "        audio_path = audio_dir / audio_filename\n",
    "        \n",
    "        # OUTPUT *_video.mp4 TO ../data/visuals/ (Videos+Audio)\n",
    "        output_filename = f\"{keyword_filter_period_postnumber}_video.mp4\"\n",
    "        output_path = visuals_dir / output_filename\n",
    "        \n",
    "        print(f\"\\nüîÑ [{success_count + fail_count + 1}/{len(video_files)}] {keyword_filter_period_postnumber}\")\n",
    "        print(f\"   üìÅ Video:  {video_path.name} (from {videos_dir.name})\")\n",
    "        print(f\"   üìÅ Audio:  {audio_filename}\")\n",
    "        print(f\"   üìÅ Output: {output_filename} (to {visuals_dir.name})\")\n",
    "        \n",
    "        # Skip if already merged\n",
    "        if output_path.exists():\n",
    "            print(\"   ‚úÖ Already exists - SKIPPING\")\n",
    "            success_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not audio_path.exists():\n",
    "            print(\"   ‚ùå Audio missing - SKIPPING\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Merge command\n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "            \"-y\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-i\", str(audio_path),\n",
    "            \"-map\", \"0:v:0\",\n",
    "            \"-map\", \"1:a:0\",\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-shortest\",\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                size_mb = output_path.stat().st_size / 1e6\n",
    "                print(f\"   ‚úÖ SUCCESS! ({size_mb:.1f} MB)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ‚ùå FAILED: {result.stderr[:200]}\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   ‚è∞ TIMEOUT - Killed\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}\")\n",
    "    print(f\"   ‚ùå Failed:  {fail_count}\")\n",
    "    print(f\"   üìÅ Total:   {len(video_files)}\")\n",
    "    print(f\"   üìÇ Source:  {videos_dir}/(*_vid*.mp4)\")\n",
    "    print(f\"   üìÇ Audio:   {audio_dir}/(*_audio.m4a)\")\n",
    "    print(f\"   üìÇ Output:  {visuals_dir}/(*_video.mp4)\")  # Videos+Audio\n",
    "\n",
    "\n",
    "# Auto-run\n",
    "if __name__ == \"__main__\":\n",
    "    auto_video_merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e96337",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üß™ TESTING\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: red;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
