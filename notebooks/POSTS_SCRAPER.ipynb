{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d29526d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# README\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5a37d",
   "metadata": {},
   "source": [
    "## ‚≠ê **Reddit/Music Threads Guidelines**\n",
    "\n",
    "**Conditions:**  \n",
    "- Description: 100‚Äì300 characters  \n",
    "- Posts must be HOT & relevant to music  \n",
    "- Downloadable visuals (images/videos)  \n",
    "- Only include posts with ‚â•100 shares  \n",
    "- High engagement / top-performing posts  \n",
    "\n",
    "**Essentials:**  \n",
    "- Keyword: `music`  \n",
    "- Focus: short text or image, simplicity, relevance  \n",
    "\n",
    "**Relevance Categories:**  \n",
    "- Music theory, instruments, production, teaching  \n",
    "- Historical highlights, facts, memes, communities  \n",
    "- Target: anyone learning, playing, or producing music  \n",
    "\n",
    "**Threads Post Rules:**  \n",
    "- Video under 15 seconds  \n",
    "- If both image & video exist, use only video  \n",
    "\n",
    "\n",
    "\n",
    "## ‚≠ê **Reddit Post Data Fields**\n",
    "\n",
    "The main data fields to extract from the Reddit Post :\n",
    "| Field Name         | Python Data Type | Description |\n",
    "|--------------------|-----------------|-------------|\n",
    "| <span style=\"color:green\">**post_title**</span>     | `str`            | Title of the post. |\n",
    "| **post_link**      | `str`            | Direct URL to the post. |\n",
    "| **post_id**        | `str`            | Unique identifier for the post. |\n",
    "| <span style=\"color:red\">**num_votes**</span>      | `int`            | Total number of upvotes the post received. |\n",
    "| <span style=\"color:red\">**num_comments**</span>   | `int`            | Total number of comments on the post. |\n",
    "| **text_length**    | `int`            | Character count of the post‚Äôs text description. |\n",
    "| <span style=\"color:green\">**post_description**</span> | `str`          | Full text description or caption of the post. |\n",
    "| **post_date**      | `date`           | Calendar date when the post was published. |\n",
    "| **post_time**      | `str`            | Time (with timezone) when the post was published. |\n",
    "| <span style=\"color:green\">**post_visual**</span>    | `list[str]`      | Direct URLs to visual content (images or videos). |\n",
    "| **visual_type**    | `str`            | Type of visual content: `\"IMAGE\"`, `\"VIDEO\"`,`\"CAROUSEL\"`, or `\"NONE\"`. |\n",
    "| **visual_count**   | `int`            | Number of visual items in the post. |\n",
    "| <span style=\"color:blue\">**filter**</span>         | `str`            | Reddit search filter used. Must be one of: `\"Relevance\"`, `\"Hot\"`, `\"Top\"`, `\"New\"`, `\"Comment count\"`. |\n",
    "| <span style=\"color:blue\">**keyword**</span>        | `str`            | Search keyword or query. |\n",
    "| <span style=\"color:blue\">**limit**</span>          | `int`            | Number of posts requested. |\n",
    "| <span style=\"color:blue\">**period**</span>         | `str`            | Time filter used when searching posts. Must be one of: `\"All time\"`, `\"Past year\"`, `\"Past month\"`, `\"Past week\"`, `\"Today\"`, `\"Past hour\"`. |\n",
    "| <span style=\"color:red\">**time_ago**</span>       | `str`            | Relative time since the post was published (e.g., `\"3 hours ago\"`). |\n",
    "\n",
    "\n",
    "\n",
    "## ‚≠ê **POST LINK - EXPLORING**\n",
    "\n",
    "| Link             | Keyword       | Filter       |\n",
    "|------------------------|-----------------------|-----------------------|\n",
    "| https://www.reddit.com/search/?q=music             |  ```music```  | ```Relevance``` by default |\n",
    "| https://www.reddit.com/search/?q=***keyword***             |  ```keyword```  | ```Relevance``` by default |\n",
    "| https://www.reddit.com/search/?q=music&type=posts&sort=hot             |  ```music``` |  ```HOT```  |\n",
    "| https://www.reddit.com/search/?q=music&type=posts&sort=top             |  ```music``` |  ```Top```  |\n",
    "| https://www.reddit.com/search/?q=***keyword***&type=posts&sort=***filter***             |  ```keyword``` |  ```filter```  |\n",
    "\n",
    "\n",
    "## ‚≠ê **FILTER**\n",
    "\n",
    "| Filter       | What it does                                             | Use it when                                      |\n",
    "|--------------|----------------------------------------------------------|-------------------------------------------------|\n",
    "| ```Relevance```    | Shows posts most related to your search terms           | You want posts that best match your search query |\n",
    "| ```Hot```          | Shows currently trending posts based on upvotes, recency, and engagement | You want popular and active posts right now    |\n",
    "| ```Top```          | Shows posts with the highest score for a given time period | You want the most upvoted or \"best\" content for a topic |\n",
    "| ```New```          | Shows posts in chronological order, newest first       | You want fresh content or to track recent activity |\n",
    "| ```Comments Count```     | Sorts posts by number of comments (descending)          | You want posts with lots of community discussion |\n",
    "\n",
    "## ‚≠ê **PERIOD**\n",
    "\n",
    "| Filter       | What it does                                             | Use it when                                      |\n",
    "|--------------|----------------------------------------------------------|-------------------------------------------------|\n",
    "| ```All time```    | Shows posts most related to your search terms           | You want posts that best match your search query |\n",
    "| ```Past year```          | Shows currently trending posts based on upvotes, recency, and engagement | You want popular and active posts right now    |\n",
    "| ```Past month```          | Shows posts with the highest score for a given time period | You want the most upvoted or \"best\" content for a topic |\n",
    "| ```Past week```          | Shows posts in chronological order, newest first       | You want fresh content or to track recent activity |\n",
    "| ```Past hour```     | Sorts posts by number of comments (descending)          | You want posts with lots of community discussion |\n",
    "\n",
    "\n",
    "## ‚≠ê **DOWNLOAD AUTOMATICALLY**\n",
    "\n",
    "| Visual Type | File Naming Formula | Example File Names |\n",
    "|-------------|-------------------|------------------|\n",
    "| ```CAROUSEL```    | **keyword_filter_period_**`<post_number>_<type>_<sequence>` | **keyword_filter_period_**`1_img_01`<br>**keyword_filter_period_**`1_img_02`<br>**keyword_filter_period_**`1_img_03` |\n",
    "| ```IMAGE```       | **keyword_filter_period_**`<post_number>_<type>` | **keyword_filter_period_**`2_img`<br>**keyword_filter_period_**`3_img`<br>**keyword_filter_period_**`4_img` |\n",
    "| ```VIDEO```       | **keyword_filter_period_**`<post_number>_<type>` | **keyword_filter_period_**`5_vid`<br>**keyword_filter_period_**`6_vid`<br>**keyword_filter_period_**`7_vid` |\n",
    "\n",
    "\n",
    "## ‚≠ê **HOW TO USE - STEPS**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 01  | Run the code |\n",
    "| 02  | Input what you want to search (example: `\"music\"`) |\n",
    "| 03  | Input the Reddit search filter: `\"Relevance\"`, `\"Hot\"`, `\"Top\"`, `\"New\"`, `\"Comment count\"` |\n",
    "| 04  | Input the time filter: `\"All time\"`, `\"Past year\"`, `\"Past month\"`, `\"Past week\"`, `\"Today\"`, `\"Past hour\"` |\n",
    "| 05  | Input the limit of how many posts you want to extract (example: `\"17\"`) |\n",
    "| 06  | Use the CSV file for data analysis and access the post visual content files in `../data/visuals/` |\n",
    "\n",
    "\n",
    "## ‚≠ê **SEARCH FILTER 5 CHOICES**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **01**   | `\"Hot\"` |\n",
    "| **02**   | `\"Top\"` |\n",
    "| **03**   | `\"Comments Count\"` |\n",
    "| **04**   | `\"New\"` |\n",
    "| **05**   | `\"Relevance\"` |\n",
    "\n",
    "\n",
    "## ‚≠ê **PERIOD FILTER 5 CHOICES**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **01**   | `\"All time\"` |\n",
    "| **02**   | `\"Past year\"` |\n",
    "| **03**   | `\"Past month\"` |\n",
    "| **04**   | `\"Past week\"` |\n",
    "| **05**   | `\"Today\"` |\n",
    "| **06**   | `\"Past hour\"` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309219d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ‚öôÔ∏è INSTALL BEFORE RUNNING CODE\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0eb49",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1Ô∏è‚É£ INSTALL using **GIT BASH**\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests beautifulsoup4 lxml\n",
    "%pip install playwright pandas beautifulsoup4 lxml\n",
    "\n",
    "#pip install selenium beautifulsoup4 requests webdriver-manager\n",
    "%pip install selenium beautifulsoup4 requests webdriver-manager\n",
    "\n",
    "# install selenium\n",
    "%pip install selenium pandas\n",
    "\n",
    "# install chromium\n",
    "%playwright install chromium\n",
    "\n",
    "# install fake-useragent\n",
    "%pip install fake-useragent\n",
    "\n",
    "# install requests & beautifulsoup\n",
    "%pip install requests beautifulsoup4 fake-useragent pandas\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628cc80",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2Ô∏è‚É£ INSTALL using **NOTEBOOK**\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5370ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core packages\n",
    "%pip install requests beautifulsoup4 lxml pandas fake-useragent\n",
    "\n",
    "# Install Selenium and WebDriver manager\n",
    "%pip install selenium webdriver-manager\n",
    "\n",
    "# Install Playwright and Chromium browser\n",
    "%pip install playwright\n",
    "%playwright install chromium\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3bc28",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ü§ñ REDDIT\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: orange;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5046db",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1Ô∏è‚É£ FIX NAMING OF COLUMNS\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [\n",
    "            f'{keyword.title()} guitar solo!',\n",
    "            f'New {keyword} album üî•',\n",
    "            f'Best {keyword} live concert',\n",
    "            f'{keyword.title()} drum cover',\n",
    "            f'{keyword.title()} masterpiece'\n",
    "        ][:limit],\n",
    "        'post_link': [\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1abc123/title1/',\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1def456/title2/',\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1ghi789/title3/',\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1jkl012/title4/',\n",
    "            f'https://www.reddit.com/r/{keyword}/comments/1mno345/title5/'\n",
    "        ][:limit]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50):\n",
    "    \"\"\"Fetch REAL Reddit posts using search API with different filters\"\"\"\n",
    "    print(f\"üîç Fetching {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    \n",
    "    filter_map = {\n",
    "        'relevance': 'relevance',\n",
    "        'top': 'top',\n",
    "        'hot': 'hot', \n",
    "        'comments': 'comments',\n",
    "        'new': 'new'\n",
    "    }\n",
    "    sort_filter = filter_map.get(filter, 'hot')\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&type=link&sort={sort_filter}&limit={min(limit,100)}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            for post in data['data']['children']:\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter\n",
    "                })\n",
    "        \n",
    "        actual_posts = min(len(posts), limit)\n",
    "        print(f\"‚úÖ Found {actual_posts}/{limit} {filter} posts\")\n",
    "        return pd.DataFrame(posts[:limit])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    # ‚úÖ DIRECT i.redd.it - KEEP AS-IS\n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    # üî• EXTRACT media_id FROM ANY preview/external-preview ‚Üí i.redd.it ONLY\n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)  # Any 13-char media_id\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50):\n",
    "    \"\"\"\n",
    "    üî• i.redd.it/xxx.png ONLY - NO preview.redd.it EVER!\n",
    "    CSV columns: post_title,post_link,post_id,num_votes,num_comments,filter,text_length,post_description,post_visual,visual_type,visual_count\n",
    "    \"\"\"\n",
    "    \n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}.csv\"\n",
    "    \n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(\"üì° Fetching real Reddit posts...\")\n",
    "        df = fetch_reddit_posts_search(keyword, filter, limit)\n",
    "        if df is not None and not df.empty:\n",
    "            os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "            df.to_csv(INPUT_FILE, index=False)\n",
    "            print(f\"‚úÖ Saved {len(df)} REAL posts to {INPUT_FILE}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Using sample data (search failed)\")\n",
    "            create_sample_main_data(keyword_clean, limit)\n",
    "    \n",
    "    df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    print(f\"üìñ Loaded {len(df)} posts from {INPUT_FILE}\")\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual)\n",
    "        visual_lower = visual_str.lower()\n",
    "        \n",
    "        if any(x in visual_lower for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        \n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        \n",
    "        if 'i.redd.it' in visual_lower or '.jpg' in visual_lower or '.png' in visual_lower or '.gif' in visual_lower:\n",
    "            return 'IMAGE', 1\n",
    "        \n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = str(description)\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        \"\"\"üî• i.redd.it/xxx.png ONLY - Converts ALL preview/external-preview\"\"\"\n",
    "        visual_urls = []\n",
    "        \n",
    "        try:\n",
    "            # 1. VIDEOS FIRST (direct playable)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                video_info = post_info['media']['reddit_video']\n",
    "                fallback_url = video_info.get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    print(f\"   ‚úÖ VIDEO: {fallback_url.split('/')[-1]}\")\n",
    "                    return visual_urls\n",
    "            \n",
    "            if post_info.get('url') and any(domain in post_info['url'].lower() \n",
    "                                        for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                print(f\"   üé• External video\")\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 2. GALLERY ‚Üí ONLY i.redd.it\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data:\n",
    "                print(f\"   üéâ GALLERY ({len(gallery_data.get('items', []))} items)\")\n",
    "                items = gallery_data.get('items', [])\n",
    "                \n",
    "                for item_idx, item in enumerate(items, 1):\n",
    "                    try:\n",
    "                        if isinstance(item, dict) and 'media_id' in item:\n",
    "                            media_id = item['media_id']\n",
    "                            viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                            if viewable_url not in visual_urls:\n",
    "                                visual_urls.append(viewable_url)\n",
    "                                print(f\"   üì∑ Gallery {item_idx}: i.redd.it/{media_id}.png\")\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 3. SINGLE IMAGE ‚Üí ONLY i.redd.it\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                visual_urls.append(viewable_url)\n",
    "                print(f\"   üñºÔ∏è Single: i.redd.it/{viewable_url.split('/')[-1]}\")\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 4. PREVIEW ‚Üí ONLY i.redd.it (source + resolutions)\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                print(f\"   üì∑ Preview ‚Üí i.redd.it conversion\")\n",
    "                for img_idx, img in enumerate(post_info['preview']['images'], 1):\n",
    "                    try:\n",
    "                        source_url = img.get('source', {}).get('url', '')\n",
    "                        if source_url:\n",
    "                            viewable_url = get_viewable_image_url(source_url)\n",
    "                            if viewable_url and 'i.redd.it' in viewable_url and viewable_url not in visual_urls:\n",
    "                                visual_urls.append(viewable_url)\n",
    "                                print(f\"   üì∑ Preview {img_idx}: i.redd.it/{viewable_url.split('/')[-1]}\")\n",
    "                                continue\n",
    "                        \n",
    "                        resolutions = img.get('resolutions', [])\n",
    "                        for res in resolutions[-2:]:  # Highest quality\n",
    "                            res_url = res.get('url', '')\n",
    "                            if res_url:\n",
    "                                viewable_url = get_viewable_image_url(res_url)\n",
    "                                if viewable_url and 'i.redd.it' in viewable_url and viewable_url not in visual_urls:\n",
    "                                    visual_urls.append(viewable_url)\n",
    "                                    print(f\"   üì∑ Preview {img_idx}: i.redd.it/{viewable_url.split('/')[-1]}\")\n",
    "                                    break\n",
    "                    except:\n",
    "                        continue\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 5. THUMBNAIL ‚Üí ONLY i.redd.it\n",
    "            if post_info.get('thumbnail') and post_info['thumbnail'] != 'self' and not visual_urls:\n",
    "                thumb_url = get_viewable_image_url(post_info['thumbnail'])\n",
    "                if thumb_url and 'i.redd.it' in thumb_url:\n",
    "                    visual_urls.append(thumb_url)\n",
    "                    print(f\"   üîç Thumbnail: i.redd.it/{thumb_url.split('/')[-1]}\")\n",
    "                \n",
    "        except Exception as visual_error:\n",
    "            print(f\"   ‚ö†Ô∏è Visual error: {str(visual_error)[:40]}\")\n",
    "        \n",
    "        return visual_urls\n",
    "    \n",
    "    print(f\"üöÄ Processing {len(df)} {filter.upper()} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìÅ Input:  {INPUT_FILE}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_link = row['post_link']\n",
    "        post_title = row['post_title']\n",
    "        \n",
    "        print(f\"üîç {idx+1}/{len(df)}: {post_title[:50]}...\")\n",
    "        \n",
    "        post_id = extract_post_id(post_link)\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå Invalid link\")\n",
    "            new_data.append(post_data)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            json_url = f\"https://www.reddit.com/comments/{post_id}.json\"\n",
    "            response = session.get(json_url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data['post_id'] = post_id\n",
    "                    post_data['num_votes'] = str(post_info.get('score', row.get('num_votes', 'N/A')))\n",
    "                    print(f\"   üó≥Ô∏è Votes: {post_data['num_votes']}\")\n",
    "                    \n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    description = 'N/A'\n",
    "                    \n",
    "                    text_body = soup.find('div', {'data-post-click-location': 'text-body'})\n",
    "                    if text_body:\n",
    "                        p_tags = text_body.find_all('p')\n",
    "                        if p_tags:\n",
    "                            description = '\\n'.join([p.get_text(strip=True) for p in p_tags])[:2000]\n",
    "                            print(f\"   ‚úÖ HTML description: {len(p_tags)} paragraphs\")\n",
    "                    \n",
    "                    if description == 'N/A':\n",
    "                        selftext = post_info.get('selftext', '')\n",
    "                        if selftext.strip():\n",
    "                            description = selftext[:2000]\n",
    "                            print(f\"   ‚úÖ JSON selftext found\")\n",
    "                    \n",
    "                    text_length = calculate_text_length(description)\n",
    "                    post_data['text_length'] = text_length\n",
    "                    post_data['post_description'] = description\n",
    "                    print(f\"   üìù Text length: {text_length} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    \n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        print(f\"   ‚úÖ {len(visual_urls)} i.redd.it URLS!\")\n",
    "                    else:\n",
    "                        post_data['post_visual'] = 'N/A'\n",
    "                    \n",
    "                    visual_type, visual_count = get_visual_type_count(post_data['post_visual'])\n",
    "                    post_data['visual_type'] = visual_type\n",
    "                    post_data['visual_count'] = visual_count\n",
    "                    print(f\"   ‚úÖ {visual_type} ({visual_count})\")\n",
    "            \n",
    "            else:\n",
    "                print(f\"   ‚ùå HTTP {response.status_code}\")\n",
    "                raise Exception(f\"HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error: {str(e)[:50]}\")\n",
    "            post_data['post_id'] = post_id\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.0)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'text_length', 'post_description', 'post_visual', \n",
    "        'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    \n",
    "    text_lengths = new_df['text_length'].fillna(0).astype(int)\n",
    "    image_posts = len(new_df[new_df['visual_type'] == 'IMAGE'])\n",
    "    carousel_posts = len(new_df[new_df['visual_type'] == 'CAROUSEL'])\n",
    "    video_posts = len(new_df[new_df['visual_type'] == 'VIDEO'])\n",
    "    print(f\"üìä STATS: {len(new_df)} posts | {image_posts} images | {carousel_posts} carousels | {video_posts} videos | {text_lengths.mean():.0f} chars avg\")\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - i.redd.it ONLY!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    keyword = input(\"Enter keyword (spaces OK): \").strip()\n",
    "    if not keyword:\n",
    "        keyword = 'music'\n",
    "    \n",
    "    num_posts = input(\"How many posts to scrape? [10]: \").strip()\n",
    "    try:\n",
    "        limit = int(num_posts) if num_posts else 10\n",
    "        limit = min(limit, 100)\n",
    "    except:\n",
    "        limit = 10\n",
    "        print(\"‚ö†Ô∏è Using 10 posts\")\n",
    "    \n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    print(f\"\\nSearch:  '{keyword}'\")\n",
    "    print(f\"Files:   {keyword_clean}_main.csv ‚Üí {keyword_clean}_{{filter}}.csv\")\n",
    "    \n",
    "    print(\"\\nFilters:\")\n",
    "    print(\"  [1] hot      ‚Üí print_art_hot.csv\")\n",
    "    print(\"  [2] top      ‚Üí print_art_top.csv\") \n",
    "    print(\"  [3] new      ‚Üí print_art_new.csv\")\n",
    "    print(\"  [4] comments ‚Üí print_art_comments.csv\")\n",
    "    print(\"  [5] relevance‚Üí print_art_relevance.csv\")\n",
    "    \n",
    "    choice = input(f\"\\nChoose filter (1-5) [1]: \").strip()\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' posts ({filter.upper()})\")\n",
    "    print(f\"üì• {keyword_clean}_main.csv ‚Üí üì§ {keyword_clean}_{filter}.csv\")\n",
    "    \n",
    "    result = extract_post_details_complete(keyword, filter, limit)\n",
    "    \n",
    "    print(f\"\\n‚úÖ DONE! Output: ../data/reddit/{keyword_clean}_{filter}.csv\")\n",
    "    print(\"üéâ ONLY i.redd.it/xxx.png - NO preview.redd.it EVER!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d762c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2Ô∏è‚É£ FIX ADD POST_DATE & POST_TIME\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53396e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50):\n",
    "    \"\"\"üî• FIXED: Uses reference code's URL - gets 20-100+ posts!\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t=month&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: {search_url.split('q=')[1][:50]}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50):\n",
    "    \"\"\"MAIN FUNCTION - FULL PROGRESS TRACKING!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                        'post_date': format_post_date(post_info.get('created_utc'))[0],\n",
    "                        'post_time': format_post_date(post_info.get('created_utc'))[1]\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_data['post_date']} | üïê {post_data['post_time'][:12]}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                        \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'post_date', 'post_time', 'text_length', 'post_description', \n",
    "        'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - FULL PROGRESS TRACKING!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    limit_input = input(\"How many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(\"\\nFilters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit)\n",
    "    \n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb1de7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3Ô∏è‚É£ FIX ADD TIME_AGO\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb656832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50):\n",
    "    \"\"\"üî• FIXED: Uses reference code's URL - gets 20-100+ posts!\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t=month&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: {search_url.split('q=')[1][:50]}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• NEW: Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        # Parse the full datetime string\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        # Calculate components\n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        # Build readable string\n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:  # Only show hours if no larger units\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50):\n",
    "    \"\"\"MAIN FUNCTION - FULL PROGRESS + TIME_AGO COLUMN!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                        \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - WITH TIME_AGO COLUMN!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    limit_input = input(\"How many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(\"\\nFilters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit)\n",
    "    \n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c6f5f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4Ô∏è‚É£ FIX ADD PERIOD FILTER\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - FULL PROGRESS + TIME_AGO + EXACT HTML PERIOD!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,period_filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - EXACT HTML DROPDOWN MATCH!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - EXACT HTML PERIOD FILTER!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    limit_input = input(\"How many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # üî• EXACT HTML DROPDOWN for Relevance/Top/Comments\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}_{period_filename}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f60e6e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5Ô∏è‚É£ STABLE SCRIPT V1 (IMAGES no file format)\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48730ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - FULL PROGRESS + TIME_AGO + EXACT HTML PERIOD!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,period_filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - EXACT HTML DROPDOWN MATCH!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - EXACT HTML PERIOD FILTER!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    limit_input = input(\"How many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # üî• EXACT HTML DROPDOWN for Relevance/Top/Comments\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}_{period_filename}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe601b",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ FIX REORDER THE INPUT KEYWORD - FILTER - PERIOD - LIMIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/comments/1abc{i}123/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'1abc{i}123' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - FULL PROGRESS + TIME_AGO + EXACT HTML PERIOD!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        viewable_url = f\"https://i.redd.it/{media_id}.png\"\n",
    "                        if viewable_url not in visual_urls:\n",
    "                            visual_urls.append(viewable_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING FOR EACH POST!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   ‚úÖ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(1.5)\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üìã Columns: post_title,post_link,post_id,num_votes,num_comments,filter,period_filter,post_date,post_time,time_ago,text_length,post_description,post_visual,visual_type,visual_count\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - NEW ORDER: keyword ‚Üí filter ‚Üí period ‚Üí limit\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR - EXACT HTML PERIOD FILTER!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts saved ‚Üí ../data/reddit/{keyword.replace(' ','_')}_{filter}_{period_filename}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7963b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 7Ô∏è‚É£ FIX POST_VISUAL the extraction of images in high quality but without file format\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfd43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD FUNCTION\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals with EXACT naming convention to ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Try each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            if '_img_' in base_filename:\n",
    "                filename = f\"{base_filename}_img_{seq_str}\"\n",
    "            elif '_vid_' in base_filename:\n",
    "                filename = f\"{base_filename}_vid_{seq_str}\"\n",
    "            else:\n",
    "                filename = f\"{base_filename}_img_{seq_str}\"\n",
    "            \n",
    "            filepath = os.path.join(visual_folder, filename)\n",
    "            \n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"   üìÅ SKIP {filename}\")\n",
    "                downloaded_files.append(filename)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                resp = requests.get(url, headers=headers_browser, timeout=15, stream=True)\n",
    "                if resp.status_code == 200:\n",
    "                    content_type = resp.headers.get('content-type', '').lower()\n",
    "                    size = len(resp.content)\n",
    "                    \n",
    "                    if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}] {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Next sequence\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è  Empty/Invalid: {size}B ({content_type[:20]})\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Error: {str(e)[:30]}\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Try each URL once\n",
    "        for url in visual_urls:\n",
    "            try:\n",
    "                resp = requests.get(url, headers=headers_browser, timeout=15, stream=True)\n",
    "                if resp.status_code == 200:\n",
    "                    content_type = resp.headers.get('content-type', '').lower()\n",
    "                    size = len(resp.content)\n",
    "                    \n",
    "                    if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                        if 'video' in content_type:\n",
    "                            filename = f\"{base_filename}_vid\"\n",
    "                        else:\n",
    "                            filename = f\"{base_filename}_img\"\n",
    "                        \n",
    "                        filepath = os.path.join(visual_folder, filename)\n",
    "                        \n",
    "                        if os.path.exists(filepath):\n",
    "                            print(f\"   üìÅ SKIP {filename}\")\n",
    "                            downloaded_files.append(filename)\n",
    "                            break\n",
    "                        \n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}] {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è  Empty/Invalid: {size}B ({content_type[:20]})\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Error: {str(e)[:30]}\")\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - FULL PROGRESS + TIME_AGO + AUTO DOWNLOAD TO ../data/visuals/\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + AUTO DOWNLOAD\n",
    "                    visual_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if visual_urls:\n",
    "                        post_data['post_visual'] = '\\n'.join(visual_urls)\n",
    "                        vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                        post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                        \n",
    "                        print(f\"   üñºÔ∏è [{progress}] {vtype} ({vcount}) - {len(visual_urls)} URLs!\")\n",
    "                        print(f\"   üíæ Downloading ‚Üí {VISUALS_FOLDER}/...\")\n",
    "                        \n",
    "                        # üî• AUTOMATIC DOWNLOAD with EXACT naming!\n",
    "                        downloaded_files = download_visual_auto(\n",
    "                            post_number, vtype, visual_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                        post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                        \n",
    "                        print(f\"   ‚úÖ [{progress}] {len(downloaded_files)} files saved!\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.0)  # Increased delay for downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üìã NEW column: downloaded_files\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - NOW WITH AUTO DOWNLOAD!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT POST EXTRACTOR + AUTO VISUAL DOWNLOAD!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts + AUTO DOWNLOAD!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fb79b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 8Ô∏è‚É£ FIX POST_VISUAL file format downloaded & fix direct links & remove broken links\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b7521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# üî• NEW: Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS ONLY\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    working_urls = []\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "    \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUTO DOWNLOAD\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                        \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE', \n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                        \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                            \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT!)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79527be9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 9Ô∏è‚É£ STABLE SCRIPT V2 (IMAGES HIGH QUALITY)\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d160e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# üî• NEW: Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS ONLY\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    working_urls = []\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "    \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUTO DOWNLOAD\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                        \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE', \n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                        \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                            \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT!)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47279513",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üîü STABLE SCRIPT V3 (IMAGES & GIFT & VIDEOS without audio)\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# üî• NEW: Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS ONLY\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    working_urls = []\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "    \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUTO DOWNLOAD\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                        \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE', \n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                        \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                            \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT!)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4d77c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üåü11üåü FIX DOWNLOAD AUDIO .m4a\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb35cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• NEW: yt-dlp for PERFECT video+audio\n",
    "import ffmpeg  # üî• NEW: Fallback audio merging\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year', \n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "        \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "            \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "    \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "        \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "        \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "        \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "        \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "        \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "# üî• NEW: PERFECT VIDEO+ AUDIO DOWNLOAD with yt-dlp\n",
    "def download_video_with_audio_yt_dlp(post_link, base_filename, visual_folder):\n",
    "    \"\"\"üöÄ Downloads SINGLE MP4 with VIDEO + AUDIO using yt-dlp (no ffmpeg needed)\"\"\"\n",
    "    try:\n",
    "        video_path = os.path.join(visual_folder, f\"{base_filename}_vid_with_audio.mp4\")\n",
    "        if os.path.exists(video_path):\n",
    "            print(f\"   üìÅ SKIP video (exists): {os.path.basename(video_path)}\")\n",
    "            return [os.path.basename(video_path)]\n",
    "        \n",
    "        print(f\"   üé• yt-dlp: PERFECT video + audio ‚Üí {os.path.basename(video_path)}\")\n",
    "        \n",
    "        cmd = [\n",
    "            'yt-dlp',\n",
    "            '--merge-output-format', 'mp4',\n",
    "            '-f', 'best[ext=mp4][height<=1080]/bestvideo[height<=1080]+bestaudio/best',\n",
    "            '--no-playlist',\n",
    "            '-o', video_path,\n",
    "            post_link\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "        if result.returncode == 0 and os.path.exists(video_path) and os.path.getsize(video_path) > 10000:\n",
    "            size_kb = os.path.getsize(video_path) / 1024\n",
    "            print(f\"   üíæ ‚úÖ VIDEO+AUDIO ({size_kb:.1f}KB): {os.path.basename(video_path)}\")\n",
    "            return [os.path.basename(video_path)]\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "        return []\n",
    "\n",
    "# üî• ENHANCED: All possible media URLs (video/audio/images)\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS + AUDIO (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\", \n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "        \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "        \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "# üî• Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "            \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS + yt-dlp VIDEO+AUDIO\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder, post_link):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí yt-dlp for videos!\"\"\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "    \n",
    "    # üî• SPECIAL CASE: REDDIT VIDEO POST ‚Üí yt-dlp MAGIC (video + audio combined!)\n",
    "    if 'v.redd.it' in post_link or any('v.redd.it' in url for url in visual_urls):\n",
    "        print(f\"   üé¨ DETECTED REDDIT VIDEO POST ‚Üí Using yt-dlp (video+audio)\")\n",
    "        yt_dlp_files = download_video_with_audio_yt_dlp(post_link, base_filename, visual_folder)\n",
    "        if yt_dlp_files:\n",
    "            downloaded_files.extend(yt_dlp_files)\n",
    "            return downloaded_files, [f\"yt-dlp:{post_link}\"]\n",
    "    \n",
    "    working_urls = []\n",
    "    \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                        \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "    \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + VIDEO+AUDIO!\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "    \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']: \n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be', 'yt-dlp']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "    \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "    \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "            \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "            \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "            \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "            \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "    \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS + VIDEO+AUDIO!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A'\n",
    "        }\n",
    "        \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                    \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                    \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                    \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                    \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                    \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                    \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                        \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS + yt-dlp!\n",
    "                        vtype = 'CAROUSEL' if len(all_candidate_urls) > 5 else 'IMAGE'\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, vtype, all_candidate_urls, base_filename, VISUALS_FOLDER, row['post_link']\n",
    "                        )\n",
    "                        \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls or downloaded_files:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls) if working_urls else 'yt-dlp_success'\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                            \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                    \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "        \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments', \n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length', \n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files'\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY + VIDEO WITH AUDIO!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS + VIDEO+AUDIO!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (VIDEO+AUDIO + PROPER .EXT!)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "    \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "    \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month', \n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "    \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "    \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY + VIDEO WITH AUDIO (.mp4) + PROPER .png/.gif extensions!\")\n",
    "    print(f\"‚úÖ pip install yt-dlp  # REQUIRED for video+audio!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "    \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n",
    "    print(f\"üéµ ALL VIDEOS NOW HAVE AUDIO! Test with VLC üé•üîä\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a3a00",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üéº12üéº ADDED CODE OF DOWNLOAD AUDIO .m4a TO MAIN \n",
    "and save them in ../data/audio/\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• ADDED for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "          \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "      \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "          \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "          \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "      \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "      \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "      \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "      \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "      \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "      \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "      \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "# üî• NEW: Enhanced media extraction from reference code\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "      \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "      \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "# üî• NEW: Test URL and get content-type + extension\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "          \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "              \n",
    "                return url, content_type, file_ext\n",
    "          \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "          \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "              \n",
    "                return url, content_type, file_ext\n",
    "              \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• FROM REFERENCE: Reddit audio downloader\n",
    "def download_reddit_audio_only(post_url, output_folder=None):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY (highest quality M4A) from Reddit video post\"\"\"\n",
    "    # Extract post ID for folder naming\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # Use provided folder or create post-specific folder\n",
    "    if output_folder:\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        audio_path = os.path.join(output_folder, f\"{post_id}_audio.m4a\")\n",
    "    else:\n",
    "        folder = f\"audio_{post_id}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        audio_path = os.path.join(folder, f\"{post_id}_audio.m4a\")\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio', # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality (smaller than MP3)\n",
    "        '--audio-quality', '0', # Highest quality (lossless)\n",
    "        '--embed-metadata', # Title, uploader info\n",
    "        '-o', audio_path, # Exact filename\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            return audio_path\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• NEW: AUTOMATIC DOWNLOAD with PROPER EXTENSIONS + WORKING LINKS ONLY\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "          \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "              \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "              \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "              \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "              \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                      \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "              \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "              \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "              \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "              \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                      \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUTO DOWNLOAD + AUDIO\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"  # üî• NEW AUDIO FOLDER\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")  # üî• NEW\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)  # üî• NEW\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "          \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "          \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "          \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "          \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "          \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "              \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + AUDIO!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "      \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• NEW AUDIO COLUMN\n",
    "        }\n",
    "      \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "      \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "      \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                  \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                  \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                  \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                  \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                  \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                  \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                      \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                      \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                          \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                  \n",
    "                    # üî• NEW: DOWNLOAD AUDIO for VIDEO posts using yt-dlp\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        audio_path = download_reddit_audio_only(post_data['post_link'], AUDIO_FOLDER)\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio saved: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio (not video post)\")\n",
    "                  \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "              \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "      \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Increased delay for testing + downloads\n",
    "        print()  # Empty line for readability\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• NEW AUDIO COLUMN\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/\")  # üî• NEW\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• INTERACTIVE RUN - WORKING LINKS + PROPER EXTENSIONS + AUDIO!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + AUTO DOWNLOAD (WORKING LINKS + PROPER .EXT + AUDIO!)\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS ONLY in post_visual + PROPER .png/.mp4/.gif extensions + AUDIO!\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/visuals/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c789e41",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üéº13üéº RENAME DOWNLOADED AUDIO .m4a \n",
    "and save them in ../data/audio/\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "         \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "     \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "         \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "         \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "     \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "     \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "     \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "     \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "     \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "     \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "     \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "     \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "     \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• FIXED: PERFECT AUDIO NAMING FORMAT\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• EXACT REQUIRED NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "         \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "             \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "             \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + PERFECT AUDIO NAMING\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "     \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "     \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "     \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "         \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + PERFECT AUDIO!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "     \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• PERFECT NAMING COLUMN\n",
    "        }\n",
    "     \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "     \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "     \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                 \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                 \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                 \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                 \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                 \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                 \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                     \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                     \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                         \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                 \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                 \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "     \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9af7c1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üéº14üéº COMBINE AUDIO + VIDEO and replace the same video without audio \n",
    "and save them in ../data/visuals/\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39290b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "print(shutil.which(\"ffmpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bafe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install moviepy\n",
    "notepad combine_moviepy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# üî• 100% Python - NO install needed - Uses built-in subprocess\n",
    "def easy_video_merge():\n",
    "    \"\"\"‚úÖ Works instantly from notebooks/ - oban_star_racers_hot_all_time\"\"\"\n",
    "    \n",
    "    # Your exact paths\n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-music\")\n",
    "    video_path = base_dir / \"data\" / \"visuals\" / \"oban_star_racers_hot_all_time\" / \"oban_star_racers_hot_all_time_2_vid.mp4\"\n",
    "    audio_path = base_dir / \"data\" / \"audio\" / \"oban_star_racers_hot_all_time\" / \"oban_star_racers_hot_all_time_2_audio.m4a\"\n",
    "    \n",
    "    print(f\"üìÅ Video: {video_path}\")\n",
    "    print(f\"üìÅ Audio: {audio_path}\")\n",
    "    \n",
    "    if not video_path.exists():\n",
    "        print(\"‚ùå Video missing!\")\n",
    "        return\n",
    "    if not audio_path.exists():\n",
    "        print(\"‚ùå Audio missing!\")\n",
    "        return\n",
    "    \n",
    "    # üî• ONE LINE FFmpeg (Windows built-in works)\n",
    "    cmd = [\n",
    "        r\"C:\\ffmpeg\\bin\\ffmpeg.exe\", \"-y\",           # Overwrite\n",
    "        \"-i\", str(video_path),                      # Input video\n",
    "        \"-i\", str(audio_path),                      # Input audio  \n",
    "        \"-c:v\", \"copy\",                             # Fast video copy\n",
    "        \"-c:a\", \"aac\",                              # Audio encode\n",
    "        \"-shortest\",                                # Match shortest duration\n",
    "        str(video_path)                             # Output = overwrite video\n",
    "    ]\n",
    "    \n",
    "    print(\"üîÑ Merging...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ SUCCESS! {video_path.name} = {video_path.stat().st_size / 1e6:.1f} MB üé•üîä\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.stderr[:200]}\")\n",
    "\n",
    "# üöÄ RUN NOW\n",
    "easy_video_merge()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def easy_video_merge_fixed():\n",
    "    \"\"\"‚úÖ Complete fixed version - Handles errors, logs everything, safe overwrite\"\"\"\n",
    "    \n",
    "    # Your exact paths\n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-music\")\n",
    "    video_path = base_dir / \"data\" / \"visuals\" / \"oban_star_racers_hot_all_time\" / \"oban_star_racers_hot_all_time_2_vid.mp4\"\n",
    "    audio_path = base_dir / \"data\" / \"audio\" / \"oban_star_racers_hot_all_time\" / \"oban_star_racers_hot_all_time_2_audio.m4a\"\n",
    "    output_path = video_path.with_suffix(\".merged.mp4\")  # Safe new output\n",
    "    \n",
    "    print(f\"üìÅ Video: {video_path}\")\n",
    "    print(f\"üìÅ Audio: {audio_path}\")\n",
    "    print(f\"üìÅ Output: {output_path}\")\n",
    "    \n",
    "    if not video_path.exists():\n",
    "        print(\"‚ùå Video missing!\")\n",
    "        return\n",
    "    if not audio_path.exists():\n",
    "        print(\"‚ùå Audio missing!\")\n",
    "        return\n",
    "    \n",
    "    # Create log file\n",
    "    log_file = base_dir / \"ffmpeg_debug.log\"\n",
    "    print(f\"üìù Log: {log_file}\")\n",
    "    \n",
    "    # üî• FIXED FFmpeg command with explicit stream mapping\n",
    "    cmd = [\n",
    "        r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "        \"-y\",  # Overwrite\n",
    "        \"-i\", str(video_path),\n",
    "        \"-i\", str(audio_path),\n",
    "        \"-map\", \"0:v:0\",   # First video stream\n",
    "        \"-map\", \"1:a:0\",   # First audio stream\n",
    "        \"-c:v\", \"copy\",    # Fast video copy\n",
    "        \"-c:a\", \"aac\",     # Re-encode audio to AAC\n",
    "        \"-shortest\",       # Match shortest duration\n",
    "        \"-progress\", str(log_file),  # Real-time progress to log\n",
    "        str(output_path)   # Output file\n",
    "    ]\n",
    "    \n",
    "    print(\"üîÑ Merging... (check log for progress)\")\n",
    "    \n",
    "    # Run with full stderr logging\n",
    "    with open(log_file, \"w\") as log:\n",
    "        result = subprocess.run(\n",
    "            cmd, \n",
    "            stderr=subprocess.STDOUT,  # Capture ALL output\n",
    "            stdout=log, \n",
    "            text=True,\n",
    "            cwd=base_dir\n",
    "        )\n",
    "    \n",
    "    # Check result\n",
    "    if result.returncode == 0 and output_path.exists():\n",
    "        size_mb = output_path.stat().st_size / 1e6\n",
    "        print(f\"‚úÖ SUCCESS! {output_path.name} ({size_mb:.1f} MB) üé•üîä\")\n",
    "        print(\"üîç Full log saved:\", log_file)\n",
    "    else:\n",
    "        print(\"‚ùå FAILED! Check full log:\")\n",
    "        with open(log_file, \"r\") as f:\n",
    "            print(f.read()[:1000])  # First 1000 chars\n",
    "        print(\"üí° Run manual test in CMD to debug further\")\n",
    "\n",
    "# üöÄ RUN NOW - SAFE VERSION\n",
    "easy_video_merge_fixed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def auto_video_merge_all():\n",
    "    \"\"\"üöÄ Automatically merge ALL video+audio pairs in your reddit-music folders\"\"\"\n",
    "    \n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-music\")\n",
    "    visuals_dir = base_dir / \"data\" / \"visuals\"\n",
    "    audio_dir = base_dir / \"data\" / \"audio\"\n",
    "    \n",
    "    print(\"üîç Scanning for video/audio pairs...\")\n",
    "    \n",
    "    # Find ALL video files matching pattern\n",
    "    video_files = list(visuals_dir.rglob(\"*_vid.mp4\"))\n",
    "    print(f\"üìä Found {len(video_files)} video files\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        # Extract keyword_filter_period_postnumber from filename\n",
    "        video_name = video_path.stem  # removes .mp4\n",
    "        if not video_name.endswith(\"_vid\"):\n",
    "            continue\n",
    "            \n",
    "        keyword_filter_period_postnumber = video_name[:-4]  # remove _vid\n",
    "        \n",
    "        # Construct matching audio path\n",
    "        audio_path = audio_dir / video_path.relative_to(visuals_dir).parent / f\"{keyword_filter_period_postnumber}_audio.m4a\"\n",
    "        output_path = video_path.with_suffix(\".merged.mp4\")\n",
    "        \n",
    "        print(f\"\\nüîÑ [{success_count + fail_count + 1}/{len(video_files)}] {keyword_filter_period_postnumber}\")\n",
    "        print(f\"   üìÅ Video:  {video_path}\")\n",
    "        print(f\"   üìÅ Audio:  {audio_path}\")\n",
    "        print(f\"   üìÅ Output: {output_path}\")\n",
    "        \n",
    "        # Skip if already merged\n",
    "        if output_path.exists():\n",
    "            print(\"   ‚úÖ Already exists - SKIPPING\")\n",
    "            success_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not audio_path.exists():\n",
    "            print(\"   ‚ùå Audio missing - SKIPPING\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Merge command\n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "            \"-y\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-i\", str(audio_path),\n",
    "            \"-map\", \"0:v:0\",\n",
    "            \"-map\", \"1:a:0\",\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-shortest\",\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                size_mb = output_path.stat().st_size / 1e6\n",
    "                print(f\"   ‚úÖ SUCCESS! ({size_mb:.1f} MB)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ‚ùå FAILED: {result.stderr[:200]}\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   ‚è∞ TIMEOUT - Killed\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}\")\n",
    "    print(f\"   ‚ùå Failed:  {fail_count}\")\n",
    "    print(f\"   üìÅ Total:   {len(video_files)}\")\n",
    "\n",
    "# üöÄ RUN ALL AUTOMATICALLY\n",
    "auto_video_merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c4d3f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üß† STABLE SCRIPT V4 üß†\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: green;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70407d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "         \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "     \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "         \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "         \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "     \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "     \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "     \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "     \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "     \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "     \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "     \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "     \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "     \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• AUDIO NAMING FORMAT\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "         \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "             \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "             \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + AUDIO NAMING\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "     \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "     \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "     \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "         \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + AUDIO NAMING\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "     \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'\n",
    "        }\n",
    "     \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "     \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "     \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                 \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                 \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                 \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                 \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                 \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                 \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                     \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                     \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                         \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                 \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ example oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                 \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "     \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed846626",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üóÉÔ∏è STEPS TO EXTRACT & DOWNLOAD\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: purple;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf6ed1",
   "metadata": {},
   "source": [
    "\n",
    "| Type                     | Directory              | Description                          |\n",
    "|--------------------------|----------------------|--------------------------------------|\n",
    "| **Videos (mute)**            | `../data/videos/`       | Video files with no audio            |\n",
    "| **Audio files**              | `../data/audio/`        | Audio-only files                     |\n",
    "| **Images, GIFs, Videos+Audio** | `../data/visuals/`   | Visual files including images, GIFs, and videos with audio |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad0345",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## üóÉÔ∏è 1 - EXTRACTION\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: purple;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc7e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\n",
      "============================================================\n",
      "\n",
      "üî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\n",
      "\n",
      "üî• Scraping 10 'oban star racers' HOT posts...\n",
      "‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\n",
      "üì° Fetching EXACTLY 10 hot posts (Period: All time)...\n",
      "üîç Fetching UP TO 10 hot posts for 'oban star racers'...\n",
      "   üì° API: q=oban%20star%20racers&sort=hot&t=month...\n",
      "   ‚úÖ API returned 100 posts available\n",
      "‚úÖ SUCCESS: 10/10 hot posts loaded!\n",
      "‚úÖ Saved 10 REAL posts ‚Üí ../data/reddit/oban_star_racers_main.csv\n",
      "\n",
      "üöÄ PROCESSING 10 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  IMAGES/GIFs ‚Üí ../data/visuals/\n",
      "üéµ AUDIO ‚Üí ../data/audio/\n",
      "====================================================================================================\n",
      "üîç [ 1/10] NEW OFICIAL OBAN STAR RACERS COMIC...\n",
      "   üîó [ 1/10] Post ID: 1pk06te\n",
      "   üìÖ [ 1/10] Thursday, December 11, 2025 | üïê 04:23:24 PM  | ‚è∞ 6 hours ago\n",
      "   üìù [ 1/10] 160 chars\n",
      "   üñºÔ∏è [ 1/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].png oban_star_racers_hot_all_time_1_img.png (597.3KB)\n",
      "   ‚úÖ [ 1/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 1/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 2/10] Respect Ondai (Oban Star Racers)...\n",
      "   üîó [ 2/10] Post ID: 1pjk4g8\n",
      "   üìÖ [ 2/10] Thursday, December 11, 2025 | üïê 01:59:47 AM  | ‚è∞ 21 hours ago\n",
      "   üìù [ 2/10] 1630 chars\n",
      "   ‚ûñ [ 2/10] No visuals\n",
      "   üéâ [ 2/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 3/10] Oban Star-Racers extended opening intro (2005)....\n",
      "   üîó [ 3/10] Post ID: 1pf2m6v\n",
      "   üìÖ [ 3/10] Friday, December 05, 2025 | üïê 07:24:56 PM  | ‚è∞ 6 days ago\n",
      "   üìù [ 3/10] 104 chars\n",
      "   üñºÔ∏è [ 3/10] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_hot_all_time_3_vid.mp4 (10595.4KB)\n",
      "   ‚úÖ [ 3/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 3/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_hot_all_time_3_audio.m4a\n",
      "   ‚úÖ Audio saved: oban_star_racers_hot_all_time_3_audio.m4a (1652.4KB)\n",
      "   ‚úÖ [ 3/10] Audio: oban_star_racers_hot_all_time_3_audio.m4a\n",
      "   üéâ [ 3/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 4/10] Oban Star Racers Chance to Shine ‚ù§Ô∏è‚Äçüî• Alwas Traps ü™§ Please S...\n",
      "   üîó [ 4/10] Post ID: 1pdpc6z\n",
      "   üìÖ [ 4/10] Thursday, December 04, 2025 | üïê 04:24:36 AM  | ‚è∞ 1 week ago\n",
      "   üìù [ 4/10] 1089 chars\n",
      "   üñºÔ∏è [ 4/10] Testing 112 candidate URLs...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/ruxxyn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_13.gif (481.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/ruxxyn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/33c01o60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_29.gif (760.8KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/33c01o60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/tcw0vn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_45.gif (728.4KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/tcw0vn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/rnvzfn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_61.gif (2640.9KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/rnvzfn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/feidon60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.webm...\n",
      "   üíæ [IMAGE].gif oban_star_racers_hot_all_time_4_img_77.gif (454.1KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.png...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpg...\n",
      "   ‚ùå Broken URL: https://i.redd.it/feidon60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/mlkeqn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_4_img_95.jpg (110.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/mlkeqn60w35g1.jpeg...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_360.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_1080...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_720...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_480...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio.mp4...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/audio.m4a...\n",
      "   ‚ùå Broken URL: https://v.redd.it/id1qxn60w35g1/DASH_audio...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.mp4...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.webm...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.gif...\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.png...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_4_img_111.jpg (143.5KB)\n",
      "   ‚ùå Broken URL: https://i.redd.it/id1qxn60w35g1.jpeg...\n",
      "   ‚úÖ [ 4/10] CAROUSEL (7) - 7 WORKING URLs!\n",
      "   üíæ 7 files saved!\n",
      "   üéâ [ 4/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 5/10] 20 Years Ago Today on December 2, 2005, the official trailer...\n",
      "   üîó [ 5/10] Post ID: 1pcl5km\n",
      "   üìÖ [ 5/10] Tuesday, December 02, 2025 | üïê 10:30:11 PM  | ‚è∞ 1 week 2 days ago\n",
      "   üìù [ 5/10] 9 chars\n",
      "   üñºÔ∏è [ 5/10] Testing 1 candidate URLs...\n",
      "   üíæ [VIDEO].mp4 oban_star_racers_hot_all_time_5_vid.mp4 (13218.3KB)\n",
      "   ‚úÖ [ 5/10] VIDEO (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéµ [ 5/10] Extracting audio...\n",
      "   üéµ Running yt-dlp ‚Üí oban_star_racers_hot_all_time_5_audio.m4a\n",
      "   ‚úÖ Audio saved: oban_star_racers_hot_all_time_5_audio.m4a (2758.7KB)\n",
      "   ‚úÖ [ 5/10] Audio: oban_star_racers_hot_all_time_5_audio.m4a\n",
      "   üéâ [ 5/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 6/10] Kurakasis: Star Wars game annoucment incoming that ends with...\n",
      "   üîó [ 6/10] Post ID: 1pjvv8j\n",
      "   üìÖ [ 6/10] Thursday, December 11, 2025 | üïê 01:05:00 PM  | ‚è∞ 9 hours ago\n",
      "   üìù [ 6/10] 98 chars\n",
      "   ‚ûñ [ 6/10] No visuals\n",
      "   üéâ [ 6/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 7/10] There‚Äôs another Star Wars game in development ending with ‚ÄòR...\n",
      "   üîó [ 7/10] Post ID: 1pjva8a\n",
      "   üìÖ [ 7/10] Thursday, December 11, 2025 | üïê 12:31:45 PM  | ‚è∞ 10 hours ago\n",
      "   ‚ûñ [ 7/10] No visuals\n",
      "   üéâ [ 7/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 8/10] Fimbul Mamba Cafe Racer with nose art - Star Atlas, by Gary ...\n",
      "   üîó [ 8/10] Post ID: 1pjrlmt\n",
      "   üìÖ [ 8/10] Thursday, December 11, 2025 | üïê 08:30:58 AM  | ‚è∞ 14 hours ago\n",
      "   üñºÔ∏è [ 8/10] Testing 1 candidate URLs...\n",
      "   üíæ [IMAGE].jpg oban_star_racers_hot_all_time_8_img.jpg (1296.5KB)\n",
      "   ‚úÖ [ 8/10] IMAGE (1) - 1 WORKING URLs!\n",
      "   üíæ 1 files saved!\n",
      "   üéâ [ 8/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [ 9/10] Kurakasis: There‚Äôs another Star Wars game announcement comin...\n",
      "   üîó [ 9/10] Post ID: 1pjxg5d\n",
      "   üìÖ [ 9/10] Thursday, December 11, 2025 | üïê 02:25:13 PM  | ‚è∞ 8 hours ago\n",
      "   ‚ûñ [ 9/10] No visuals\n",
      "   üéâ [ 9/10] COMPLETE ‚úì\n",
      "\n",
      "üîç [10/10] Star Citizen The Cup - Origin M50 Turbo, Loved By Racers!...\n",
      "   üîó [10/10] Post ID: 1pgpnyd\n",
      "   üìÖ [10/10] Sunday, December 07, 2025 | üïê 07:33:10 PM  | ‚è∞ 4 days ago\n",
      "   üñºÔ∏è [10/10] Testing 1 candidate URLs...\n",
      "   ‚ùå [10/10] No working URLs found\n",
      "   üéâ [10/10] COMPLETE ‚úì\n",
      "\n",
      "\n",
      "üéâ SAVED 10/10 posts ‚Üí ../data/reddit/oban_star_racers_hot_all_time.csv\n",
      "üíæ VIDEOS(mute) ‚Üí ../data/videos/\n",
      "üñºÔ∏è  VISUALS ‚Üí ../data/visuals/\n",
      "üéµ AUDIO ‚Üí ../data/audio/ (keyword_filter_period_postnumber_audio.m4a)\n",
      "‚úÖ post_visual = WORKING LINKS ONLY!\n",
      "\n",
      "‚úÖ DONE! 10 posts + media ‚Üí ../data/\n",
      "üìÅ Videos(mute): ../data/videos/\n",
      "üìÅ Visuals: ../data/visuals/\n",
      "üéµ Audio: ../data/audio/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "    \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "        \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "        \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "    \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "    \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "    \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "    \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "    \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "    \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "    \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "    \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "        \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "        \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "                \n",
    "                return url, content_type, file_ext\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# üî• FIXED: PERFECT AUDIO NAMING FORMAT - SAVES TO ../data/audio/\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• EXACT REQUIRED NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visuals_folder, videos_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí CORRECT FOLDERS BY TYPE\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "        \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # üî• ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                else:  # images, gifs\n",
    "                    target_folder = visuals_folder\n",
    "                    file_prefix = 'img'\n",
    "                \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "            \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "    \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "                \n",
    "                # üî• ROUTE TO CORRECT FOLDER BY CONTENT TYPE\n",
    "                if 'video' in content_type:\n",
    "                    target_folder = videos_folder\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:  # images, gifs\n",
    "                    target_folder = visuals_folder\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "                \n",
    "                filepath = os.path.join(target_folder, filename)\n",
    "            \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "                \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + PERFECT AUDIO + CORRECT FOLDERS\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    \n",
    "    # üî• NEW FOLDER STRUCTURE PER SPECS\n",
    "    VIDEOS_FOLDER = \"../data/videos\"           # Videos (mute)\n",
    "    VISUALS_FOLDER = \"../data/visuals\"         # Images, GIFs, Videos+Audio  \n",
    "    AUDIO_FOLDER = \"../data/audio\"             # Audio files\n",
    "    \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  IMAGES/GIFs ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VIDEOS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "    \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "    \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "    \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "    \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + PERFECT AUDIO + CORRECT FOLDERS!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "    \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• PERFECT NAMING COLUMN\n",
    "        }\n",
    "    \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "    \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "    \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD TO CORRECT FOLDERS\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                    \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS + CORRECT FOLDERS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER, VIDEOS_FOLDER  # üî• PASS BOTH FOLDERS\n",
    "                        )\n",
    "                    \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                        \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING TO ../data/audio/\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER  # üî• FIXED: ../data/audio/\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "    \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ VIDEOS(mute) ‚Üí {VIDEOS_FOLDER}/\")\n",
    "    print(f\"üñºÔ∏è  VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ VIDEOS‚Üí../data/videos/ | IMAGES‚Üí../data/visuals/ | AUDIO‚Üí../data/audio/\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + media ‚Üí ../data/\")\n",
    "    print(f\"üìÅ Videos(mute): ../data/videos/\")\n",
    "    print(f\"üìÅ Visuals: ../data/visuals/\")\n",
    "    print(f\"üéµ Audio: ../data/audio/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35728298",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üóÉÔ∏è STEP 01: EXTRACT ALL DATA & CONTENT \n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: hotpink;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68597ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import subprocess  # üî• for yt-dlp audio\n",
    "\n",
    "def create_sample_main_data(keyword, limit):\n",
    "    \"\"\"Generate sample Reddit main data if input file missing\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    sample_data = {\n",
    "        'post_title': [f'{keyword.title()} post {i+1}' for i in range(limit)],\n",
    "        'post_link': [f'https://www.reddit.com/r/{keyword}/{i+1}/title{i+1}/' for i in range(limit)],\n",
    "        'post_id': [f'{i+1}' for i in range(limit)]\n",
    "    }\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "    pd.DataFrame(sample_data).to_csv(INPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Created sample MAIN data ({limit} posts): {INPUT_FILE}\")\n",
    "    return INPUT_FILE\n",
    "\n",
    "def get_period_param(period_filter):\n",
    "    \"\"\"üî• EXACT HTML MATCH: Convert display text ‚Üí Reddit API 't=' parameter\"\"\"\n",
    "    period_map = {\n",
    "        'All time': 'all',\n",
    "        'Past year': 'year',\n",
    "        'Past month': 'month',\n",
    "        'Past week': 'week',\n",
    "        'Today': 'day',\n",
    "        'Past hour': 'hour'\n",
    "    }\n",
    "    return period_map.get(period_filter, 'month')\n",
    "\n",
    "def fetch_reddit_posts_search(search_keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• UPDATED: Uses EXACT HTML period_filter values\"\"\"\n",
    "    print(f\"üîç Fetching UP TO {limit} {filter} posts for '{search_keyword}'...\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "   \n",
    "    encoded_keyword = urllib.parse.quote(search_keyword)\n",
    "    period_param = get_period_param(period_filter) if period_filter else 'month'\n",
    "    search_url = f\"https://www.reddit.com/search.json?q={encoded_keyword}&sort={filter}&limit=100&t={period_param}&type=link\"\n",
    "   \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'application/json,'\n",
    "    }\n",
    "   \n",
    "    try:\n",
    "        print(f\"   üì° API: q={encoded_keyword}&sort={filter}&t={period_param}...\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Search failed: HTTP {response.status_code}\")\n",
    "            return None\n",
    "         \n",
    "        data = response.json()\n",
    "        posts = []\n",
    "     \n",
    "        if 'data' in data and 'children' in data['data']:\n",
    "            available = len(data['data']['children'])\n",
    "            print(f\"   ‚úÖ API returned {available} posts available\")\n",
    "         \n",
    "            for i, post in enumerate(data['data']['children'][:limit]):\n",
    "                post_data = post['data']\n",
    "                posts.append({\n",
    "                    'post_title': post_data.get('title', 'N/A'),\n",
    "                    'post_link': f\"https://www.reddit.com{post_data.get('permalink', '')}\",\n",
    "                    'post_id': post_data.get('id', 'N/A'),\n",
    "                    'num_votes': post_data.get('score', 0),\n",
    "                    'num_comments': post_data.get('num_comments', 0),\n",
    "                    'filter': filter,\n",
    "                    'period_filter': period_filter or 'N/A'\n",
    "                })\n",
    "         \n",
    "        actual_posts = len(posts)\n",
    "        print(f\"‚úÖ SUCCESS: {actual_posts}/{limit} {filter} posts loaded!\")\n",
    "        return pd.DataFrame(posts)\n",
    "     \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_viewable_image_url(url):\n",
    "    \"\"\"üî• ONLY i.redd.it/xxx.png - NO preview/external-preview EVER\"\"\"\n",
    "    if not url or 'reddit.com' not in url.lower():\n",
    "        return url\n",
    "   \n",
    "    url_lower = url.lower()\n",
    "   \n",
    "    if 'i.redd.it' in url_lower:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "   \n",
    "    match = re.search(r'preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'external-preview\\.redd\\.it/([a-z0-9]+)', url_lower)\n",
    "    if not match:\n",
    "        match = re.search(r'/([a-z0-9]{13})\\.', url_lower)\n",
    "   \n",
    "    if match:\n",
    "        media_id = match.group(1)\n",
    "        return f\"https://i.redd.it/{media_id}.png\"\n",
    "   \n",
    "    return url\n",
    "\n",
    "def format_post_date(created_utc):\n",
    "    \"\"\"Convert Reddit UTC timestamp to readable date/time\"\"\"\n",
    "    if not created_utc or created_utc == 'N/A':\n",
    "        return 'N/A', 'N/A'\n",
    "   \n",
    "    try:\n",
    "        timestamp = float(created_utc)\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        post_date = dt.strftime(\"%A, %B %d, %Y\")\n",
    "        post_time = dt.strftime(\"%I:%M:%S %p UTC\")\n",
    "        return post_date, post_time\n",
    "    except:\n",
    "        return 'ERROR', 'ERROR'\n",
    "\n",
    "def calculate_time_ago(post_date_str, post_time_str):\n",
    "    \"\"\"üî• Calculate '1 year 2 month 3 week 4 day 5 hour ago' format\"\"\"\n",
    "    if post_date_str == 'N/A' or post_time_str == 'N/A':\n",
    "        return 'N/A'\n",
    "   \n",
    "    try:\n",
    "        datetime_str = f\"{post_date_str} {post_time_str.replace(' UTC', '')}\"\n",
    "        post_dt = datetime.strptime(datetime_str, \"%A, %B %d, %Y %I:%M:%S %p\")\n",
    "     \n",
    "        now = datetime.now()\n",
    "        delta = now - post_dt\n",
    "     \n",
    "        years = delta.days // 365\n",
    "        months = (delta.days % 365) // 30\n",
    "        weeks = (delta.days % 30) // 7\n",
    "        days = delta.days % 7\n",
    "        hours = delta.seconds // 3600\n",
    "     \n",
    "        parts = []\n",
    "        if years > 0:\n",
    "            parts.append(f\"{years} year\" + (\"s\" if years > 1 else \"\"))\n",
    "        if months > 0:\n",
    "            parts.append(f\"{months} month\" + (\"s\" if months > 1 else \"\"))\n",
    "        if weeks > 0:\n",
    "            parts.append(f\"{weeks} week\" + (\"s\" if weeks > 1 else \"\"))\n",
    "        if days > 0:\n",
    "            parts.append(f\"{days} day\" + (\"s\" if days > 1 else \"\"))\n",
    "        if hours > 0 and len(parts) == 0:\n",
    "            parts.append(f\"{hours} hour\" + (\"s\" if hours > 1 else \"\"))\n",
    "     \n",
    "        if not parts:\n",
    "            return \"just now\"\n",
    "     \n",
    "        time_ago = \" \".join(parts) + \" ago\"\n",
    "        return time_ago\n",
    "     \n",
    "    except:\n",
    "        return 'ERROR'\n",
    "\n",
    "def get_enhanced_media_candidates(media_id):\n",
    "    \"\"\"üî• Generate ALL possible media URLs for a media_id (videos/images/gifs/audio)\"\"\"\n",
    "    return [\n",
    "        # VIDEOS (highest quality first)\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_360.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_1080\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_720\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_480\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio.mp4\",\n",
    "        f\"https://v.redd.it/{media_id}/audio.m4a\",\n",
    "        f\"https://v.redd.it/{media_id}/DASH_audio\",\n",
    "     \n",
    "        # DIRECT VIDEOS\n",
    "        f\"https://i.redd.it/{media_id}.mp4\",\n",
    "        f\"https://i.redd.it/{media_id}.webm\",\n",
    "     \n",
    "        # IMAGES/GIFS\n",
    "        f\"https://i.redd.it/{media_id}.gif\",\n",
    "        f\"https://i.redd.it/{media_id}.png\",\n",
    "        f\"https://i.redd.it/{media_id}.jpg\",\n",
    "        f\"https://i.redd.it/{media_id}.jpeg\"\n",
    "    ]\n",
    "\n",
    "def test_url_working(url, headers_browser, timeout=10):\n",
    "    \"\"\"üî• Returns (working_url, content_type, file_ext) or None if broken\"\"\"\n",
    "    try:\n",
    "        resp = requests.head(url, headers=headers_browser, timeout=timeout, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = int(resp.headers.get('content-length', 0) or 0)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                # Extract extension from content-type\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "        # Fallback to GET if HEAD fails\n",
    "        resp = requests.get(url, headers=headers_browser, timeout=timeout, stream=True)\n",
    "        if resp.status_code == 200:\n",
    "            content_type = resp.headers.get('content-type', '').lower()\n",
    "            size = len(resp.content)\n",
    "         \n",
    "            if size > 1000 and any(media_type in content_type for media_type in ['video', 'image', 'audio']):\n",
    "                if 'video' in content_type:\n",
    "                    file_ext = '.mp4' if 'mp4' in content_type else '.webm'\n",
    "                elif 'image' in content_type:\n",
    "                    if 'gif' in content_type:\n",
    "                        file_ext = '.gif'\n",
    "                    elif 'png' in content_type:\n",
    "                        file_ext = '.png'\n",
    "                    elif 'jpeg' in content_type:\n",
    "                        file_ext = '.jpg'\n",
    "                    else:\n",
    "                        file_ext = '.jpg'\n",
    "                else:\n",
    "                    file_ext = '.bin'\n",
    "             \n",
    "                return url, content_type, file_ext\n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# üî• FIXED: PERFECT AUDIO NAMING FORMAT\n",
    "def download_reddit_audio_only(post_url, keyword_clean, filter_param, period_str, post_number, audio_folder):\n",
    "    \"\"\"üöÄ PERFECT AUDIO ONLY with EXACT naming: keyword_filter_period_postnumber_audio.m4a\"\"\"\n",
    "    # Extract post ID for validation\n",
    "    post_id = re.search(r'/comments/([a-zA-Z0-9]+)', post_url)\n",
    "    if not post_id:\n",
    "        return None\n",
    "    post_id = post_id.group(1)\n",
    "\n",
    "    # üî• EXACT REQUIRED NAMING FORMAT\n",
    "    audio_filename = f\"{keyword_clean}_{filter_param}_{period_str}_{post_number}_audio.m4a\"\n",
    "    audio_path = os.path.join(audio_folder, audio_filename)\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"   üìÅ Audio exists: {audio_filename}\")\n",
    "        return audio_path\n",
    "\n",
    "    # üî• yt-dlp PERFECT AUDIO command\n",
    "    cmd = [\n",
    "        'yt-dlp',\n",
    "        '--extract-audio',      # Audio only (no video)\n",
    "        '--audio-format', 'm4a', # Best quality M4A\n",
    "        '--audio-quality', '0',  # Highest quality (lossless)\n",
    "        '--embed-metadata',     # Title, uploader info\n",
    "        '-o', audio_path,       # EXACT filename required\n",
    "        post_url\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"   üéµ Running yt-dlp ‚Üí {audio_filename}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=90)\n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            file_size = os.path.getsize(audio_path) / 1024\n",
    "            print(f\"   ‚úÖ Audio saved: {audio_filename} ({file_size:.1f}KB)\")\n",
    "            return audio_path\n",
    "        else:\n",
    "            print(f\"   ‚ùå yt-dlp failed: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ‚è∞ yt-dlp timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  yt-dlp error: {str(e)[:50]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_visual_auto(post_number, visual_type, visual_urls, base_filename, visual_folder):\n",
    "    \"\"\"üî• DOWNLOADS visuals ‚Üí ONLY WORKING LINKS ‚Üí PROPER EXTENSIONS ‚Üí ../data/visuals/\"\"\"\n",
    "    downloaded_files = []\n",
    "   \n",
    "    headers_browser = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'video/*,image/*,audio/*,*/*;q=0.8',\n",
    "        'Referer': 'https://www.reddit.com/'\n",
    "    }\n",
    "   \n",
    "    working_urls = []\n",
    "   \n",
    "    if visual_type == 'CAROUSEL':\n",
    "        # CAROUSEL: Test each URL for each sequence number\n",
    "        for seq_idx, url in enumerate(visual_urls, 1):\n",
    "            seq_str = f\"{seq_idx:02d}\"\n",
    "            result = test_url_working(url, headers_browser)\n",
    "         \n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "             \n",
    "                filename = f\"{base_filename}_{file_prefix}_{seq_str}{file_ext}\"\n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].upper()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Broken URL: {url[:60]}...\")\n",
    "   \n",
    "    else:\n",
    "        # SINGLE IMAGE/VIDEO: Test each URL once\n",
    "        for url in visual_urls:\n",
    "            result = test_url_working(url, headers_browser)\n",
    "            if result:\n",
    "                working_url, content_type, file_ext = result\n",
    "                working_urls.append(working_url)\n",
    "             \n",
    "                # Determine file type prefix\n",
    "                if 'video' in content_type:\n",
    "                    file_prefix = 'vid'\n",
    "                    filename = f\"{base_filename}_vid{file_ext}\"\n",
    "                else:\n",
    "                    file_prefix = 'img'\n",
    "                    filename = f\"{base_filename}_img{file_ext}\"\n",
    "             \n",
    "                filepath = os.path.join(visual_folder, filename)\n",
    "             \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"   üìÅ SKIP {filename}\")\n",
    "                    downloaded_files.append(filename)\n",
    "                    break\n",
    "             \n",
    "                # Download with proper extension\n",
    "                try:\n",
    "                    resp = requests.get(working_url, headers=headers_browser, timeout=15, stream=True)\n",
    "                    if resp.status_code == 200:\n",
    "                        size = len(resp.content)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            for chunk in resp.iter_content(8192):\n",
    "                                f.write(chunk)\n",
    "                     \n",
    "                        media_type = content_type.split('/')[0].UPPER()\n",
    "                        print(f\"   üíæ [{media_type}]{file_ext} {filename} ({size/1024:.1f}KB)\")\n",
    "                        downloaded_files.append(filename)\n",
    "                        time.sleep(0.5)\n",
    "                        break  # Success! Done with this post\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Download error: {str(e)[:30]}\")\n",
    "                break\n",
    "   \n",
    "    return downloaded_files, working_urls\n",
    "\n",
    "def extract_post_details_complete(keyword, filter='hot', limit=50, period_filter=None):\n",
    "    \"\"\"üî• MAIN FUNCTION - WORKING LINKS ONLY + PROPER EXTENSIONS + PERFECT AUDIO NAMING\"\"\"\n",
    "    keyword_clean = keyword.replace(' ', '_')\n",
    "    period_str = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    INPUT_FILE = f\"../data/reddit/{keyword_clean}_main.csv\"\n",
    "    OUTPUT_FILE = f\"../data/reddit/{keyword_clean}_{filter}_{period_str}.csv\"\n",
    "    VISUALS_FOLDER = f\"../data/visuals/{keyword_clean}_{filter}_{period_str}\"\n",
    "    AUDIO_FOLDER = f\"../data/audio/{keyword_clean}_{filter}_{period_str}\"\n",
    "   \n",
    "    print(f\"üì° Fetching EXACTLY {limit} {filter} posts (Period: {period_filter or 'All time'})...\")\n",
    "    df = fetch_reddit_posts_search(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è  Search failed ‚Üí Using sample data\")\n",
    "        create_sample_main_data(keyword_clean, limit)\n",
    "        df = pd.read_csv(INPUT_FILE).head(limit)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(INPUT_FILE), exist_ok=True)\n",
    "        df.to_csv(INPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df)} REAL posts ‚Üí {INPUT_FILE}\")\n",
    "   \n",
    "    total_posts = len(df)\n",
    "    print(f\"\\nüöÄ PROCESSING {total_posts} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ DOWNLOADING visuals to ‚Üí {VISUALS_FOLDER}\")\n",
    "    print(f\"üéµ DOWNLOADING audio to ‚Üí {AUDIO_FOLDER}\")\n",
    "    print(\"=\" * 100)\n",
    "   \n",
    "    os.makedirs(VISUALS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "   \n",
    "    new_data = []\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.reddit.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin'\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "   \n",
    "    def extract_post_id(url):\n",
    "        if pd.isna(url): return None\n",
    "        url = str(url).strip()\n",
    "        match = re.search(r'/comments/([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        match = re.search(r't3_([a-zA-Z0-9]+)', url)\n",
    "        if match: return match.group(1)\n",
    "        return None\n",
    "   \n",
    "    def get_visual_type_count(visual):\n",
    "        if visual in ['N/A', 'MEDIA_ERROR', 'ERROR']:\n",
    "            return 'NONE', 0\n",
    "        visual_str = str(visual).lower()\n",
    "        if any(x in visual_str for x in ['.mp4', 'v.redd.it', 'youtube.com', 'youtu.be']):\n",
    "            return 'VIDEO', 1\n",
    "        if '\\n' in visual_str:\n",
    "            return 'CAROUSEL', len(visual_str.splitlines())\n",
    "        if 'i.redd.it' in visual_str or any(ext in visual_str for ext in ['.jpg', '.png', '.gif']):\n",
    "            return 'IMAGE', 1\n",
    "        return 'OTHER', 1\n",
    "   \n",
    "    def calculate_text_length(description):\n",
    "        if not description or description in ['N/A', 'ERROR', 'INVALID_LINK']:\n",
    "            return 0\n",
    "        text = re.sub(r'http[s]?://\\S+', '', str(description))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return len(text)\n",
    "   \n",
    "    # üî• ENHANCED: Comprehensive visual extraction with carousel + video support\n",
    "    def extract_visual_urls(post_info):\n",
    "        visual_urls = []\n",
    "        try:\n",
    "            # 1. REDDIT VIDEO (highest priority)\n",
    "            if post_info.get('is_video') and post_info.get('media', {}).get('reddit_video'):\n",
    "                fallback_url = post_info['media']['reddit_video'].get('fallback_url', '')\n",
    "                if fallback_url:\n",
    "                    visual_urls.append(fallback_url)\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 2. YOUTUBE/EXTERNAL VIDEO\n",
    "            if any(domain in post_info.get('url', '').lower() for domain in ['youtube.com', 'youtu.be', 'v.redd.it']):\n",
    "                visual_urls.append(post_info['url'])\n",
    "                return visual_urls\n",
    "     \n",
    "            # üî• 3. CAROUSEL - NEW ENHANCED EXTRACTION\n",
    "            gallery_data = post_info.get('gallery_data')\n",
    "            if gallery_data and gallery_data.get('items'):\n",
    "                for item in gallery_data['items']:\n",
    "                    if isinstance(item, dict) and 'media_id' in item:\n",
    "                        media_id = item['media_id']\n",
    "                        # üî• Try ALL possible formats for this media_id\n",
    "                        media_candidates = get_enhanced_media_candidates(media_id)\n",
    "                        for candidate_url in media_candidates:\n",
    "                            visual_urls.append(candidate_url)\n",
    "                if visual_urls:\n",
    "                    return visual_urls\n",
    "     \n",
    "            # 4. SINGLE IMAGE\n",
    "            post_url = post_info.get('url', '')\n",
    "            viewable_url = get_viewable_image_url(post_url)\n",
    "            if viewable_url and 'i.redd.it' in viewable_url:\n",
    "                return [viewable_url]\n",
    "     \n",
    "            # 5. PREVIEW IMAGES\n",
    "            if post_info.get('preview', {}).get('images'):\n",
    "                for img in post_info['preview']['images']:\n",
    "                    source_url = img.get('source', {}).get('url', '')\n",
    "                    if source_url:\n",
    "                        viewable_url = get_viewable_image_url(source_url)\n",
    "                        if 'i.redd.it' in viewable_url:\n",
    "                            return [viewable_url]\n",
    "     \n",
    "            # 6. THUMBNAIL FALLBACK\n",
    "            if post_info.get('thumbnail') and 'i.redd.it' in post_info['thumbnail']:\n",
    "                return [post_info['thumbnail']]\n",
    "         \n",
    "        except:\n",
    "            pass\n",
    "        return visual_urls\n",
    "   \n",
    "    # üî• FULL PROGRESS TRACKING + AUTO DOWNLOAD + WORKING LINKS ONLY + PERFECT AUDIO!\n",
    "    for idx, row in df.iterrows():\n",
    "        progress = f\"{idx+1:2d}/{total_posts}\"\n",
    "        post_title = str(row['post_title'])[:60]\n",
    "        post_number = idx + 1\n",
    "        print(f\"üîç [{progress}] {post_title}...\")\n",
    "     \n",
    "        post_data = {\n",
    "            'post_title': row.get('post_title', 'N/A'),\n",
    "            'post_link': row.get('post_link', 'N/A'),\n",
    "            'post_id': 'N/A',\n",
    "            'num_votes': row.get('num_votes', 'N/A'),\n",
    "            'num_comments': row.get('num_comments', 'N/A'),\n",
    "            'filter': filter,\n",
    "            'period_filter': period_filter or 'N/A',\n",
    "            'post_date': 'N/A',\n",
    "            'post_time': 'N/A',\n",
    "            'time_ago': 'N/A',\n",
    "            'text_length': 0,\n",
    "            'post_description': 'N/A',\n",
    "            'post_visual': 'N/A',\n",
    "            'visual_type': 'NONE',\n",
    "            'visual_count': 0,\n",
    "            'downloaded_files': 'N/A',\n",
    "            'audio_file': 'N/A'  # üî• PERFECT NAMING COLUMN\n",
    "        }\n",
    "     \n",
    "        post_id = extract_post_id(row['post_link'])\n",
    "        if not post_id:\n",
    "            print(f\"   ‚ùå [{progress}] Invalid link - SKIPPED\")\n",
    "            new_data.append(post_data)\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "     \n",
    "        print(f\"   üîó [{progress}] Post ID: {post_id}\")\n",
    "     \n",
    "        try:\n",
    "            response = session.get(f\"https://www.reddit.com/comments/{post_id}.json\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data) > 0 and 'data' in data[0]:\n",
    "                    post_info = data[0]['data']['children'][0]['data']\n",
    "                 \n",
    "                    post_data.update({\n",
    "                        'post_id': post_id,\n",
    "                        'num_votes': str(post_info.get('score', 'N/A')),\n",
    "                    })\n",
    "                 \n",
    "                    # üî• DATE/TIME + TIME_AGO\n",
    "                    created_utc = post_info.get('created_utc')\n",
    "                    post_date, post_time = format_post_date(created_utc)\n",
    "                    post_data['post_date'] = post_date\n",
    "                    post_data['post_time'] = post_time\n",
    "                    post_data['time_ago'] = calculate_time_ago(post_date, post_time)\n",
    "                 \n",
    "                    print(f\"   üìÖ [{progress}] {post_date} | üïê {post_time[:12]} | ‚è∞ {post_data['time_ago']}\")\n",
    "                 \n",
    "                    selftext = post_info.get('selftext', '')[:2000]\n",
    "                    if selftext.strip():\n",
    "                        post_data['post_description'] = selftext\n",
    "                        post_data['text_length'] = calculate_text_length(selftext)\n",
    "                        print(f\"   üìù [{progress}] {post_data['text_length']} chars\")\n",
    "                 \n",
    "                    # üî• ENHANCED VISUAL EXTRACTION + TEST WORKING LINKS + AUTO DOWNLOAD\n",
    "                    all_candidate_urls = extract_visual_urls(post_info)\n",
    "                    base_filename = f\"{keyword_clean}_{filter}_{period_str}_{post_number}\"\n",
    "                 \n",
    "                    if all_candidate_urls:\n",
    "                        print(f\"   üñºÔ∏è [{progress}] Testing {len(all_candidate_urls)} candidate URLs...\")\n",
    "                     \n",
    "                        # üî• TEST + DOWNLOAD with EXACT naming + PROPER EXTENSIONS!\n",
    "                        downloaded_files, working_urls = download_visual_auto(\n",
    "                            post_number, 'CAROUSEL' if '\\n' in '\\n'.join(all_candidate_urls) else 'IMAGE',\n",
    "                            all_candidate_urls, base_filename, VISUALS_FOLDER\n",
    "                        )\n",
    "                     \n",
    "                        # üî• ONLY WORKING LINKS go to post_visual!\n",
    "                        if working_urls:\n",
    "                            post_data['post_visual'] = '\\n'.join(working_urls)\n",
    "                            vtype, vcount = get_visual_type_count(post_data['post_visual'])\n",
    "                            post_data.update({'visual_type': vtype, 'visual_count': vcount})\n",
    "                            post_data['downloaded_files'] = '; '.join(downloaded_files) if downloaded_files else 'ERROR'\n",
    "                         \n",
    "                            print(f\"   ‚úÖ [{progress}] {vtype} ({vcount}) - {len(working_urls)} WORKING URLs!\")\n",
    "                            print(f\"   üíæ {len(downloaded_files)} files saved!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ùå [{progress}] No working URLs found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ûñ [{progress}] No visuals\")\n",
    "                 \n",
    "                    # üî• PERFECT AUDIO DOWNLOAD with EXACT NAMING\n",
    "                    if post_data['visual_type'] in ['VIDEO'] and post_id:\n",
    "                        print(f\"   üéµ [{progress}] Extracting audio...\")\n",
    "                        # üî• PASS ALL PARAMETERS for PERFECT NAMING\n",
    "                        audio_path = download_reddit_audio_only(\n",
    "                            post_data['post_link'], \n",
    "                            keyword_clean, \n",
    "                            filter, \n",
    "                            period_str, \n",
    "                            post_number, \n",
    "                            AUDIO_FOLDER\n",
    "                        )\n",
    "                        if audio_path:\n",
    "                            audio_filename = os.path.basename(audio_path)\n",
    "                            post_data['audio_file'] = audio_filename  # ‚úÖ oban_star_racers_hot_all_time_2_audio.m4a\n",
    "                            print(f\"   ‚úÖ [{progress}] Audio: {audio_filename}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ûñ [{progress}] No audio extracted\")\n",
    "                 \n",
    "                    print(f\"   üéâ [{progress}] COMPLETE ‚úì\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå [{progress}] No post data\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå [{progress}] HTTP {response.status_code}\")\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  [{progress}] Error: {str(e)[:40]}\")\n",
    "     \n",
    "        new_data.append(post_data)\n",
    "        time.sleep(2.5)  # Rate limiting\n",
    "        print()  # Empty line\n",
    "   \n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    new_df = pd.DataFrame(new_data, columns=[\n",
    "        'post_title', 'post_link', 'post_id', 'num_votes', 'num_comments',\n",
    "        'filter', 'period_filter', 'post_date', 'post_time', 'time_ago', 'text_length',\n",
    "        'post_description', 'post_visual', 'visual_type', 'visual_count', 'downloaded_files',\n",
    "        'audio_file'  # üî• PERFECT NAMING\n",
    "    ])\n",
    "    new_df.to_csv(OUTPUT_FILE, index=False)\n",
    "   \n",
    "    print(f\"\\nüéâ SAVED {len(new_df)}/{limit} posts ‚Üí {OUTPUT_FILE}\")\n",
    "    print(f\"üíæ ALL VISUALS ‚Üí {VISUALS_FOLDER}/\")\n",
    "    print(f\"üéµ ALL AUDIO ‚Üí {AUDIO_FOLDER}/ (keyword_filter_period_postnumber_audio.m4a)\")\n",
    "    print(f\"‚úÖ post_visual = WORKING LINKS ONLY!\")\n",
    "    return new_df\n",
    "\n",
    "# üî• COMPLETE INTERACTIVE RUN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT EXTRACTOR + VISUALS + PERFECT AUDIO NAMING!\")\n",
    "    print(\"=\" * 60)\n",
    "   \n",
    "    # 1. KEYWORD FIRST\n",
    "    keyword = input(\"Enter keyword: \").strip() or 'music'\n",
    "   \n",
    "    # 2. FILTER NEXT\n",
    "    print(\"\\nüî• Filters: 1=hot, 2=top, 3=new, 4=comments, 5=relevance\")\n",
    "    choice = input(\"Choose filter [1]: \").strip() or '1'\n",
    "    filter_map = {'1': 'hot', '2': 'top', '3': 'new', '4': 'comments', '5': 'relevance'}\n",
    "    filter = filter_map.get(choice, 'hot')\n",
    "   \n",
    "    # 3. PERIOD NEXT (for relevance/top/comments)\n",
    "    period_filter = None\n",
    "    if filter in ['relevance', 'top', 'comments']:\n",
    "        print(f\"\\n‚è∞ PERIOD FILTER (HTML dropdown match):\")\n",
    "        print(\"1=All time, 2=Past year, 3=Past month, 4=Past week, 5=Today, 6=Past hour\")\n",
    "        period_choice = input(\"Choose period [2=Past year]: \").strip() or '2'\n",
    "        period_map = {\n",
    "            '1': 'All time', '2': 'Past year', '3': 'Past month',\n",
    "            '4': 'Past week', '5': 'Today', '6': 'Past hour'\n",
    "        }\n",
    "        period_filter = period_map.get(period_choice, 'Past year')\n",
    "        print(f\"   ‚úÖ Using period: {period_filter} ‚Üí API t={get_period_param(period_filter)}\")\n",
    "   \n",
    "    # 4. LIMIT LAST\n",
    "    limit_input = input(\"\\nHow many posts? (1-100) [20]: \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else 20\n",
    "    limit = min(max(limit, 1), 100)\n",
    "   \n",
    "    print(f\"\\nüî• Scraping {limit} '{keyword}' {filter.upper()} posts...\")\n",
    "    print(f\"‚úÖ WORKING LINKS + PROPER .png/.mp4 + AUDIO: keyword_filter_period_postnumber_audio.m4a\")\n",
    "    if period_filter:\n",
    "        print(f\"   ‚è∞ Time filter: {period_filter}\")\n",
    "    result = extract_post_details_complete(keyword, filter, limit, period_filter)\n",
    "   \n",
    "    period_filename = period_filter.replace(' ', '_').lower() if period_filter else 'all_time'\n",
    "    print(f\"\\n‚úÖ DONE! {len(result)} posts + visuals + audio ‚Üí ../data/\")\n",
    "    print(f\"üéµ Audio files ‚Üí ../data/audio/{keyword.replace(' ','_')}_{filter}_{period_filename}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bead4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üéûÔ∏è STEP 02: AUTOMATED MERGING OF AUDIOS AND VIDEOS \n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: hotpink;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95668355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def auto_video_merge_all():\n",
    "    \"\"\"üöÄ Automatically merge ALL video+audio pairs - outputs *_video.mp4\"\"\"\n",
    "    \n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-posts-scraper\")\n",
    "    visuals_dir = base_dir / \"data\" / \"visuals\"\n",
    "    audio_dir = base_dir / \"data\" / \"audio\"\n",
    "    \n",
    "    print(\"üîç Scanning for video/audio pairs...\")\n",
    "    \n",
    "    # Find ALL video files matching pattern *_postnumber_vid.mp4\n",
    "    video_files = list(visuals_dir.rglob(\"*_vid.mp4\"))\n",
    "    print(f\"üìä Found {len(video_files)} video files\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        # Extract keyword_filter_period_postnumber from *_vid.mp4\n",
    "        video_name = video_path.stem  # removes .mp4\n",
    "        if not video_name.endswith(\"_vid\"):\n",
    "            continue\n",
    "            \n",
    "        keyword_filter_period_postnumber = video_name[:-4]  # remove _vid\n",
    "        \n",
    "        # Construct matching audio path: same name + _audio.m4a\n",
    "        audio_path = audio_dir / video_path.relative_to(visuals_dir).parent / f\"{keyword_filter_period_postnumber}_audio.m4a\"\n",
    "        \n",
    "        # üî• NEW OUTPUT: keyword_filter_period_postnumber_video.mp4\n",
    "        output_path = video_path.parent / f\"{keyword_filter_period_postnumber}_video.mp4\"\n",
    "        \n",
    "        print(f\"\\nüîÑ [{success_count + fail_count + 1}/{len(video_files)}] {keyword_filter_period_postnumber}\")\n",
    "        print(f\"   üìÅ Video:  {video_path}\")\n",
    "        print(f\"   üìÅ Audio:  {audio_path}\")\n",
    "        print(f\"   üìÅ Output: {output_path}\")\n",
    "        \n",
    "        # Skip if already merged\n",
    "        if output_path.exists():\n",
    "            print(\"   ‚úÖ Already exists - SKIPPING\")\n",
    "            success_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not audio_path.exists():\n",
    "            print(\"   ‚ùå Audio missing - SKIPPING\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Merge command\n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "            \"-y\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-i\", str(audio_path),\n",
    "            \"-map\", \"0:v:0\",\n",
    "            \"-map\", \"1:a:0\",\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-shortest\",\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                size_mb = output_path.stat().st_size / 1e6\n",
    "                print(f\"   ‚úÖ SUCCESS! ({size_mb:.1f} MB)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ‚ùå FAILED: {result.stderr[:200]}\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   ‚è∞ TIMEOUT - Killed\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}\")\n",
    "    print(f\"   ‚ùå Failed:  {fail_count}\")\n",
    "    print(f\"   üìÅ Total:   {len(video_files)}\")\n",
    "\n",
    "# üöÄ RUN ALL AUTOMATICALLY\n",
    "auto_video_merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e96337",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# üß™ TESTING\n",
    "\n",
    "\n",
    "<style>\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    color: red;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h2 {\n",
    "    text-align: left;\n",
    "    color: darkblue;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h3 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<style>\n",
    "h4 {\n",
    "    text-align: center;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# User inputs - MODIFY THESE\n",
    "KEYWORD = \"your_keyword_here\"  # e.g. \"lofi\" or \"synthwave\"\n",
    "FILTER = \"hot\"  # \"hot\", \"new\", \"top\"\n",
    "PERIOD = \"day\"  # \"hour\", \"day\", \"week\", \"month\", \"year\", \"all\"\n",
    "LIMIT = 10  # Number of posts to process\n",
    "\n",
    "def auto_video_merge_selected():\n",
    "    \"\"\"üöÄ Merge video+audio pairs for specific keyword/filter/period/limit\"\"\"\n",
    "    \n",
    "    base_dir = Path(r\"C:/Users/sboub/Documents/GitHub/reddit-music\")\n",
    "    visuals_dir = base_dir / \"data\" / \"visuals\"\n",
    "    audio_dir = base_dir / \"data\" / \"audio\"\n",
    "    \n",
    "    # Construct the naming pattern: keyword_filter_period\n",
    "    pattern_name = f\"{KEYWORD}_{FILTER}_{PERIOD}\"\n",
    "    \n",
    "    print(f\"üîç Scanning for: {pattern_name} (limit: {LIMIT})\")\n",
    "    \n",
    "    # Find video files matching EXACT pattern: keyword_filter_period_*_vid.mp4\n",
    "    video_pattern = f\"*{pattern_name}*_vid.mp4\"\n",
    "    video_files = list(visuals_dir.rglob(video_pattern))\n",
    "    \n",
    "    # Sort by postnumber (natural sort) and limit\n",
    "    video_files.sort(key=lambda p: int(p.stem.split('_')[-2]) if p.stem.split('_')[-1] == 'vid' else 0)\n",
    "    video_files = video_files[:LIMIT]\n",
    "    \n",
    "    print(f\"üìä Found {len(video_files)} matching video files\")\n",
    "    \n",
    "    if not video_files:\n",
    "        print(\"‚ùå No matching files found!\")\n",
    "        return\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        # Extract keyword_filter_period_postnumber from *_vid.mp4\n",
    "        video_name = video_path.stem  # removes .mp4\n",
    "        if not video_name.endswith(\"_vid\"):\n",
    "            continue\n",
    "            \n",
    "        keyword_filter_period_postnumber = video_name[:-4]  # remove _vid\n",
    "        \n",
    "        # Construct matching audio path: same name + _audio.m4a\n",
    "        audio_path = audio_dir / video_path.relative_to(visuals_dir).parent / f\"{keyword_filter_period_postnumber}_audio.m4a\"\n",
    "        \n",
    "        # üî• NEW OUTPUT: keyword_filter_period_postnumber_video.mp4\n",
    "        output_path = video_path.parent / f\"{keyword_filter_period_postnumber}_video.mp4\"\n",
    "        \n",
    "        print(f\"\\nüîÑ [{success_count + fail_count + 1}/{len(video_files)}] {keyword_filter_period_postnumber}\")\n",
    "        print(f\"   üìÅ Video:  {video_path}\")\n",
    "        print(f\"   üìÅ Audio:  {audio_path}\")\n",
    "        print(f\"   üìÅ Output: {output_path}\")\n",
    "        \n",
    "        # Skip if already merged\n",
    "        if output_path.exists():\n",
    "            print(\"   ‚úÖ Already exists - SKIPPING\")\n",
    "            success_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not audio_path.exists():\n",
    "            print(\"   ‚ùå Audio missing - SKIPPING\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Merge command\n",
    "        cmd = [\n",
    "            r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
    "            \"-y\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-i\", str(audio_path),\n",
    "            \"-map\", \"0:v:0\",\n",
    "            \"-map\", \"1:a:0\",\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-shortest\",\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                size_mb = output_path.stat().st_size / 1e6\n",
    "                print(f\"   ‚úÖ SUCCESS! ({size_mb:.1f} MB)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ‚ùå FAILED: {result.stderr[:200]}\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   ‚è∞ TIMEOUT - Killed\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ SUMMARY for {pattern_name}:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}\")\n",
    "    print(f\"   ‚ùå Failed:  {fail_count}\")\n",
    "    print(f\"   üìÅ Total:   {len(video_files)}\")\n",
    "\n",
    "# üöÄ RUN with your parameters\n",
    "auto_video_merge_selected()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
